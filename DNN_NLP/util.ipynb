{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# training with SGLD with annealing and save models\n",
    "def train(X_train, y_train, X_valid, y_valid, X_test, y_test, model, args):\n",
    "    model.train()\n",
    "    batch = args.batch_size\n",
    "\n",
    "    parameters = [parameter for parameter in model.parameters()]\n",
    "\n",
    "\n",
    "    set_scale = [parameter.data.std().item() for parameter in model.parameters()]\n",
    "    set_scale = [scale / max(set_scale) for scale in set_scale] # normalize\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        corrects = 0\n",
    "        epsilon = args.lr * ((epoch * 1.0) ** (-0.333)) # optimal decay rate\n",
    "        for idx in range(int(X_train.shape[0]/batch) + 1):\n",
    "            feature = torch.LongTensor(X_train[(idx*batch):(idx*batch+batch),])\n",
    "            target = torch.LongTensor(y_train[(idx*batch):(idx*batch+batch)])\n",
    "            if args.cuda:\n",
    "                feature, target = feature.cuda(), target.cuda()\n",
    "            logit = model(feature)\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            for layer_no, param in enumerate(model.parameters()):\n",
    "                if args.static and layer_no == 0: # fixed embedding layer cannot update\n",
    "                    continue\n",
    "                # by default I assume you train the models using GPU\n",
    "                noise = torch.cuda.FloatTensor(param.data.size()).normal_() * np.sqrt(epsilon / args.t)\n",
    "                #noise = torch.cuda.FloatTensor(param.data.size()).normal_() * set_scale[layer_no]\n",
    "                parameters[layer_no].data += (- epsilon / 2 * param.grad + noise)\n",
    "\n",
    "            corrects += (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum().item()\n",
    "            accuracy = 100.0 * corrects / batch / (idx + 1)\n",
    "            sys.stdout.write('\\rEpoch[{}] Batch[{}] - loss: {:.4f}  acc: {:.2f}%({}/{}) tempreture: {}'.format(\n",
    "                             epoch, idx, loss.item(), accuracy, corrects, batch * (idx + 1), int(args.t)))\n",
    "            args.t = args.t + 1 # annealing\n",
    "        if epoch % 5 != 0:\n",
    "            continue\n",
    "        '''\n",
    "        try:\n",
    "            set_scale = [parameter.grad.data.std().item() for parameter in model.parameters()]\n",
    "            set_scale = [scale / max(set_scale) for scale in set_scale] # normalize\n",
    "        except:\n",
    "            set_scale = [parameter.data.std().item() for parameter in model.parameters()]\n",
    "            set_scale = [scale / max(set_scale) for scale in set_scale] # normalize\n",
    "        '''\n",
    "        save(model, args.save_dir, epoch)\n",
    "        print()\n",
    "        eval(X_valid, y_valid, model, 'Validation', args)\n",
    "        eval(X_test, y_test, model, 'Testing   ', args)\n",
    "\n",
    "\n",
    "def eval(X, y, model, term, args):\n",
    "    model.eval()\n",
    "    corrects, TP, avg_loss = 0, 0, 0\n",
    "    correct_part, total_part = {0.2:0, 0.4:0}, {0.2:1e-16, 0.4:1e-16}\n",
    "    batch = args.batch_size\n",
    "\n",
    "    for idx in range(int(X.shape[0]/batch) + 1):\n",
    "        feature = torch.LongTensor(X[(idx*batch):(idx*batch+batch),])\n",
    "        target = torch.LongTensor(y[(idx*batch):(idx*batch+batch)])\n",
    "        if args.cuda:\n",
    "            feature, target = feature.cuda(), target.cuda()\n",
    "\n",
    "        logit = model(feature)\n",
    "        loss = F.cross_entropy(logit, target, size_average=False)\n",
    "        avg_loss += loss.data.item()\n",
    "        predictor = torch.exp(logit[:, 1]) / (torch.exp(logit[:, 0]) + torch.exp(logit[:, 1]))\n",
    "        for xnum in range(1, 3):\n",
    "            thres = round(0.2 * xnum, 1)\n",
    "            idx_thres = (predictor > 0.5 + thres) + (predictor < 0.5 - thres)\n",
    "            correct_part[thres] += (torch.max(logit, 1)[1][idx_thres] == target.data[idx_thres]).sum().item()\n",
    "            total_part[thres] += idx_thres.sum().item()\n",
    "\n",
    "        corrects += (torch.max(logit, 1)[1] == target.data).sum().item()\n",
    "        TP += (((torch.max(logit, 1)[1] == target.data).int() + (torch.max(logit, 1)[1]).int()) == 2).sum().item()\n",
    "\n",
    "    size = y.shape[0]\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    # TP, TN: True Positive/True Negative\n",
    "    print('         {} - loss: {:.4f} acc: {:.2f}%({}/{}) {:.2f}%({}/{}) {:.2f}%({}/{}) TP/TN: ({}/{}) \\n'.format(term,\n",
    "          avg_loss, accuracy, corrects, size, 100.0 * correct_part[0.2] / total_part[0.2], correct_part[0.2], int(total_part[0.2]), \n",
    "          100.0 * correct_part[0.4] / total_part[0.4], correct_part[0.4], int(total_part[0.4]), TP, corrects - TP))\n",
    "    return accuracy\n",
    "\n",
    "def bma_eval(X, y, mymodels, term, args):\n",
    "    \n",
    "    corrects, TP, avg_loss = 0, 0, 0\n",
    "    correct_part, total_part = {0.2:0, 0.4:0}, {0.2:1e-16,0.4:1e-16}\n",
    "    batch = args.batch_size\n",
    "\n",
    "    for model in mymodels:\n",
    "        model.eval()\n",
    "        for idx in range(int(X.shape[0]/batch) + 1):\n",
    "            feature = torch.LongTensor(X[(idx*batch):(idx*batch+batch),])\n",
    "            target = torch.LongTensor(y[(idx*batch):(idx*batch+batch)])\n",
    "            if args.cuda:\n",
    "                feature, target = feature.cuda(), target.cuda()\n",
    "\n",
    "            logit = model(feature)\n",
    "            loss = F.cross_entropy(logit, target, size_average=False)\n",
    "            avg_loss += loss.data.item() / (len(mymodels) * 1.0)\n",
    "            predictor = torch.exp(logit[:, 1]) / (torch.exp(logit[:, 0]) + torch.exp(logit[:, 1]))\n",
    "            for xnum in range(1, 3):\n",
    "                thres = round(0.2 * xnum, 1)\n",
    "                idx_thres = (predictor > 0.5 + thres) + (predictor < 0.5 - thres)\n",
    "                correct_part[thres] += (torch.max(logit, 1)[1][idx_thres] == target.data[idx_thres]).sum().item() / (len(mymodels) * 1.0)\n",
    "                total_part[thres] += idx_thres.sum().item() / (len(mymodels) * 1.0)\n",
    "            corrects += (torch.max(logit, 1)[1] == target.data).sum().item() / (len(mymodels) * 1.0)\n",
    "            TP += (((torch.max(logit, 1)[1] == target.data).int() + (torch.max(logit, 1)[1]).int()) == 2).sum().item()\n",
    "\n",
    "    size = y.shape[0]\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    TP = TP * 1.0 / (len(mymodels) * 1.0)\n",
    "    print('         {} - loss: {:.4f} acc: {:.2f}%({}/{}) {:.2f}%({}/{}) {:.2f}%({}/{}) TP/TN: ({}/{}) \\n'.format(term,\n",
    "            avg_loss, accuracy, corrects, size, 100.0 * correct_part[0.2] / total_part[0.2], correct_part[0.2], int(total_part[0.2]), \n",
    "            100.0 * correct_part[0.4] / total_part[0.4], correct_part[0.4], int(total_part[0.4]), TP, corrects - TP))\n",
    "    return accuracy\n",
    "\n",
    "def predictor_preprocess(cnn, args):\n",
    "    # load trained thinning samples (Bayesian CNN models) from input/models/\n",
    "    mymodels = []\n",
    "\n",
    "    for num, each_model in enumerate(os.listdir(args.save_dir)):\n",
    "        print(args.save_dir + each_model)\n",
    "        if args.cuda:\n",
    "            cnn.load_state_dict(torch.load(args.save_dir + each_model))\n",
    "        else:\n",
    "            cnn.load_state_dict(torch.load(args.save_dir + each_model, map_location=lambda storage, loc: storage))\n",
    "        mymodels.append(copy.deepcopy(cnn))\n",
    "        if num > 30: # in case memory overloads\n",
    "            break\n",
    "\n",
    "    with open('./input/word2idx', 'r') as file:\n",
    "        word2idx = json.load(file)\n",
    "\n",
    "    stopWords = set()\n",
    "    with open('./input/stopWords', encoding = 'utf-8') as file:\n",
    "        for word in file:\n",
    "            stopWords.add(word.strip())\n",
    "    return(mymodels, word2idx, stopWords)\n",
    "\n",
    "def predict(sentence, mymodels, word2idx, stopWords, args):\n",
    "    tokens = tokenize_news(sentence, stopWords)\n",
    "    tokens = [word2idx[t] if t in word2idx else word2idx['UNKNOWN'] for t in tokens]\n",
    "    if len(tokens) < 5 or tokens == [word2idx['UNKNOWN']] * len(tokens): # tokens cannot be too short or unknown\n",
    "        signal = 'Unknown'\n",
    "    else:\n",
    "        feature = torch.LongTensor([tokens])\n",
    "        logits = []\n",
    "        for model in mymodels:\n",
    "            model.eval()\n",
    "            if args.cuda:\n",
    "                feature = feature.cuda()\n",
    "            logit = model(feature)\n",
    "            predictor = torch.exp(logit[:, 1]) / (torch.exp(logit[:, 0]) + torch.exp(logit[:, 1]))\n",
    "            logits.append(predictor.item())\n",
    "        signal = signals(np.mean(logits))\n",
    "    return(signal)\n",
    "\n",
    "\n",
    "def daily_predict(cnn, args):\n",
    "    mymodels, word2idx, stopWords = predictor_preprocess(cnn, args)\n",
    "    output = './input/news/' + args.date[:4] + '/news_' + args.date + '.csv'\n",
    "    fout = open(output + '_bak', 'w', encoding = 'utf-8')\n",
    "    with open(output, encoding = 'utf-8') as f:\n",
    "        for num, line in enumerate(f):\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) == 6:\n",
    "                ticker, name, day, headline, body, newsType = line\n",
    "            elif len(line) == 7:\n",
    "                ticker, name, day, headline, body, newsType, signal = line\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            #if newsType != 'topStory': # newsType: [topStory, normal]\n",
    "            #    signal = 'Unknown'\n",
    "            content = headline + ' ' + body\n",
    "            signal = predict(content, mymodels, word2idx, stopWords, args)\n",
    "            fout.write(','.join([ticker, name, day, headline, body, newsType, signal]) + '\\n')\n",
    "    fout.close()\n",
    "    print('change file name')\n",
    "    print('mv ' + output + '_bak ' + output)\n",
    "    os.system('mv ' + output + '_bak ' + output)\n",
    "    return(signal)\n",
    "\n",
    "\n",
    "def save(model, save_dir, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = '{}/model_{}.pt'.format(save_dir,steps)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "def signals(digit):\n",
    "    strong_signal = 0.4\n",
    "    unknown_thres = 0.05\n",
    "    if digit > 0.5 + strong_signal:\n",
    "        return('Strong Buy')\n",
    "    elif digit > 0.5 + unknown_thres:\n",
    "        return('Buy')\n",
    "    elif digit > 0.5 - unknown_thres:\n",
    "        return('Unknown')\n",
    "    elif digit > 0.5 - strong_signal:\n",
    "        return('Sell')\n",
    "    else:\n",
    "        return('Strong Sell')\n",
    "\n",
    "def padding(sentencesVec, keepNum):\n",
    "    shape = sentencesVec.shape[0]\n",
    "    ownLen = sentencesVec.shape[1]\n",
    "    if ownLen < keepNum:\n",
    "        return np.hstack((np.ones([shape, keepNum-ownLen]), sentencesVec)).flatten()\n",
    "    else:\n",
    "        return sentencesVec[:, -keepNum:].flatten()\n",
    "\n",
    "\n",
    "def dateGenerator(numdays): # generate N days until now, eg [20151231, 20151230]\n",
    "    base = datetime.datetime.today()\n",
    "    date_list = [base - datetime.timedelta(days=x) for x in range(0, numdays)]\n",
    "    for i in range(len(date_list)):\n",
    "        date_list[i] = date_list[i].strftime(\"%Y%m%d\")\n",
    "    return set(date_list)\n",
    "\n",
    "\n",
    "def generate_past_n_days(numdays):\n",
    "    \"\"\"Generate N days until now, e.g., [20151231, 20151230].\"\"\"\n",
    "    base = datetime.datetime.today()\n",
    "    date_range = [base - datetime.timedelta(days=x) for x in range(0, numdays)]\n",
    "    return [x.strftime(\"%Y%m%d\") for x in date_range]\n",
    "\n",
    "def unify_word(word):  # went -> go, apples -> apple, BIG -> big\n",
    "    \"\"\"unify verb tense and noun singular\"\"\"\n",
    "    ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    for wt in [ADJ, ADJ_SAT, ADV, NOUN, VERB]:\n",
    "        try:\n",
    "            word = WordNetLemmatizer().lemmatize(word, pos=wt)\n",
    "        except:\n",
    "            pass\n",
    "    return word.lower()\n",
    "\n",
    "def digit_filter(word):\n",
    "    check = re.match(r'\\d*\\.?\\d*', word).group()\n",
    "    if check == \"\":\n",
    "        return word\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def unify_word_meaning(word):\n",
    "    if word in [\"bigger-than-expected\", \"higher-than-expected\", \"better-than-expected\", \"stronger-than-expected\"]:\n",
    "        return \"better\"\n",
    "    elif word in [\"smaller-than-expected\", \"lower-than-expected\", \"weaker-than-expected\", \"worse-than-expected\"]:\n",
    "        return \"lower\"\n",
    "    elif word in [\"no\", \"not\", \"n't\"]:\n",
    "        return \"not\"\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def get_soup_with_repeat(url, repeat_times=3, verbose=True):\n",
    "    for i in range(repeat_times): # repeat in case of http failure\n",
    "        try:\n",
    "            time.sleep(np.random.poisson(3))\n",
    "            response = urlopen(url)\n",
    "            data = response.read().decode('utf-8')\n",
    "            return BeautifulSoup(data, \"lxml\")\n",
    "        except Exception as e:\n",
    "            if i == 0:\n",
    "                print(e)\n",
    "            if verbose:\n",
    "                print('retry...')\n",
    "            continue\n",
    "\n",
    "def tokenize_news(headline, stopWords):\n",
    "    tokens = nltk.word_tokenize(headline) #+ nltk.word_tokenize(body)\n",
    "    tokens = list(map(unify_word, tokens))\n",
    "    tokens = list(map(unify_word, tokens)) # some words fail filtering in the 1st time\n",
    "    tokens = list(map(digit_filter, tokens)) \n",
    "    tokens = list(map(unify_word_meaning, tokens))\n",
    "    tokens = [t for t in tokens if t not in stopWords and t != \"\"]\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "def value2int(y, clusters=2):\n",
    "    label = np.copy(y)\n",
    "    label[y < np.percentile(y, 100 / clusters)] = 0\n",
    "    for i in range(1, clusters):\n",
    "        label[y > np.percentile(y, 100 * i / clusters)] = i\n",
    "    return label\n",
    "\n",
    "def value2int_simple(y):\n",
    "    label = np.copy(y)\n",
    "    label[y < 0] = 0\n",
    "    label[y >= 0] = 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def model_eval(net, data_loader, if_print=1):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for cnt, (images, labels) in enumerate(data_loader):\n",
    "        images, labels = Variable(images), Variable(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        outputs = net.forward(images)\n",
    "        prediction = outputs.data.max(1)[1]\n",
    "        correct += prediction.eq(labels.data).sum().item()\n",
    "    print('\\nTest set: Accuracy: {:0.2f}%'.format(100.0 * correct / len(data_loader.dataset)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
