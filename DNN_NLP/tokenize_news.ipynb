{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import operator\n",
    "import argparse\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import util\n",
    "\n",
    "\n",
    "\"\"\" Use pretrained word vector to generate our target features\n",
    "Required input data:\n",
    "./input/stopWords\n",
    "./input/stockReturns.json\n",
    "./input/news/*/*\n",
    "\n",
    "Output file name: \n",
    "input/featureMatrix_train\n",
    "input/featureMatrix_test \n",
    "input/word2idx\"\"\"\n",
    "\n",
    "# credit: https://github.com/lazyprogrammer/machine_learning_examples/tree/master/nlp_class2\n",
    "\n",
    "def tokenize(news_file, price_file, stopWords_file, output, output_wd2idx, sen_len, term_type, n_vocab, mtype):\n",
    "    # load price data\n",
    "    with open(price_file) as file:\n",
    "        print(\"Loading price info ... \" + mtype)\n",
    "        priceDt = json.load(file)[term_type]\n",
    "\n",
    "    testDates = util.dateGenerator(90) # the most recent days are used for testing\n",
    "    os.system('rm ' + output + mtype)\n",
    "\n",
    "    # load stop words\n",
    "    stopWords = set()\n",
    "    with open(stopWords_file) as file:\n",
    "        for word in file:\n",
    "            stopWords.add(word.strip())\n",
    "\n",
    "    # build feature matrix\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    current_idx = 2\n",
    "    word_idx_count = {0: float('inf'), 1: float('inf')}\n",
    "    sentences, labels = [], []\n",
    "    os.system('cat ./input/news/*/* > ./input/news_reuters.csv')\n",
    "    with open(news_file) as f:\n",
    "        for num, line in enumerate(f):\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) not in [6, 7]:\n",
    "                continue\n",
    "            if len(line) == 6:\n",
    "                ticker, name, day, headline, body, newsType = line\n",
    "            else:\n",
    "                ticker, name, day, headline, body, newsType, suggestion = line\n",
    "            if newsType != 'topStory': # newsType: [topStory, normal]\n",
    "                continue # skip normal news\n",
    "            \n",
    "            if ticker not in priceDt: \n",
    "                continue # skip if no corresponding company found\n",
    "            if day not in priceDt[ticker]: \n",
    "                continue # skip if no corresponding date found\n",
    "\n",
    "            if num % 10000 == 0: \n",
    "                print(\"%sing samples %d\" % (mtype, num))\n",
    "            if mtype == \"test\" and day not in testDates: \n",
    "                continue\n",
    "            if mtype == \"train\" and day in testDates: \n",
    "                continue\n",
    "            content = headline + ' ' + body\n",
    "            content = content.replace(\"-\", \" \") \n",
    "            tokens = util.tokenize_news(content, stopWords)\n",
    "\n",
    "            for t in tokens:\n",
    "                if t not in word2idx:\n",
    "                    word2idx[t] = current_idx\n",
    "                    idx2word.append(t)\n",
    "                    current_idx += 1\n",
    "                idx = word2idx[t]\n",
    "                word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "            sentence_by_idx = [word2idx[t] for t in tokens if t not in stopWords]\n",
    "            sentences.append(sentence_by_idx)\n",
    "            labels.append(round(priceDt[ticker][day], 6))\n",
    "\n",
    "    # restrict vocab size\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    total_num, cdf = 0.0, 0.0\n",
    "\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        if count == \"inf\" or count == float('inf'):\n",
    "            continue\n",
    "        total_num += count\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        if count == \"inf\" or count == float('inf'):\n",
    "            continue\n",
    "        cdf += (count * 1.0 / (total_num * 1.0))\n",
    "        print(word, count, str(cdf)[:5])\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    # map old idx to new idx\n",
    "    features = [] # shorter sentence idx\n",
    "    for num, sentence in enumerate(sentences):\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            # padding\n",
    "            if len(new_sentence) > sen_len:\n",
    "                new_sentence = new_sentence[:sen_len]\n",
    "            else:\n",
    "                new_sentence = new_sentence + [1] * (sen_len - len(new_sentence))\n",
    "            new_sentence.append(labels[num])\n",
    "            features.append(new_sentence)\n",
    "\n",
    "    features = np.matrix(features)\n",
    "    print(features.shape)\n",
    "\n",
    "\n",
    "    with open(output_wd2idx, 'w') as fp:\n",
    "        json.dump(word2idx_small, fp)\n",
    "\n",
    "    with open(output + mtype, 'a+') as file:\n",
    "        np.savetxt(file, features, fmt=\"%s\")\n",
    "\n",
    "def main():\n",
    "    news_file = \"./input/news_reuters.csv\"\n",
    "    stopWords_file = \"./input/stopWords\"\n",
    "    price_file = \"./input/stockReturns.json\"\n",
    "\n",
    "    \n",
    "    output = './input/featureMatrix_'\n",
    "    output_wd2idx = \"./input/word2idx\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Tokenize Reuters news')\n",
    "    parser.add_argument('-vocabs', type=int, default=6000, help='total number of vocabularies [default: 1000]')\n",
    "    parser.add_argument('-words', type=int, default=40, help='max number of words in a sentence [default: 20]')\n",
    "    parser.add_argument('-term', type=str, default='short', help='return type [short mid long] [default: short]')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    tokenize(news_file, price_file, stopWords_file, output, output_wd2idx, args.words, args.term, args.vocabs, 'train')\n",
    "    tokenize(news_file, price_file, stopWords_file, output, output_wd2idx, args.words, args.term, args.vocabs, 'test')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
