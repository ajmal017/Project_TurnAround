{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import os\n",
    "import en\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import reuters\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "\n",
    "def gen_financial_top_words(maxN=40000): # generate corpus based on Reuters news\n",
    "    if not os.path.isfile('./input/topWords.json'):\n",
    "        wordCnt = {}\n",
    "        for field in reuters.fileids():\n",
    "            for word in reuters.words(field):\n",
    "                word = unify_word(word)\n",
    "                if word in nltk.corpus.stopwords.words('english'):\n",
    "                    continue\n",
    "                wordCnt[word] = wordCnt.get(word, 0) + 1\n",
    "\n",
    "        sorted_wordCnt = sorted(wordCnt.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        wordCnt = {} # reset wordCnt\n",
    "        for i in sorted_wordCnt[:maxN]: wordCnt[i[0]] = i[1] # convert list to dict\n",
    "        with open('./input/topWords.json', 'w') as fout: json.dump(wordCnt, fout, indent=4)\n",
    "    else: return\n",
    "\n",
    "def unify_word(word):\n",
    "    try: word = en.verb.present(word) # unify tense\n",
    "    except: pass\n",
    "    try: word = en.noun.singular(word) # unify noun\n",
    "    except: pass\n",
    "    return word.lower()\n",
    "\n",
    "def build_FeatureMatrix(max_words=20, n_vocab=2000):\n",
    "    if not os.path.isfile('./input/topWords.json'):\n",
    "        gen_financial_top_words()\n",
    "    with open('./input/topWords.json') as data_file:    \n",
    "        topWords = json.load(data_file)\n",
    "\n",
    "    with open('./input/stockPrices.json') as data_file:    \n",
    "        priceDt = json.load(data_file)\n",
    "    loc = './input/'\n",
    "    input_files = [f for f in os.listdir(loc) if f.startswith('news_reuters.csv')]\n",
    "    sentences = []\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    current_idx = 2\n",
    "    word_idx_cnt = {0: float('inf'), 1: float('inf')}\n",
    "    labels = []\n",
    "    dp = {} # only consider one news for a company everyday\n",
    "    cnt = 0\n",
    "    for file in input_files:\n",
    "        for line in open(loc + file):\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) != 5: continue\n",
    "            ticker, name, day, headline, body = line\n",
    "            \n",
    "            if ticker not in priceDt: continue\n",
    "            if day not in priceDt[ticker]: continue\n",
    "            # avoid repeating news\n",
    "            if ticker not in dp: dp[ticker] = set()\n",
    "            if day in dp[ticker]: continue\n",
    "            dp[ticker].add(day)\n",
    "            print(cnt, ticker); cnt += 1\n",
    "            tokens = nltk.word_tokenize(headline) + nltk.word_tokenize(body)\n",
    "            tokens = [t for t in tokens if t in topWords]\n",
    "            for t in tokens:\n",
    "                if t not in word2idx:\n",
    "                    word2idx[t] = current_idx\n",
    "                    idx2word.append(t)\n",
    "                    current_idx += 1\n",
    "                idx = word2idx[t]\n",
    "                word_idx_cnt[idx] = word_idx_cnt.get(idx, 0) + 1\n",
    "            sentence_by_idx = [word2idx[t] for t in tokens]\n",
    "            sentences.append(sentence_by_idx)\n",
    "            labels.append(round(priceDt[ticker][day], 6))\n",
    "\n",
    "\n",
    "    # restrict vocabulary size\n",
    "    sorted_word_idx_cnt = sorted(word_idx_cnt.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_map = {}\n",
    "    for idx, count in sorted_word_idx_cnt[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    # map full dict to truncated dict\n",
    "    sentences_small = []\n",
    "    new_label = []\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_map[idx] if idx in idx_new_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "            new_label.append(label)\n",
    "\n",
    "    \n",
    "    truncated_small = np.matrix(sequence.pad_sequences(sentences_small, maxlen=max_words))\n",
    "    truncated_small = truncated_small.astype('int').astype('str')\n",
    "    new_label = np.matrix(new_label).astype('str')\n",
    "\n",
    "    featureMatrix = np.concatenate((truncated_small, new_label.T), axis=1)\n",
    "    np.savetxt('./input/featureMatrix.csv', featureMatrix, fmt=\"%s\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    build_FeatureMatrix(max_words=20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
