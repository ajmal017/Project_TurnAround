{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import os\n",
    "import en\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import operator\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import reuters\n",
    "from keras.preprocessing import sequence\n",
    "from crawler_reuters import news_Reuters\n",
    "\n",
    "\n",
    "\n",
    "# def gen_financial_top_words(maxN=40000): # generate corpus based on Reuters news\n",
    "#     if not os.path.isfile('./input/topWords.json'):\n",
    "#         wordCnt = {}\n",
    "#         for field in reuters.fileids():\n",
    "#             for word in reuters.words(field):\n",
    "#                 word = unify_word(word)\n",
    "#                 if word in nltk.corpus.stopwords.words('english'):\n",
    "#                     continue\n",
    "#                 wordCnt[word] = wordCnt.get(word, 0) + 1\n",
    "\n",
    "#         sorted_wordCnt = sorted(wordCnt.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#         wordCnt = {} # reset wordCnt\n",
    "#         for i in sorted_wordCnt[:maxN]: wordCnt[i[0]] = i[1] # convert list to dict\n",
    "#         with open('./input/topWords.json', 'w') as fout: json.dump(wordCnt, fout, indent=4)\n",
    "#     else: return\n",
    "\n",
    "def unify_word(word): # went -> go, apples -> apple, BIG -> big\n",
    "    try: word = en.verb.present(word) # unify tense\n",
    "    except: pass\n",
    "    try: word = en.noun.singular(word) # unify noun\n",
    "    except: pass\n",
    "    return word.lower()\n",
    "\n",
    "def dateGenerator(numdays): # generate N days until now, eg [20151231, 20151230]\n",
    "    base = datetime.datetime.today()\n",
    "    date_list = [base - datetime.timedelta(days=x) for x in range(0, numdays)]\n",
    "    for i in range(len(date_list)): date_list[i] = date_list[i].strftime(\"%Y%m%d\")\n",
    "    return set(date_list)\n",
    "\n",
    "\n",
    "'''\n",
    "The following function is a little complicated.\n",
    "It consists of the following steps\n",
    "1, load top words dictionary, load prices data to make correlation\n",
    "2, build feature matrix for training data\n",
    "    2.1 tokenize sentense, check if the word belongs to the top words, unify the format of words\n",
    "    2.2 create word2idx/idx2word list, and a list to count the occurence of words\n",
    "    2.3 concatenate multi-news into a single one if they happened at the same day\n",
    "    2.4 limit the vocabulary size to e.g. 2000, and let the unkown words as the last one\n",
    "    2.5 map full dict to truncated dict, pad the sequence to the same length, done\n",
    "3, project the test feature in the word2idx for the traning data\n",
    "'''\n",
    "def build_FeatureMatrix(max_words=60, n_vocab=2000):\n",
    "    # step 1, load top words dictionary, load prices data to make correlation\n",
    "    if not os.path.isfile('./input/topWords.json'):\n",
    "        gen_financial_top_words()\n",
    "    # with open('./input/topWords.json') as data_file:    \n",
    "    #     topWords = json.load(data_file)\n",
    "\n",
    "    with open('./input/stockPrices.json') as data_file:    \n",
    "        priceDt = json.load(data_file)\n",
    "    # step 2, build feature matrix for training data\n",
    "    loc = './input/'\n",
    "    input_files = [f for f in os.listdir(loc) if f.startswith('news_reuters.csv')]\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    current_idx = 2\n",
    "    word_idx_cnt = {0: float('inf'), 1: float('inf')}\n",
    "    dp = {} # only consider one news for a company everyday\n",
    "    cnt = 0\n",
    "    testDates = dateGenerator(100)\n",
    "    stopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "    for file in input_files:\n",
    "        for line in open(loc + file):\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) != 5: continue\n",
    "            ticker, name, day, headline, body = line\n",
    "            if ticker not in priceDt: continue # skip if no corresponding company found\n",
    "            if day not in priceDt[ticker]: continue # skip if no corresponding date found\n",
    "            # # avoid repeating news\n",
    "            if ticker not in dp: dp[ticker] = {}\n",
    "            if day not in dp[ticker]: dp[ticker][day] = {'feature':[], 'label':[]}\n",
    "            # if ticker not in dp: dp[ticker] = set()\n",
    "            # if day in dp[ticker]: continue\n",
    "            # dp[ticker].add(day)\n",
    "            # 2.1 tokenize sentense, check if the word belongs to the top words, unify the format of words\n",
    "            tokens = nltk.word_tokenize(headline) + nltk.word_tokenize(body)\n",
    "            tokens = [unify_word(t) for t in tokens]\n",
    "            tokens = [t for t in tokens if t in stopWords]\n",
    "            #tokens = [t for t in tokens if t in topWords]\n",
    "            # 2.2 create word2idx/idx2word list, and a list to count the occurence of words\n",
    "            for t in tokens:\n",
    "                if t not in word2idx:\n",
    "                    word2idx[t] = current_idx\n",
    "                    idx2word.append(t)\n",
    "                    current_idx += 1\n",
    "                idx = word2idx[t]\n",
    "                word_idx_cnt[idx] = word_idx_cnt.get(idx, 0) + 1\n",
    "            if day in testDates: continue # this step only considers training set\n",
    "            sentence_by_idx = [word2idx[t] for t in tokens]\n",
    "            print(\"training\", cnt, ticker); cnt += 1\n",
    "            #sentences.append(sentence_by_idx)\n",
    "            dp[ticker][day]['feature'].append(sentence_by_idx)\n",
    "            dp[ticker][day]['label'] = round(priceDt[ticker][day], 6)\n",
    "\n",
    "    # 2.3 concatenate multi-news into a single one if they happened at the same day\n",
    "    sentences, labels, sentenceLen = [], [], []\n",
    "    for ticker in dp:\n",
    "        for day in dp[ticker]:\n",
    "            res = []\n",
    "            for i in dp[ticker][day]['feature']: res += i\n",
    "            sentenceLen.append(len(res))\n",
    "            sentences.append(res)\n",
    "            labels.append(dp[ticker][day]['label'])\n",
    "\n",
    "    sentenceLen = np.array(sentenceLen)\n",
    "\n",
    "    for percent in [50, 70, 80, 90, 95, 99]:\n",
    "        print(\"Sentence length %d%% percentile: %d\" % (percent, np.percentile(sentenceLen, percent)))\n",
    "\n",
    "    # 2.4 limit the vocabulary size to e.g. 2000, and let the unkown words as the last one\n",
    "    sorted_word_idx_cnt = sorted(word_idx_cnt.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_map = {}\n",
    "    for idx, count in sorted_word_idx_cnt[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    # 2.5 map full dict to truncated dict, pad the sequence to the same length, done\n",
    "    sentences_small = []\n",
    "    new_label = []\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_map[idx] if idx in idx_new_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "            new_label.append(label)\n",
    "\n",
    "    pad_small = np.matrix(sequence.pad_sequences(sentences_small, maxlen=max_words))\n",
    "    pad_small = pad_small.astype('int').astype('str')\n",
    "    new_label = np.matrix(new_label).astype('str')\n",
    "\n",
    "    featureMatrix = np.concatenate((pad_small, new_label.T), axis=1)\n",
    "    np.savetxt('./input/featureMatrix.csv', featureMatrix, fmt=\"%s\")\n",
    "\n",
    "    # step 3, project the test feature in the word2idx for the traning data\n",
    "    dp = {}; cnt = 0\n",
    "    for file in input_files:\n",
    "        for line in open(loc + file):\n",
    "            line = line.strip().split(',')\n",
    "            if len(line) != 5: continue\n",
    "            ticker, name, day, headline, body = line\n",
    "            if day not in testDates: continue # this step only considers test set\n",
    "            if ticker not in priceDt: continue # continue if no corresponding prices information found\n",
    "            if day not in priceDt[ticker]: continue\n",
    "            # modify repeating news\n",
    "            if ticker not in dp: dp[ticker] = {}\n",
    "            if day not in dp[ticker]: dp[ticker][day] = {'feature':[], 'label':[]}\n",
    "            cnt += 1\n",
    "            tokens = nltk.word_tokenize(headline) + nltk.word_tokenize(body)\n",
    "            tokens = [unify_word(t) for t in tokens]\n",
    "            tokens = [t for t in tokens if t in stopWords]\n",
    "            #tokens = [t for t in tokens if t in topWords]\n",
    "            sentence_by_idx = [word2idx_small[t] for t in tokens if t in word2idx_small]\n",
    "            dp[ticker][day]['feature'].append(sentence_by_idx)\n",
    "            dp[ticker][day]['label'] = round(priceDt[ticker][day], 6)\n",
    "    print(\"test\", cnt)\n",
    "    sentences_test, labels_test = [], []\n",
    "    for ticker in dp:\n",
    "        for day in dp[ticker]:\n",
    "            res = []\n",
    "            for i in dp[ticker][day]['feature']: res += i\n",
    "            sentences_test.append(res)\n",
    "            labels_test.append(dp[ticker][day]['label'])\n",
    "\n",
    "    pad_test = np.matrix(sequence.pad_sequences(sentences_test, maxlen=max_words))\n",
    "    pad_test = pad_test.astype('int').astype('str')\n",
    "    labels_test = np.matrix(labels_test).astype('str')\n",
    "    featureMatrix = np.concatenate((pad_test, labels_test.T), axis=1)\n",
    "    np.savetxt('./input/featureMatrixTest.csv', featureMatrix, fmt=\"%s\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    build_FeatureMatrix(max_words=80)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
