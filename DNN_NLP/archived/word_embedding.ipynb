{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from utils import generate_past_n_days, unify_word\n",
    "\n",
    "\n",
    "# reference https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/glove.py\n",
    "\n",
    "class Glove(object):\n",
    "    def __init__(self, D, V, context_sz):\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.context_sz = context_sz\n",
    "\n",
    "    def fit(self, sentences, cc_matrix=None, learning_rate=10e-5, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=True):\n",
    "        # build co-occurrence matrix\n",
    "        # paper calls it X, so we will call it X, instead of calling\n",
    "        # the training data X\n",
    "        # TODO: would it be better to use a sparse matrix?\n",
    "        t0 = datetime.now()\n",
    "        V = self.V\n",
    "        D = self.D\n",
    "\n",
    "        if os.path.exists(cc_matrix):\n",
    "            X = np.load(cc_matrix)\n",
    "        else:\n",
    "            X = np.zeros((V, V))\n",
    "            N = len(sentences)\n",
    "            print \"number of sentences to process:\", N\n",
    "            it = 0\n",
    "            for sentence in sentences:\n",
    "                it += 1\n",
    "                if it % 10000 == 0:\n",
    "                    print \"processed\", it, \"/\", N\n",
    "                n = len(sentence)\n",
    "                for i in xrange(n):\n",
    "                    wi = sentence[i]\n",
    "\n",
    "                    start = max(0, i - self.context_sz)\n",
    "                    end = min(n, i + self.context_sz)\n",
    "\n",
    "                    # we can either choose only one side as context, or both\n",
    "                    # here we are doing both\n",
    "\n",
    "                    # make sure \"start\" and \"end\" tokens are part of some context\n",
    "                    # otherwise their f(X) will be 0 (denominator in bias update)\n",
    "                    if i - self.context_sz < 0:\n",
    "                        points = 1.0 / (i + 1)\n",
    "                        X[wi,0] += points\n",
    "                        X[0,wi] += points\n",
    "                    if i + self.context_sz > n:\n",
    "                        points = 1.0 / (n - i)\n",
    "                        X[wi,1] += points\n",
    "                        X[1,wi] += points\n",
    "\n",
    "                    for j in xrange(start, i):\n",
    "                        if j == i: continue\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / abs(i - j) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "            # save the cc matrix because it takes forever to create\n",
    "            np.save(cc_matrix, X)\n",
    "\n",
    "        print \"max in X:\", X.max()\n",
    "\n",
    "        # weighting\n",
    "        fX = np.zeros((V, V))\n",
    "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "        fX[X >= xmax] = 1\n",
    "\n",
    "        print \"max in f(X):\", fX.max()\n",
    "\n",
    "        # target\n",
    "        logX = np.log(X + 1)\n",
    "\n",
    "        print \"max in log(X):\", logX.max()\n",
    "\n",
    "        print \"time to build co-occurrence matrix:\", (datetime.now() - t0)\n",
    "\n",
    "        # initialize weights\n",
    "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        b = np.zeros(V)\n",
    "        U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        c = np.zeros(V)\n",
    "        mu = logX.mean()\n",
    "\n",
    "        if gd and use_theano:\n",
    "            thW = theano.shared(W)\n",
    "            thb = theano.shared(b)\n",
    "            thU = theano.shared(U)\n",
    "            thc = theano.shared(c)\n",
    "            thLogX = T.matrix('logX')\n",
    "            thfX = T.matrix('fX')\n",
    "\n",
    "            params = [thW, thb, thU, thc]\n",
    "\n",
    "            thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n",
    "            thCost = ( thfX * thDelta * thDelta ).sum()\n",
    "\n",
    "            grads = T.grad(thCost, params)\n",
    "\n",
    "            updates = [(p, p - learning_rate*g) for p, g in zip(params, grads)]\n",
    "\n",
    "            train_op = theano.function(\n",
    "                inputs=[thfX, thLogX],\n",
    "                updates=updates,\n",
    "            )\n",
    "\n",
    "        costs = []\n",
    "        sentence_indexes = range(len(sentences))\n",
    "        for epoch in xrange(epochs):\n",
    "            delta = W.dot(U.T) + b.reshape(V, 1) + c.reshape(1, V) + mu - logX\n",
    "            cost = ( fX * delta * delta ).sum()\n",
    "            costs.append(cost)\n",
    "            print \"epoch:\", epoch, \"cost:\", cost\n",
    "\n",
    "            if gd:\n",
    "                # gradient descent method\n",
    "\n",
    "                if use_theano:\n",
    "                    train_op(fX, logX)\n",
    "                    W = thW.get_value()\n",
    "                    b = thb.get_value()\n",
    "                    U = thU.get_value()\n",
    "                    c = thc.get_value()\n",
    "\n",
    "                else:\n",
    "                    # update W\n",
    "                    oldW = W.copy()\n",
    "                    for i in xrange(V):\n",
    "                        W[i] -= learning_rate*(fX[i,:]*delta[i,:]).dot(U)\n",
    "                    W -= learning_rate*reg*W\n",
    "\n",
    "                    # update b\n",
    "                    for i in xrange(V):\n",
    "                        b[i] -= learning_rate*fX[i,:].dot(delta[i,:])\n",
    "                    b -= learning_rate*reg*b\n",
    "\n",
    "                    # update U\n",
    "                    for j in xrange(V):\n",
    "                        U[j] -= learning_rate*(fX[:,j]*delta[:,j]).dot(oldW)\n",
    "                    U -= learning_rate*reg*U\n",
    "\n",
    "                    # update c\n",
    "                    for j in xrange(V):\n",
    "                        c[j] -= learning_rate*fX[:,j].dot(delta[:,j])\n",
    "                    c -= learning_rate*reg*c\n",
    "\n",
    "            else:\n",
    "                # ALS method\n",
    "\n",
    "                # update W\n",
    "                # fast way\n",
    "                # t0 = datetime.now()\n",
    "                for i in xrange(V):\n",
    "                    # matrix = reg*np.eye(D) + np.sum((fX[i,j]*np.outer(U[j], U[j]) for j in xrange(V)), axis=0)\n",
    "                    matrix = reg*np.eye(D) + (fX[i,:]*U.T).dot(U)\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 10e-5)\n",
    "                    vector = (fX[i,:]*(logX[i,:] - b[i] - c - mu)).dot(U)\n",
    "                    W[i] = np.linalg.solve(matrix, vector)\n",
    "                # print \"fast way took:\", (datetime.now() - t0)\n",
    "\n",
    "                # update b\n",
    "                for i in xrange(V):\n",
    "                    denominator = fX[i,:].sum()\n",
    "                    # assert(denominator > 0)\n",
    "                    numerator = fX[i,:].dot(logX[i,:] - W[i].dot(U.T) - c - mu)\n",
    "                    # for j in xrange(V):\n",
    "                    #     numerator += fX[i,j]*(logX[i,j] - W[i].dot(U[j]) - c[j])\n",
    "                    b[i] = numerator / denominator / (1 + reg)\n",
    "                # print \"updated b\"\n",
    "\n",
    "                # update U\n",
    "                for j in xrange(V):\n",
    "                    matrix = reg*np.eye(D) + (fX[:,j]*W.T).dot(W)\n",
    "                    vector = (fX[:,j]*(logX[:,j] - b - c[j] - mu)).dot(W)\n",
    "                    U[j] = np.linalg.solve(matrix, vector)\n",
    "\n",
    "                # update c\n",
    "                for j in xrange(V):\n",
    "                    denominator = fX[:,j].sum()\n",
    "                    numerator = fX[:,j].dot(logX[:,j] - W.dot(U[j]) - b  - mu)\n",
    "                    c[j] = numerator / denominator / (1 + reg)\n",
    "\n",
    "        self.W = W\n",
    "        self.U = U\n",
    "\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, fn):\n",
    "        # function word_analogies expects a (V,D) matrx and a (D,V) matrix\n",
    "        arrays = [self.W, self.U.T]\n",
    "        np.savez(fn, *arrays)\n",
    "\n",
    "def get_reuters_data(n_vocab):\n",
    "    # return variables\n",
    "    sentences = []\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    current_idx = 2\n",
    "    word_idx_count = {0: float('inf'), 1: float('inf')}\n",
    "    tag = 0\n",
    "    for field in reuters.fileids():\n",
    "        sentence = reuters.words(field)\n",
    "        tokens = [unify_word(t) for t in sentence]\n",
    "        for t in tokens:\n",
    "            if t not in word2idx:\n",
    "                word2idx[t] = current_idx\n",
    "                idx2word.append(t)\n",
    "                current_idx += 1\n",
    "            idx = word2idx[t]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "        sentence_by_idx = [word2idx[t] for t in tokens]\n",
    "        sentences.append(sentence_by_idx)\n",
    "        tag += 1\n",
    "        print(tag)\n",
    "\n",
    "    # restrict vocab size\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        print word, count\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small\n",
    "\n",
    "def main(we_file, w2i_file, sen):\n",
    "    cc_matrix = \"./input/cc_matrix.npy\"\n",
    "    if not os.path.isfile(w2i_file):\n",
    "        sentences, word2idx = get_reuters_data(n_vocab=2000)\n",
    "        with open(w2i_file, 'w') as f:\n",
    "            json.dump(word2idx, f)\n",
    "        with open(sen, 'w') as f:\n",
    "            json.dump(sentences, f)\n",
    "    else:\n",
    "        with open(w2i_file) as data_file:    \n",
    "            word2idx = json.load(data_file)\n",
    "        with open(sen) as data_file:    \n",
    "            sentences = json.load(data_file)\n",
    "\n",
    "    V = len(word2idx)\n",
    "    model = Glove(50, V, 10)\n",
    "    # model.fit(sentences, cc_matrix=cc_matrix, epochs=20) # ALS\n",
    "    model.fit(\n",
    "        sentences,\n",
    "        cc_matrix=cc_matrix,\n",
    "        learning_rate=3*10e-5,\n",
    "        reg=0.01,\n",
    "        epochs=2000,\n",
    "        gd=True,\n",
    "        use_theano=True\n",
    "    ) # gradient descent\n",
    "    model.save(we_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    we = './input/glove_model_50.npz'\n",
    "    w2i = './input/word2idx.json'\n",
    "    sen = './input/sentences.json'\n",
    "    main(we, w2i, sen)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
