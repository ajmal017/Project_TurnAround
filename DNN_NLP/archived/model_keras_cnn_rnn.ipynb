{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def value2Categorical(y, clusters=2):\n",
    "    label = np.copy(y)\n",
    "    label[y<np.percentile(y, 100/clusters)] = 0\n",
    "    for i in range(1, clusters):\n",
    "        label[y>np.percentile(y, 100*i/clusters)] = i\n",
    "    return label\n",
    "\n",
    "\n",
    "\n",
    "def get_Feature_Label(clusters=2, hasJunk=True):\n",
    "    data = np.genfromtxt('./input/featureMatrix_body.csv')\n",
    "    test = np.genfromtxt('./input/featureMatrixTest_body.csv')\n",
    "    np.random.shuffle(data)\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    label = to_categorical(value2Categorical(y, clusters)).astype(\"int\")\n",
    "\n",
    "    validation_ratio = 0.1\n",
    "    D = int(data.shape[0] * validation_ratio)  # total number of test data\n",
    "    X_train, y_train, X_valid, y_valid = X[:-D], label[:-D,:], X[-D:], label[-D:,:]\n",
    "\n",
    "    \n",
    "    X_test, y_test = test[:, :-1], test[:, -1]\n",
    "    y_test = to_categorical(value2Categorical(y_test, clusters)).astype(\"int\")\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def embeddingNN(shape, clusters=2, embedLayer=200, middle = 100):\n",
    "    top_words = 2001\n",
    "    lossType = 'binary_crossentropy' if clusters == 2 else 'categorical_crossentropy'\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedLayer, input_length=shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(middle, activation='relu'))\n",
    "    model.add(Dense(clusters, activation='sigmoid'))\n",
    "    model.compile(loss=lossType, optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def embeddingCNN(shape, clusters=2, embedLayer=200, middle = 100):\n",
    "    top_words = 2001\n",
    "    lossType = 'binary_crossentropy' if clusters == 2 else 'categorical_crossentropy'\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedLayer, input_length=shape))\n",
    "    model.add(Convolution1D(nb_filter=embedLayer, filter_length=3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(middle, activation='relu'))\n",
    "    model.add(Dense(clusters, activation='sigmoid'))\n",
    "    model.compile(loss=lossType, optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_selection(): # random sampling is better than grid search\n",
    "    sampling_Num = 5\n",
    "    clusters = 2\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = get_Feature_Label(clusters=clusters)\n",
    "    model_list = []\n",
    "    for embeddings in range(20, 200, 10):\n",
    "        for middle in range(100, 300, 5):\n",
    "            model_list.append((embeddings, middle))\n",
    "\n",
    "    # rand_smpl = [model_list[i] for i in sorted(random.sample(xrange(len(model_list)), sampling_Num))]\n",
    "    rand_smpl = [model_list[random.randint(0,len(model_list) - 1)] for i in range(sampling_Num)]\n",
    "    \n",
    "    performance = {}\n",
    "    cnt = {}\n",
    "    scores = {}\n",
    "    for pars in rand_smpl:\n",
    "        embeddings, middle = pars\n",
    "        model = embeddingNN(X_train.shape[1], clusters, embeddings, middle)\n",
    "        print(model.summary())\n",
    "        model.fit(X_train, y_train, validation_data=(X_valid, y_valid), nb_epoch=2, batch_size=128, verbose=2)\n",
    "        # Final evaluation of the model\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        # calculate predictions\n",
    "        predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "        conf = confusion_matrix(np.argmax(y_test, axis=-1), predictions)\n",
    "        print(\"Test on %d samples\" % (len(y_test)))\n",
    "        print(conf)\n",
    "        conf = np.array(conf)\n",
    "        for i in range(clusters):\n",
    "            print(\"Label %d Precision: %.2f%%\" % (i, conf[i,i] * 100.0 / sum(conf[:,i])))\n",
    "        performance[pars] = performance.get(pars, 0) + conf[i,i] * 100.0 / sum(conf[:,i])\n",
    "        cnt[pars] = cnt.get(pars, 0) + 1\n",
    "\n",
    "    for pars in cnt:\n",
    "        scores[pars] = round(performance[pars] / cnt[pars], 2)\n",
    "    sorted_Scores = sorted(scores.items(), key=operator.itemgetter(1))\n",
    "    for num, i in enumerate(sorted_Scores):\n",
    "        print(\"pars=%s, score, %.2f, trials=%d\" % (i[0], i[1], cnt[i[0]]))\n",
    "\n",
    "def main():\n",
    "    model_selection()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
