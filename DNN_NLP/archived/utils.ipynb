{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopted from https://github.com/lazyprogrammer/machine_learning_examples/blob/master/rnn_class/util.py\n",
    "# Adopted form https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/util.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import operator\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from datetime import datetime\n",
    "\n",
    "import time  # for debug\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "eng_stop = set(stopwords.words('english'))\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = remove_punctuation(s)\n",
    "    s = s.lower() # downcase\n",
    "    # remove stopwords\n",
    "    return [i for i in s.split() if i not in eng_stop]\n",
    "\n",
    "def get_wikipedia_data(filename, n_vocab, by_paragraph=False):\n",
    "    prefix = './input/'\n",
    "    # return variables\n",
    "    sentences = []\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    current_idx = 2\n",
    "    word_idx_count = {0: float('inf'), 1: float('inf')}\n",
    "\n",
    "    print \"reading:\", filename\n",
    "    for line in open(prefix + filename):\n",
    "        line = line.strip()\n",
    "        # don't count headers, structured data, lists, etc...\n",
    "        if line and line[0] not in ('[', '*', '-', '|', '=', '{', '}'):\n",
    "            if by_paragraph:\n",
    "                sentence_lines = [line]\n",
    "            else:\n",
    "                sentence_lines = line.split('. ')\n",
    "            for sentence in sentence_lines:\n",
    "                tokens = my_tokenizer(sentence)\n",
    "                for t in tokens:\n",
    "                    if t not in word2idx:\n",
    "                        word2idx[t] = current_idx\n",
    "                        idx2word.append(t)\n",
    "                        current_idx += 1\n",
    "                    idx = word2idx[t]\n",
    "                    word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "                sentence_by_idx = [word2idx[t] for t in tokens]\n",
    "                sentences.append(sentence_by_idx)\n",
    "\n",
    "    print '# of unique words: ', len(word2idx)\n",
    "\n",
    "    # restrict vocab size\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        print word, count\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    # assert('king' in word2idx_small)\n",
    "    # assert('queen' in word2idx_small)\n",
    "    # assert('man' in word2idx_small)\n",
    "    # assert('woman' in word2idx_small)\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small\n",
    "\n",
    "\n",
    "def find_analogies(w1, w2, w3, We, word2idx):\n",
    "    king = We[word2idx[w1]]\n",
    "    man = We[word2idx[w2]]\n",
    "    woman = We[word2idx[w3]]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    def dist1(a, b):\n",
    "        return np.linalg.norm(a - b)\n",
    "    def dist2(a, b):\n",
    "        return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    for dist, name in [(dist1, 'Euclidean'), (dist2, 'cosine')]:\n",
    "        min_dist = float('inf')\n",
    "        best_word = ''\n",
    "        for word, idx in word2idx.iteritems():\n",
    "            if word not in (w1, w2, w3):\n",
    "                v1 = We[idx]\n",
    "                d = dist(v0, v1)\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "                    best_word = word\n",
    "        print \"closest match by\", name, \"distance:\", best_word\n",
    "        print w1, \"-\", w2, \"=\", best_word, \"-\", w3    \n",
    "\n",
    "\n",
    "def get_news_data_with_price(filename, prefix='./input/'):\n",
    "    df = pd.read_csv(prefix+filename, header=None)\n",
    "    # use line numbers to check if data is filtered or not\n",
    "    lineNo = df.shape[0]\n",
    "    filtered_filename = './filtered/'+ str(lineNo) + '_' + filename\n",
    "    print 'try to read', filtered_filename\n",
    "    if os.path.isfile(filtered_filename):\n",
    "        df = pd.read_csv(filtered_filename)\n",
    "        data = df.as_matrix()\n",
    "        X = data[:, :-1]\n",
    "        Y = data[:, -1]\n",
    "        print 'Done!'\n",
    "        return X, Y\n",
    "    \n",
    "    # save if new\n",
    "    print \"filtered data doesn't exist, filter and save\"\n",
    "    df.columns = ['Ticker', 'Comp_name', 'Date', 'Title', 'Summary']\n",
    "    print df.head()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # get_wikipedia_data('file', 5000)\n",
    "    get_news_data_with_price('news_bloomberg_part0.csv')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
