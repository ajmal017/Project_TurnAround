{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Price Prediction (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejihoon6065/Project_TurnAround/blob/Hyundai/Price_Prediction_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XW5wuqkJc0W",
        "colab_type": "text"
      },
      "source": [
        "# Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:44.162598Z",
          "start_time": "2019-05-26T23:38:44.145254Z"
        },
        "id": "Qb_TCvJgJc0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "7abe48df-0147-42bf-ec29-9081e651314c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 49.5MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 58.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.35.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=e40924661df21bc8cc1ea01f80360b9e1a8be86d7a45f83d7e4cacd9eb731f91\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorboard, keras-applications, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpkEEz1MMpE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "555cce14-631f-46bf-8dac-44bc48e8d484"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.536401Z",
          "start_time": "2019-05-26T23:38:44.276741Z"
        },
        "id": "APTB_Z3NJc0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = 'embedding_files/'\n",
        "\n",
        "max_embedding = pd.read_json('/content/drive/My Drive/max_embedding.json')\n",
        "min_embedding = pd.read_json('/content/drive/My Drive/min_embedding.json')\n",
        "mean_embedding = pd.read_json('/content/drive/My Drive/mean_embedding.json')\n",
        "sum_embedding = pd.read_json('/content/drive/My Drive/sum_embedding.json')\n",
        "\n",
        "djia1 = pd.read_csv('/content/drive/My Drive/Combined_News_DJIA.csv')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPB3yKQoRLMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd4a5e0c-b59c-4afa-e431-9b7f05e98d6b"
      },
      "source": [
        "djia1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Label</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>0</td>\n",
              "      <td>b\"Georgia 'downs two Russian warplanes' as cou...</td>\n",
              "      <td>b'BREAKING: Musharraf to be impeached.'</td>\n",
              "      <td>b'Russia Today: Columns of troops roll into So...</td>\n",
              "      <td>b'Russian tanks are moving towards the capital...</td>\n",
              "      <td>b\"Afghan children raped with 'impunity,' U.N. ...</td>\n",
              "      <td>b'150 Russian tanks have entered South Ossetia...</td>\n",
              "      <td>b\"Breaking: Georgia invades South Ossetia, Rus...</td>\n",
              "      <td>b\"The 'enemy combatent' trials are nothing but...</td>\n",
              "      <td>b'Georgian troops retreat from S. Osettain cap...</td>\n",
              "      <td>b'Did the U.S. Prep Georgia for War with Russia?'</td>\n",
              "      <td>b'Rice Gives Green Light for Israel to Attack ...</td>\n",
              "      <td>b'Announcing:Class Action Lawsuit on Behalf of...</td>\n",
              "      <td>b\"So---Russia and Georgia are at war and the N...</td>\n",
              "      <td>b\"China tells Bush to stay out of other countr...</td>\n",
              "      <td>b'Did World War III start today?'</td>\n",
              "      <td>b'Georgia Invades South Ossetia - if Russia ge...</td>\n",
              "      <td>b'Al-Qaeda Faces Islamist Backlash'</td>\n",
              "      <td>b'Condoleezza Rice: \"The US would not act to p...</td>\n",
              "      <td>b'This is a busy day:  The European Union has ...</td>\n",
              "      <td>b\"Georgia will withdraw 1,000 soldiers from Ir...</td>\n",
              "      <td>b'Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
              "      <td>b'Caucasus in crisis: Georgia invades South Os...</td>\n",
              "      <td>b'Indian shoe manufactory  - And again in a se...</td>\n",
              "      <td>b'Visitors Suffering from Mental Illnesses Ban...</td>\n",
              "      <td>b\"No Help for Mexico's Kidnapping Surge\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>1</td>\n",
              "      <td>b'Why wont America and Nato help us? If they w...</td>\n",
              "      <td>b'Bush puts foot down on Georgian conflict'</td>\n",
              "      <td>b\"Jewish Georgian minister: Thanks to Israeli ...</td>\n",
              "      <td>b'Georgian army flees in disarray as Russians ...</td>\n",
              "      <td>b\"Olympic opening ceremony fireworks 'faked'\"</td>\n",
              "      <td>b'What were the Mossad with fraudulent New Zea...</td>\n",
              "      <td>b'Russia angered by Israeli military sale to G...</td>\n",
              "      <td>b'An American citizen living in S.Ossetia blam...</td>\n",
              "      <td>b'Welcome To World War IV! Now In High Definit...</td>\n",
              "      <td>b\"Georgia's move, a mistake of monumental prop...</td>\n",
              "      <td>b'Russia presses deeper into Georgia; U.S. say...</td>\n",
              "      <td>b'Abhinav Bindra wins first ever Individual Ol...</td>\n",
              "      <td>b' U.S. ship heads for Arctic to define territ...</td>\n",
              "      <td>b'Drivers in a Jerusalem taxi station threaten...</td>\n",
              "      <td>b'The French Team is Stunned by Phelps and the...</td>\n",
              "      <td>b'Israel and the US behind the Georgian aggres...</td>\n",
              "      <td>b'\"Do not believe TV, neither Russian nor Geor...</td>\n",
              "      <td>b'Riots are still going on in Montreal (Canada...</td>\n",
              "      <td>b'China to overtake US as largest manufacturer'</td>\n",
              "      <td>b'War in South Ossetia [PICS]'</td>\n",
              "      <td>b'Israeli Physicians Group Condemns State Tort...</td>\n",
              "      <td>b' Russia has just beaten the United States ov...</td>\n",
              "      <td>b'Perhaps *the* question about the Georgia - R...</td>\n",
              "      <td>b'Russia is so much better at war'</td>\n",
              "      <td>b\"So this is what it's come to: trading sex fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-08-12</td>\n",
              "      <td>0</td>\n",
              "      <td>b'Remember that adorable 9-year-old who sang a...</td>\n",
              "      <td>b\"Russia 'ends Georgia operation'\"</td>\n",
              "      <td>b'\"If we had no sexual harassment we would hav...</td>\n",
              "      <td>b\"Al-Qa'eda is losing support in Iraq because ...</td>\n",
              "      <td>b'Ceasefire in Georgia: Putin Outmaneuvers the...</td>\n",
              "      <td>b'Why Microsoft and Intel tried to kill the XO...</td>\n",
              "      <td>b'Stratfor: The Russo-Georgian War and the Bal...</td>\n",
              "      <td>b\"I'm Trying to Get a Sense of This Whole Geor...</td>\n",
              "      <td>b\"The US military was surprised by the timing ...</td>\n",
              "      <td>b'U.S. Beats War Drum as Iran Dumps the Dollar'</td>\n",
              "      <td>b'Gorbachev: \"Georgian military attacked the S...</td>\n",
              "      <td>b'CNN use footage of Tskhinvali ruins to cover...</td>\n",
              "      <td>b'Beginning a war as the Olympics were opening...</td>\n",
              "      <td>b'55 pyramids as large as the Luxor stacked in...</td>\n",
              "      <td>b'The 11 Top Party Cities in the World'</td>\n",
              "      <td>b'U.S. troops still in Georgia (did you know t...</td>\n",
              "      <td>b'Why Russias response to Georgia was right'</td>\n",
              "      <td>b'Gorbachev accuses U.S. of making a \"serious ...</td>\n",
              "      <td>b'Russia, Georgia, and NATO: Cold War Two'</td>\n",
              "      <td>b'Remember that adorable 62-year-old who led y...</td>\n",
              "      <td>b'War in Georgia: The Israeli connection'</td>\n",
              "      <td>b'All signs point to the US encouraging Georgi...</td>\n",
              "      <td>b'Christopher King argues that the US and NATO...</td>\n",
              "      <td>b'America: The New Mexico?'</td>\n",
              "      <td>b\"BBC NEWS | Asia-Pacific | Extinction 'by man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-08-13</td>\n",
              "      <td>0</td>\n",
              "      <td>b' U.S. refuses Israel weapons to attack Iran:...</td>\n",
              "      <td>b\"When the president ordered to attack Tskhinv...</td>\n",
              "      <td>b' Israel clears troops who killed Reuters cam...</td>\n",
              "      <td>b'Britain\\'s policy of being tough on drugs is...</td>\n",
              "      <td>b'Body of 14 year old found in trunk; Latest (...</td>\n",
              "      <td>b'China has moved 10 *million* quake survivors...</td>\n",
              "      <td>b\"Bush announces Operation Get All Up In Russi...</td>\n",
              "      <td>b'Russian forces sink Georgian ships '</td>\n",
              "      <td>b\"The commander of a Navy air reconnaissance s...</td>\n",
              "      <td>b\"92% of CNN readers: Russia's actions in Geor...</td>\n",
              "      <td>b'USA to send fleet into Black Sea to help Geo...</td>\n",
              "      <td>b\"US warns against Israeli plan to strike agai...</td>\n",
              "      <td>b\"In an intriguing cyberalliance, two Estonian...</td>\n",
              "      <td>b'The CNN Effect: Georgia Schools Russia in In...</td>\n",
              "      <td>b'Why Russias response to Georgia was right'</td>\n",
              "      <td>b'Elephants extinct by 2020?'</td>\n",
              "      <td>b'US humanitarian missions soon in Georgia - i...</td>\n",
              "      <td>b\"Georgia's DDOS came from US sources\"</td>\n",
              "      <td>b'Russian convoy heads into Georgia, violating...</td>\n",
              "      <td>b'Israeli defence minister: US against strike ...</td>\n",
              "      <td>b'Gorbachev: We Had No Choice'</td>\n",
              "      <td>b'Witness: Russian forces head towards Tbilisi...</td>\n",
              "      <td>b' Quarter of Russians blame U.S. for conflict...</td>\n",
              "      <td>b'Georgian president  says US military will ta...</td>\n",
              "      <td>b'2006: Nobel laureate Aleksander Solzhenitsyn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-08-14</td>\n",
              "      <td>1</td>\n",
              "      <td>b'All the experts admit that we should legalis...</td>\n",
              "      <td>b'War in South Osetia - 89 pictures made by a ...</td>\n",
              "      <td>b'Swedish wrestler Ara Abrahamian throws away ...</td>\n",
              "      <td>b'Russia exaggerated the death toll in South O...</td>\n",
              "      <td>b'Missile That Killed 9 Inside Pakistan May Ha...</td>\n",
              "      <td>b\"Rushdie Condemns Random House's Refusal to P...</td>\n",
              "      <td>b'Poland and US agree to missle defense deal. ...</td>\n",
              "      <td>b'Will the Russians conquer Tblisi? Bet on it,...</td>\n",
              "      <td>b'Russia exaggerating South Ossetian death tol...</td>\n",
              "      <td>b' Musharraf expected to resign rather than fa...</td>\n",
              "      <td>b'Moscow Made Plans Months Ago to Invade Georgia'</td>\n",
              "      <td>b'Why Russias response to Georgia was right'</td>\n",
              "      <td>b'Nigeria has handed over the potentially oil-...</td>\n",
              "      <td>b'The US and Poland have agreed a preliminary ...</td>\n",
              "      <td>b'Russia apparently is sabotaging infrastructu...</td>\n",
              "      <td>b'Bank analyst forecast Georgian crisis 2 days...</td>\n",
              "      <td>b\"Georgia confict could set back Russia's US r...</td>\n",
              "      <td>b'War in the Caucasus is as much the product o...</td>\n",
              "      <td>b'\"Non-media\" photos of South Ossetia/Georgia ...</td>\n",
              "      <td>b'Georgian TV reporter shot by Russian sniper ...</td>\n",
              "      <td>b'Saudi Arabia: Mother moves to block child ma...</td>\n",
              "      <td>b'Taliban wages war on humanitarian aid workers'</td>\n",
              "      <td>b'Russia: World  \"can forget about\" Georgia\\'s...</td>\n",
              "      <td>b'Darfur rebels accuse Sudan of mounting major...</td>\n",
              "      <td>b'Philippines : Peace Advocate say Muslims nee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1984</th>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>0</td>\n",
              "      <td>Barclays and RBS shares suspended from trading...</td>\n",
              "      <td>Pope says Church should ask forgiveness from g...</td>\n",
              "      <td>Poland 'shocked' by xenophobic abuse of Poles ...</td>\n",
              "      <td>There will be no second referendum, cabinet ag...</td>\n",
              "      <td>Scotland welcome to join EU, Merkel ally says</td>\n",
              "      <td>Sterling dips below Friday's 31-year low amid ...</td>\n",
              "      <td>No negative news about South African President...</td>\n",
              "      <td>Surge in Hate Crimes in the U.K. Following U.K...</td>\n",
              "      <td>Weapons shipped into Jordan by the CIA and Sau...</td>\n",
              "      <td>Angela Merkel said the U.K. must file exit pap...</td>\n",
              "      <td>In a birth offering hope to a threatened speci...</td>\n",
              "      <td>Sky News Journalist Left Speechless As Leave M...</td>\n",
              "      <td>Giant panda in Macau gives birth to twins</td>\n",
              "      <td>Get out now: EU leader tells Britain it must i...</td>\n",
              "      <td>Sea turtle 'beaten and left for dead' on beach...</td>\n",
              "      <td>German lawyers to probe Erdogan over alleged w...</td>\n",
              "      <td>Boris Johnson says the UK will continue to \"in...</td>\n",
              "      <td>Richard Branson is calling on the UK governmen...</td>\n",
              "      <td>Turkey 'sorry for downing Russian jet'</td>\n",
              "      <td>Edward Snowden lawyer vows new push for pardon...</td>\n",
              "      <td>Brexit opinion poll reveals majority don't wan...</td>\n",
              "      <td>Conservative MP Leave Campaigner: \"The leave c...</td>\n",
              "      <td>Economists predict UK recession, further weake...</td>\n",
              "      <td>New EU 'superstate plan by France, Germany: Cr...</td>\n",
              "      <td>Pakistani clerics declare transgender marriage...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985</th>\n",
              "      <td>2016-06-28</td>\n",
              "      <td>1</td>\n",
              "      <td>2,500 Scientists To Australia: If You Want To ...</td>\n",
              "      <td>The personal details of 112,000 French police ...</td>\n",
              "      <td>S&amp;amp;P cuts United Kingdom sovereign credit r...</td>\n",
              "      <td>Huge helium deposit found in Africa</td>\n",
              "      <td>CEO of the South African state broadcaster qui...</td>\n",
              "      <td>Brexit cost investors $2 trillion, the worst o...</td>\n",
              "      <td>Hong Kong democracy activists call for return ...</td>\n",
              "      <td>Brexit: Iceland president says UK can join 'tr...</td>\n",
              "      <td>UK's Osborne: 'Absolutely' going to have to cu...</td>\n",
              "      <td>'Do not let Scotland down now' : Scottish MEP ...</td>\n",
              "      <td>British pound could hit history-making dollar ...</td>\n",
              "      <td>Merkel vows to strengthen EU, tells UK no 'che...</td>\n",
              "      <td>\"Ryanair will not deploy new aircraft on route...</td>\n",
              "      <td>People, ever more greedy and stupid, destroy t...</td>\n",
              "      <td>Siemens freezes new UK wind power investment f...</td>\n",
              "      <td>US, Canada and Mexico pledge 50% of power from...</td>\n",
              "      <td>There is increasing evidence that Australia is...</td>\n",
              "      <td>Richard Branson, the founder of Virgin Group, ...</td>\n",
              "      <td>37,000-yr-old skull from Borneo reveals surpri...</td>\n",
              "      <td>Palestinians stone Western Wall worshipers; po...</td>\n",
              "      <td>Jean-Claude Juncker asks Farage: Why are you h...</td>\n",
              "      <td>\"Romanians for Remainians\" offering a new home...</td>\n",
              "      <td>Brexit: Gibraltar in talks with Scotland to st...</td>\n",
              "      <td>8 Suicide Bombers Strike Lebanon</td>\n",
              "      <td>Mexico's security forces routinely use 'sexual...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1986</th>\n",
              "      <td>2016-06-29</td>\n",
              "      <td>1</td>\n",
              "      <td>Explosion At Airport In Istanbul</td>\n",
              "      <td>Yemeni former president: Terrorism is the offs...</td>\n",
              "      <td>UK must accept freedom of movement to access E...</td>\n",
              "      <td>Devastated: scientists too late to captive bre...</td>\n",
              "      <td>British Labor Party leader Jeremy Corbyn loses...</td>\n",
              "      <td>A Muslim Shop in the UK Was Just Firebombed Wh...</td>\n",
              "      <td>Mexican Authorities Sexually Torture Women in ...</td>\n",
              "      <td>UK shares and pound continue to recover</td>\n",
              "      <td>Iceland historian Johannesson wins presidentia...</td>\n",
              "      <td>99-Million-Yr-Old Bird Wings Found Encased in ...</td>\n",
              "      <td>A chatbot programmed by a British teenager has...</td>\n",
              "      <td>The Philippine president-elect said Monday he ...</td>\n",
              "      <td>Former Belgian Prime Minister ridicules Nigel ...</td>\n",
              "      <td>Brexiteer Nigel Farage To EU: 'You're Not Laug...</td>\n",
              "      <td>Islamic State bombings in southern Yemen kill ...</td>\n",
              "      <td>Escape Tunnel, Dug by Hand, Is Found at Holoca...</td>\n",
              "      <td>The land under Beijing is sinking by as much a...</td>\n",
              "      <td>Car bomb and Anti-Islamic attack on Mosque in ...</td>\n",
              "      <td>Emaciated lions in Taiz Zoo are trapped in blo...</td>\n",
              "      <td>Rupert Murdoch describes Brexit as 'wonderful'...</td>\n",
              "      <td>More than 40 killed in Yemen suicide attacks</td>\n",
              "      <td>Google Found Disastrous Symantec and Norton Vu...</td>\n",
              "      <td>Extremist violence on the rise in Germany: Dom...</td>\n",
              "      <td>BBC News: Labour MPs pass Corbyn no-confidence...</td>\n",
              "      <td>Tiny New Zealand town with 'too many jobs' lau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>1</td>\n",
              "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
              "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
              "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
              "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
              "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
              "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
              "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
              "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
              "      <td>Afghan suicide bomber 'kills up to 40' - BBC News</td>\n",
              "      <td>US airstrikes kill at least 250 ISIS fighters ...</td>\n",
              "      <td>Turkish Cop Who Took Down Istanbul Gunman Hail...</td>\n",
              "      <td>Cannabis compounds could treat Alzheimer's by ...</td>\n",
              "      <td>Japan's top court has approved blanket surveil...</td>\n",
              "      <td>CIA Gave Romania Millions to Host Secret Prisons</td>\n",
              "      <td>Groups urge U.N. to suspend Saudi Arabia from ...</td>\n",
              "      <td>Googles free wifi at Indian railway stations i...</td>\n",
              "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
              "      <td>The men who carried out Tuesday's terror attac...</td>\n",
              "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
              "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
              "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
              "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
              "      <td>We will be swimming in ridicule - French beach...</td>\n",
              "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
              "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>1</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "      <td>Brazil: Huge spike in number of police killing...</td>\n",
              "      <td>Austria's highest court annuls presidential el...</td>\n",
              "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
              "      <td>Switzerland denies Muslim girls citizenship af...</td>\n",
              "      <td>China kills millions of innocent meditators fo...</td>\n",
              "      <td>France Cracks Down on Factory Farms - A viral ...</td>\n",
              "      <td>Abbas PLO Faction Calls Killer of 13-Year-Old ...</td>\n",
              "      <td>Taiwanese warship accidentally fires missile t...</td>\n",
              "      <td>Iran celebrates American Human Rights Week, mo...</td>\n",
              "      <td>U.N. panel moves to curb bias against L.G.B.T....</td>\n",
              "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
              "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
              "      <td>India gets $1 billion loan from World Bank for...</td>\n",
              "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
              "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
              "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
              "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
              "      <td>Venezuela, where anger over food shortages is ...</td>\n",
              "      <td>A Hindu temple worker has been killed by three...</td>\n",
              "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1989 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date  ...                                              Top25\n",
              "0     2008-08-08  ...           b\"No Help for Mexico's Kidnapping Surge\"\n",
              "1     2008-08-11  ...  b\"So this is what it's come to: trading sex fo...\n",
              "2     2008-08-12  ...  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...\n",
              "3     2008-08-13  ...  b'2006: Nobel laureate Aleksander Solzhenitsyn...\n",
              "4     2008-08-14  ...  b'Philippines : Peace Advocate say Muslims nee...\n",
              "...          ...  ...                                                ...\n",
              "1984  2016-06-27  ...  Pakistani clerics declare transgender marriage...\n",
              "1985  2016-06-28  ...  Mexico's security forces routinely use 'sexual...\n",
              "1986  2016-06-29  ...  Tiny New Zealand town with 'too many jobs' lau...\n",
              "1987  2016-06-30  ...  Law Enforcement Sources: Gun Used in Paris Ter...\n",
              "1988  2016-07-01  ...  Ozone layer hole seems to be healing - US &amp...\n",
              "\n",
              "[1989 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-JeF-GtOw7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "d6f8618c-c5bb-4f8e-c60a-f57ddc388120"
      },
      "source": [
        "# !pip uninstall pandas\n",
        "# !pip install pandas==1.0.4"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling pandas-1.0.5:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/pandas-1.0.5.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/pandas/*\n",
            "Proceed (y/n)? n\n",
            "Collecting pandas==1.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/86/c14387d6813ebadb7bf61b9ad270ffff111c8b587e4d266e07de774e385e/pandas-1.0.4-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.4) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.4) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.4) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.4) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "Successfully installed pandas-1.0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcc4L2_WOKbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "161f7739-4cbc-400e-9612-a3b7a7f61242"
      },
      "source": [
        "#djia = djia1.loc[:, ['Date', 'Open', 'Adj Close']].sort_values('Date').set_index('Date')\n",
        "djia = djia1.loc[:, ['Date']].sort_values('Date').set_index('Date')\n",
        "djia"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-12</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-13</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-14</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-06-27</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-06-28</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-06-29</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-06-30</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-07-01</th>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1989 rows × 0 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: [2008-08-08, 2008-08-11, 2008-08-12, 2008-08-13, 2008-08-14, 2008-08-15, 2008-08-18, 2008-08-19, 2008-08-20, 2008-08-21, 2008-08-22, 2008-08-25, 2008-08-26, 2008-08-27, 2008-08-28, 2008-08-29, 2008-09-02, 2008-09-03, 2008-09-04, 2008-09-05, 2008-09-08, 2008-09-09, 2008-09-10, 2008-09-11, 2008-09-12, 2008-09-15, 2008-09-16, 2008-09-17, 2008-09-18, 2008-09-19, 2008-09-22, 2008-09-23, 2008-09-24, 2008-09-25, 2008-09-26, 2008-09-29, 2008-09-30, 2008-10-01, 2008-10-02, 2008-10-03, 2008-10-06, 2008-10-07, 2008-10-08, 2008-10-09, 2008-10-10, 2008-10-13, 2008-10-14, 2008-10-15, 2008-10-16, 2008-10-17, 2008-10-20, 2008-10-21, 2008-10-22, 2008-10-23, 2008-10-24, 2008-10-27, 2008-10-28, 2008-10-29, 2008-10-30, 2008-10-31, 2008-11-03, 2008-11-04, 2008-11-05, 2008-11-06, 2008-11-07, 2008-11-10, 2008-11-11, 2008-11-12, 2008-11-13, 2008-11-14, 2008-11-17, 2008-11-18, 2008-11-19, 2008-11-20, 2008-11-21, 2008-11-24, 2008-11-25, 2008-11-26, 2008-11-28, 2008-12-01, 2008-12-02, 2008-12-03, 2008-12-04, 2008-12-05, 2008-12-08, 2008-12-09, 2008-12-10, 2008-12-11, 2008-12-12, 2008-12-15, 2008-12-16, 2008-12-17, 2008-12-18, 2008-12-19, 2008-12-22, 2008-12-23, 2008-12-24, 2008-12-26, 2008-12-29, 2008-12-30, ...]\n",
              "\n",
              "[1989 rows x 0 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3mTOP8vSZWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_names=['Open', 'Adj Close']\n",
        "\n",
        "djia = djia.reindex(columns = column_names)\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46qskuq5Sr87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "fd807317-950c-44ea-dee8-920153796b1b"
      },
      "source": [
        "djia"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-06-27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-06-28</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-06-29</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-06-30</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-07-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Open  Adj Close\n",
              "Date                       \n",
              "2008-08-08   NaN        NaN\n",
              "2008-08-11   NaN        NaN\n",
              "2008-08-12   NaN        NaN\n",
              "2008-08-13   NaN        NaN\n",
              "2008-08-14   NaN        NaN\n",
              "...          ...        ...\n",
              "2016-06-27   NaN        NaN\n",
              "2016-06-28   NaN        NaN\n",
              "2016-06-29   NaN        NaN\n",
              "2016-06-30   NaN        NaN\n",
              "2016-07-01   NaN        NaN\n",
              "\n",
              "[1989 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQJIf5VJc0f",
        "colab_type": "text"
      },
      "source": [
        "I only needed Date, Open, and Adj Close columns from the djia data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.560248Z",
          "start_time": "2019-05-26T23:38:45.543235Z"
        },
        "id": "5V0_gGeRJc0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "open_price = djia[['Open']]\n",
        "adj_close_price = djia[['Adj Close']]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.583538Z",
          "start_time": "2019-05-26T23:38:45.564590Z"
        },
        "id": "n9Qg9gTkJc0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "60a39998-9435-4277-b302-03f488a4529e"
      },
      "source": [
        "djia.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Open  Adj Close\n",
              "Date                       \n",
              "2008-08-08   NaN        NaN\n",
              "2008-08-11   NaN        NaN\n",
              "2008-08-12   NaN        NaN\n",
              "2008-08-13   NaN        NaN\n",
              "2008-08-14   NaN        NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.620754Z",
          "start_time": "2019-05-26T23:38:45.587089Z"
        },
        "id": "bDfVMvURJc0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "cb249f59-1cef-4bc3-8a68-0e0c6155732f"
      },
      "source": [
        "max_embedding.head(1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>[0.809297204, 0.5163459778, 0.3755577505, 0.59...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          Max\n",
              "2008-08-08  [0.809297204, 0.5163459778, 0.3755577505, 0.59..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF233oNJJc0r",
        "colab_type": "text"
      },
      "source": [
        "Since each value in the list is a feature, I redefined the dataframe by separating them into each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.636433Z",
          "start_time": "2019-05-26T23:38:45.625200Z"
        },
        "id": "aGvuK9NYJc0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_data(tbl):\n",
        "    \n",
        "    tbl = pd.DataFrame(tbl.iloc[:, 0].tolist())\n",
        "    tbl = tbl.set_index(djia.index)\n",
        "\n",
        "    return tbl"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.826153Z",
          "start_time": "2019-05-26T23:38:45.640278Z"
        },
        "id": "EtA82mDTJc0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_embedding = transform_data(max_embedding)\n",
        "\n",
        "min_embedding = transform_data(min_embedding)\n",
        "\n",
        "sum_embedding = transform_data(sum_embedding)\n",
        "\n",
        "mean_embedding = transform_data(mean_embedding)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.888134Z",
          "start_time": "2019-05-26T23:38:47.828345Z"
        },
        "id": "QROEelC2Jc0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "8080cd5e-728a-4407-8501-6d79974a15e1"
      },
      "source": [
        "max_embedding.head(1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>0.809297</td>\n",
              "      <td>0.516346</td>\n",
              "      <td>0.375558</td>\n",
              "      <td>0.592091</td>\n",
              "      <td>0.372241</td>\n",
              "      <td>0.27578</td>\n",
              "      <td>0.672928</td>\n",
              "      <td>0.902444</td>\n",
              "      <td>1.321722</td>\n",
              "      <td>0.690093</td>\n",
              "      <td>0.672894</td>\n",
              "      <td>0.605886</td>\n",
              "      <td>0.665229</td>\n",
              "      <td>1.167292</td>\n",
              "      <td>0.292562</td>\n",
              "      <td>0.694127</td>\n",
              "      <td>0.4684</td>\n",
              "      <td>0.443298</td>\n",
              "      <td>0.443909</td>\n",
              "      <td>0.66092</td>\n",
              "      <td>0.285502</td>\n",
              "      <td>0.332785</td>\n",
              "      <td>0.343287</td>\n",
              "      <td>0.528204</td>\n",
              "      <td>0.840047</td>\n",
              "      <td>0.885478</td>\n",
              "      <td>0.576059</td>\n",
              "      <td>1.648164</td>\n",
              "      <td>0.869075</td>\n",
              "      <td>0.820374</td>\n",
              "      <td>0.654135</td>\n",
              "      <td>0.46944</td>\n",
              "      <td>0.341206</td>\n",
              "      <td>0.548649</td>\n",
              "      <td>1.005088</td>\n",
              "      <td>0.575558</td>\n",
              "      <td>0.66084</td>\n",
              "      <td>1.020263</td>\n",
              "      <td>0.473604</td>\n",
              "      <td>0.824405</td>\n",
              "      <td>...</td>\n",
              "      <td>0.311239</td>\n",
              "      <td>0.520639</td>\n",
              "      <td>0.264339</td>\n",
              "      <td>0.056403</td>\n",
              "      <td>0.453568</td>\n",
              "      <td>0.893871</td>\n",
              "      <td>0.511386</td>\n",
              "      <td>0.81734</td>\n",
              "      <td>1.364261</td>\n",
              "      <td>0.346905</td>\n",
              "      <td>0.855979</td>\n",
              "      <td>0.744539</td>\n",
              "      <td>0.415343</td>\n",
              "      <td>0.067126</td>\n",
              "      <td>0.558187</td>\n",
              "      <td>0.452133</td>\n",
              "      <td>0.412646</td>\n",
              "      <td>0.732482</td>\n",
              "      <td>0.864727</td>\n",
              "      <td>0.577902</td>\n",
              "      <td>0.391317</td>\n",
              "      <td>0.412251</td>\n",
              "      <td>0.784168</td>\n",
              "      <td>0.486302</td>\n",
              "      <td>0.710385</td>\n",
              "      <td>0.339733</td>\n",
              "      <td>0.693118</td>\n",
              "      <td>0.671879</td>\n",
              "      <td>0.745116</td>\n",
              "      <td>0.309403</td>\n",
              "      <td>0.414205</td>\n",
              "      <td>0.687436</td>\n",
              "      <td>0.144865</td>\n",
              "      <td>0.403365</td>\n",
              "      <td>0.304636</td>\n",
              "      <td>0.796824</td>\n",
              "      <td>0.586465</td>\n",
              "      <td>0.883279</td>\n",
              "      <td>0.854595</td>\n",
              "      <td>0.175066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0         1         2    ...       765       766       767\n",
              "Date                                      ...                              \n",
              "2008-08-08  0.809297  0.516346  0.375558  ...  0.883279  0.854595  0.175066\n",
              "\n",
              "[1 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.902602Z",
          "start_time": "2019-05-26T23:38:47.892407Z"
        },
        "id": "TKBxyAPEJc02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21a0d0a3-e1f9-4f51-eb92-9f28e5db591a"
      },
      "source": [
        "max_embedding.shape, open_price.shape, adj_close_price.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1989, 768), (1989, 1), (1989, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERgziJUbJc05",
        "colab_type": "text"
      },
      "source": [
        "I separated them into testing and training next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.916449Z",
          "start_time": "2019-05-26T23:38:47.906161Z"
        },
        "id": "2N9vIejyJc06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_test(embedding, test_size):\n",
        "    \n",
        "    embedding_test = embedding.iloc[-test_size:, :]\n",
        "    embedding = embedding.iloc[:-test_size, :]\n",
        "    \n",
        "    return embedding_test, embedding"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.952839Z",
          "start_time": "2019-05-26T23:38:47.919527Z"
        },
        "id": "DZKnAAt5Jc1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_size = 300\n",
        "\n",
        "max_embedding_test, max_embedding = split_test(max_embedding, test_size)\n",
        "min_embedding_test, min_embedding = split_test(min_embedding, test_size)\n",
        "sum_embedding_test, sum_embedding = split_test(sum_embedding, test_size)\n",
        "mean_embedding_test, mean_embedding = split_test(mean_embedding, test_size)\n",
        "\n",
        "combined_embedding = pd.concat((mean_embedding, max_embedding, min_embedding, sum_embedding), axis=1)\n",
        "combined_embedding_test = pd.concat((mean_embedding_test, max_embedding_test, min_embedding_test, sum_embedding_test), axis=1)\n",
        "\n",
        "open_test, open_price = split_test(open_price, test_size)\n",
        "adj_close_test, adj_close_price = split_test(adj_close_price, test_size)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:50.784333Z",
          "start_time": "2019-05-26T23:38:50.767288Z"
        },
        "id": "eVhPTEU0Jc1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b4d10e8-7280-44a1-bdec-cf9b32363b06"
      },
      "source": [
        "max_embedding.shape, combined_embedding.shape, open_price.shape, adj_close_price.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1689, 768), (1689, 3072), (1689, 1), (1689, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w30xvCquJc1I",
        "colab_type": "text"
      },
      "source": [
        "<b>combined_embedding</b> is another dataset I tried to see if how using all of their features affect the open price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBZnuhFsJc1I",
        "colab_type": "text"
      },
      "source": [
        "So now there are total of 5 different models with different data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_a03Y98Jc1J",
        "colab_type": "text"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmWwxq2dJc1J",
        "colab_type": "text"
      },
      "source": [
        "Finally, below is my custom data loader and model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T22:57:50.404182Z",
          "start_time": "2019-05-26T22:57:50.334600Z"
        },
        "id": "jyugYO9dJc1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "d711f499-ddd4-405a-8443-b421eea3ca00"
      },
      "source": [
        "!pip install tensorflow==1.15.0rc2\n",
        "\n",
        "def data_loader(data, batch_size, num_iter=100):\n",
        "    \n",
        "    # x : Embedding Values\n",
        "    # y : Open Price\n",
        "    # z : Close Price\n",
        "    \n",
        "    x = data[0]\n",
        "    y = data[1]\n",
        "    z = data[2]\n",
        "    \n",
        "    # num_iter iterations per epoch\n",
        "    # mini batch\n",
        "    \n",
        "    for _ in range(num_iter):\n",
        "    \n",
        "        idx = np.random.choice(np.arange(x.shape[0]), size=batch_size, replace=False)\n",
        "\n",
        "        batch_x = x.iloc[idx, :]\n",
        "        batch_y = y.iloc[idx]\n",
        "        batch_z = z.iloc[idx]\n",
        "        \n",
        "        yield batch_x, batch_y, batch_z\n",
        "\n",
        "\n",
        "     \n",
        "        \n",
        "class get_model():\n",
        "    \n",
        "    def __init__(self, learning_rate=1e-3, dropout_rate=.5):\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # BERT Embedding\n",
        "        self.x = tf.placeholder(tf.float32, shape=(None, 768))\n",
        "        # Open Price\n",
        "        self.y = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        # Adj Close Price\n",
        "        self.z = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.pred = self.run_model()\n",
        "        \n",
        "        self.loss = tf.sqrt(tf.losses.mean_squared_error(self.z, self.pred), name='loss')\n",
        "        \n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='optimizer').minimize(self.loss)\n",
        "        \n",
        "        self.saver = tf.train.Saver()\n",
        "        \n",
        "    def run_model(self):\n",
        "        \n",
        "        # Dense model\n",
        "        layer1 = tf.contrib.layers.fully_connected(self.x, 1000)\n",
        "        layer1 = tf.nn.dropout(layer1, rate=self.dropout_rate)\n",
        "        layer1 = tf.layers.batch_normalization(layer1)\n",
        "        \n",
        "        layer2 = tf.contrib.layers.fully_connected(layer1, 500)\n",
        "        layer2 = tf.nn.dropout(layer2, rate=self.dropout_rate)\n",
        "        layer2 = tf.layers.batch_normalization(layer2)\n",
        "        \n",
        "        # This would be the value of coefficient indicating how much it impacts on a day's open price\n",
        "        layer3 = tf.contrib.layers.fully_connected(layer2, 1)\n",
        "        \n",
        "        layer4 = layer3 * self.y\n",
        "        \n",
        "        layer5 = tf.contrib.layers.fully_connected(layer4, 100)\n",
        "        layer5 = tf.nn.dropout(layer5, rate=self.dropout_rate)\n",
        "        layer5 = tf.layers.batch_normalization(layer5)\n",
        "        \n",
        "        output = tf.contrib.layers.fully_connected(layer5, 1)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/bf/ee59aeef074bcecf7421380b3ed6c6880c04c059acee72cb8591beeaad91/tensorflow-1.15.0rc2-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.32.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (0.35.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.15.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (0.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0rc2) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0rc2) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0rc2) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0rc2) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0rc2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0rc2) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0rc2) (3.1.0)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorflow-1.15.0rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y6M48owJc1M",
        "colab_type": "text"
      },
      "source": [
        "From first to third layer is to extract the value that indicates how much given articles affect the same day's open price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1udlkdptJc1M",
        "colab_type": "text"
      },
      "source": [
        "<b>get_data</b> below concatenates embedding with open price and split them into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:03:01.893010Z",
          "start_time": "2019-05-26T23:03:01.879465Z"
        },
        "id": "ezup6FZ3Jc1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(embedding):\n",
        "    \n",
        "    X = pd.concat((embedding, open_price), axis=1)\n",
        "    \n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X, adj_close_price, test_size=.2)\n",
        "    \n",
        "    return [X_train.iloc[:, :-1], X_train.iloc[:, -1:], y_train], [X_valid.iloc[:, :-1], X_valid.iloc[:, -1:], y_valid]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:03:02.752152Z",
          "start_time": "2019-05-26T23:03:02.492354Z"
        },
        "id": "DWfRXglvJc1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_data_train, mean_data_valid = get_data(mean_embedding)\n",
        "max_data_train, max_data_valid = get_data(max_embedding)\n",
        "min_data_train, min_data_valid = get_data(min_embedding)\n",
        "sum_data_train, sum_data_valid = get_data(sum_embedding)\n",
        "\n",
        "combined_data_train, combined_data_valid = get_data(combined_embedding)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKp2R2WzJc1S",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:05:29.713798Z",
          "start_time": "2019-05-26T23:05:29.627187Z"
        },
        "scrolled": true,
        "id": "dQYfQMeQJc1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_name = {'mean_embedding':[mean_data_train, mean_data_valid],\n",
        "            'max_embedding':[max_data_train, max_data_valid],\n",
        "            'min_embedding':[min_data_train, min_data_valid],\n",
        "            'sum_embedding':[sum_data_train, sum_data_valid]}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model(embedding_name, learning_rate=1e-5, epochs=300, batch_size=16, dropout_rate=.5, load_params=True,\n",
        "               verbose=True, save_model=True):\n",
        "    \n",
        "    data_train, data_valid = data_name[embedding_name]\n",
        "    \n",
        "    #tf.reset_default_graph()\n",
        "\n",
        "    model = get_model(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
        "\n",
        "    # For plots\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        if load_params:\n",
        "        # Load Model\n",
        "            try:\n",
        "                print(f'------------- Attempting to Load {embedding_name} Model -------------')\n",
        "                model.saver.restore(sess, f'./model/{embedding_name}_model.ckpt')\n",
        "                print(f'------------- {embedding_name} Model Loaded -------------')\n",
        "            except:\n",
        "                print('Training New Model')\n",
        "        else:\n",
        "            print('Training New Model')\n",
        "\n",
        "        # Train Model\n",
        "        print('\\n------------- Training Model -------------\\n')\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for x, y, z in data_loader(data_train, batch_size=batch_size):\n",
        "\n",
        "                train_loss, _ = sess.run([model.loss, model.optimizer], feed_dict={model.x:x, \n",
        "                                                                             model.y:y, \n",
        "                                                                             model.z:z})\n",
        "\n",
        "            # x : embedding, y : open price, z : close price\n",
        "            valid_loss = sess.run(model.loss, feed_dict={model.x:data_valid[0], \n",
        "                                                         model.y:data_valid[1], \n",
        "                                                         model.z:data_valid[2]})\n",
        "\n",
        "            # print losses\n",
        "            if verbose:\n",
        "                print(f'Epoch {epoch+1}/{epochs},  Train RMSE Loss {train_loss}, Valid RMSE Loss {valid_loss}')\n",
        "\n",
        "                \n",
        "                \n",
        "            # Save Model at every 20 epochs\n",
        "            if save_model:\n",
        "                \n",
        "                if (epoch+1) % 20 == 0 and epoch > 0:\n",
        "                    if not os.path.exists('./model'):\n",
        "                        os.mkdir('./model/')\n",
        "\n",
        "                    model.saver.save(sess, f\"./model/{embedding_name}_model.ckpt\")\n",
        "                    print('\\n------------- Model Saved -------------\\n')\n",
        "                \n",
        "            train_losses.append(train_loss)\n",
        "            valid_losses.append(valid_loss)\n",
        "\n",
        "            \n",
        "    return model, train_losses, valid_losses"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dptqfomXddy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "257b60e2-69bf-4eac-db7d-84d65fb6108b"
      },
      "source": [
        "pip install --user --upgrade tensorflow-probability"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
            "Collecting gast>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (0.1.5)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle==1.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (1.3.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc2 has requirement gast==0.2.2, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast\n",
            "Successfully installed gast-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh1yWGSsZ56H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall keras\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:06:39.749656Z",
          "start_time": "2019-05-26T23:06:39.730376Z"
        },
        "scrolled": true,
        "id": "PRonHZqIJc1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ae1b23a-e669-4e52-ae58-b18cff0e7b81"
      },
      "source": [
        "# Possible Names : mean_embedding, max_embedding, min_embedding, sum_embedding\n",
        "epochs = 300\n",
        "learning_rate = 1e-4\n",
        "import os\n",
        "\n",
        "mean_model, mean_train_loss, mean_valid_loss = train_model('mean_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
        "max_model, max_train_loss, max_valid_loss = train_model('max_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
        "min_model, min_train_loss, min_valid_loss = train_model('min_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
        "sum_model, sum_train_loss, sum_valid_loss = train_model('sum_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training New Model\n",
            "\n",
            "------------- Training Model -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Training New Model\n",
            "\n",
            "------------- Training Model -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Training New Model\n",
            "\n",
            "------------- Training Model -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Training New Model\n",
            "\n",
            "------------- Training Model -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPlYiYHUJc1Y",
        "colab_type": "text"
      },
      "source": [
        "Running four models took about 25 minutes on surface pro 4. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MKGIH-bJc1Y",
        "colab_type": "text"
      },
      "source": [
        "## New Model for Combined Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0LtBe3_Jc1Z",
        "colab_type": "text"
      },
      "source": [
        "Since the combined_embedding is in different shape, I created a new model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:11:13.698323Z",
          "start_time": "2019-05-26T23:11:13.666655Z"
        },
        "id": "cwFTSREAJc1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class combined_model():\n",
        "    \n",
        "    def __init__(self, learning_rate=1e-3, dropout_rate=.5):\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # BERT Embedding\n",
        "        self.x = tf.placeholder(tf.float32, shape=(None, 3072))\n",
        "        # Open Price\n",
        "        self.y = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        # Adj Close Price\n",
        "        self.z = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.pred = self.run_model()\n",
        "        \n",
        "        self.loss = tf.sqrt(tf.losses.mean_squared_error(self.z, self.pred), name='loss')\n",
        "        \n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='optimizer').minimize(self.loss)\n",
        "        \n",
        "        self.saver = tf.train.Saver()\n",
        "        \n",
        "    def run_model(self):\n",
        "        \n",
        "        # Dense layers\n",
        "        layer1 = tf.contrib.layers.fully_connected(self.x, 1000)\n",
        "        layer1 = tf.nn.dropout(layer1, rate=self.dropout_rate)\n",
        "        layer1 = tf.layers.batch_normalization(layer1)\n",
        "        \n",
        "        layer2 = tf.contrib.layers.fully_connected(layer1, 500)\n",
        "        layer2 = tf.nn.dropout(layer2, rate=self.dropout_rate)\n",
        "        layer2 = tf.layers.batch_normalization(layer2)\n",
        "        \n",
        "        # Coefficient of impact values\n",
        "        layer3 = tf.contrib.layers.fully_connected(layer2, 1)\n",
        "        \n",
        "        layer4 = layer3 * self.y\n",
        "        \n",
        "        layer5 = tf.contrib.layers.fully_connected(layer4, 100)\n",
        "        layer5 = tf.nn.dropout(layer5, rate=self.dropout_rate)\n",
        "        layer5 = tf.layers.batch_normalization(layer5)\n",
        "        \n",
        "        output = tf.contrib.layers.fully_connected(layer5, 1)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PljAb6CJc1c",
        "colab_type": "text"
      },
      "source": [
        "combined model took about 4 minutes on surface pro 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T21:36:21.916256Z",
          "start_time": "2019-05-03T21:32:30.742573Z"
        },
        "scrolled": true,
        "id": "-ilRBSVyJc1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c94e7603-416a-4412-cca3-275140c50a81"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "model = combined_model(learning_rate=1e-4, dropout_rate=.5)\n",
        "epochs = 300\n",
        "\n",
        "combined_train_losses = []\n",
        "combined_valid_losses = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    try:\n",
        "        print(f'------------- Attempting to Load Combined Model -------------')\n",
        "        model.saver.restore(sess, f'./model/combined_model.ckpt')\n",
        "        print(f'------------- Combined Model Loaded -------------')\n",
        "        \n",
        "    except:\n",
        "        print('Training New Model')\n",
        "\n",
        "    # Train Model\n",
        "    print('\\n------------- Training Model -------------\\n')\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        for x, y, z in data_loader(combined_data_train, batch_size=16):\n",
        "\n",
        "            train_loss, _ = sess.run([model.loss, model.optimizer], feed_dict={model.x:x, \n",
        "                                                                         model.y:y, \n",
        "                                                                         model.z:z})\n",
        "\n",
        "        valid_loss = sess.run(model.loss, feed_dict={model.x:combined_data_valid[0], \n",
        "                                                     model.y:combined_data_valid[1], \n",
        "                                                     model.z:combined_data_valid[2]})\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            \n",
        "            print(f'Epoch {epoch+1}/{epochs},  Combined Train RMSE Loss {train_loss}, Combined Valid RMSE Loss {valid_loss}')\n",
        "\n",
        "            if not os.path.exists('./model'):\n",
        "                os.mkdir('./model/')\n",
        "\n",
        "            model.saver.save(sess, f\"./model/combined_model.ckpt\")\n",
        "            print('\\n------------- Model Saved -------------\\n')\n",
        "\n",
        "        combined_train_losses.append(train_loss)\n",
        "        combined_valid_losses.append(valid_loss)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------- Attempting to Load Combined Model -------------\n",
            "Training New Model\n",
            "\n",
            "------------- Training Model -------------\n",
            "\n",
            "Epoch 1/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 21/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 41/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 61/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 81/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 101/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 121/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 141/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 161/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 181/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 201/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 221/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 241/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 261/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 281/300,  Combined Train RMSE Loss nan, Combined Valid RMSE Loss nan\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPkPc2PoJc1f",
        "colab_type": "text"
      },
      "source": [
        "## Losses of each model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-BWok1vJc1f",
        "colab_type": "text"
      },
      "source": [
        "Here are the results of loss plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh2q_qz-Jc1g",
        "colab_type": "text"
      },
      "source": [
        "![Four Models' Losses](plots/4_losses.png)\n",
        "![Combined Loss](plots/combined_loss.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLhRH0liJc1g",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-TjMFsOJc1h",
        "colab_type": "text"
      },
      "source": [
        "Now let's predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T20:34:45.545725Z",
          "start_time": "2019-05-03T20:34:45.501679Z"
        },
        "id": "5fHu1CHmJc1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_data = {'mean_embedding':mean_embedding_test,\n",
        "                 'max_embedding':max_embedding_test,\n",
        "                 'min_embedding':min_embedding_test,\n",
        "                 'sum_embedding':sum_embedding_test}\n",
        "\n",
        "def predict_model(embedding_name):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    data = embedding_data[embedding_name]\n",
        "\n",
        "    model = get_model(learning_rate=1e-5)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    #     Load Model\n",
        "        try:\n",
        "            print(f'------------- Attempting to Load {embedding_name} Model -------------')\n",
        "            model.saver.restore(sess, f'./model/{embedding_name}_model.ckpt')\n",
        "            print('------------- Model Loaded -------------')\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "        pred = sess.run(model.pred, feed_dict={model.x:data, \n",
        "                                                    model.y:open_test})\n",
        "        \n",
        "    return model, pred"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T20:35:05.630224Z",
          "start_time": "2019-05-03T20:34:59.413690Z"
        },
        "scrolled": true,
        "id": "_0LdeX3RJc1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "bd6a12b6-d59c-44c9-e0f3-7a07fd6d9157"
      },
      "source": [
        "mean_model, mean_pred = predict_model('mean_embedding')\n",
        "max_model, max_pred = predict_model('max_embedding')\n",
        "sum_model, sum_pred = predict_model('sum_embedding')\n",
        "min_model, min_pred = predict_model('min_embedding')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------- Attempting to Load mean_embedding Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/mean_embedding_model.ckpt\n",
            "------------- Model Loaded -------------\n",
            "------------- Attempting to Load max_embedding Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/max_embedding_model.ckpt\n",
            "------------- Model Loaded -------------\n",
            "------------- Attempting to Load sum_embedding Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/sum_embedding_model.ckpt\n",
            "------------- Model Loaded -------------\n",
            "------------- Attempting to Load min_embedding Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/min_embedding_model.ckpt\n",
            "------------- Model Loaded -------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T21:48:22.897336Z",
          "start_time": "2019-05-03T21:48:21.103206Z"
        },
        "id": "bdJmJ59iJc1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ee37c49b-322c-46bd-8c36-9e2b6e08abd5"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "model = combined_model(learning_rate=1e-5)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#     Load Model\n",
        "    try:\n",
        "        print(f'------------- Attempting to Load Combined Model -------------')\n",
        "        model.saver.restore(sess, f'./model/combined_model.ckpt')\n",
        "        print('------------- Model Loaded -------------')\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    combined_pred = sess.run(model.pred, feed_dict={model.x:combined_embedding_test, \n",
        "                                                model.y:open_test})\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------- Attempting to Load Combined Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/combined_model.ckpt\n",
            "------------- Model Loaded -------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T20:35:05.639733Z",
          "start_time": "2019-05-03T20:35:05.634009Z"
        },
        "id": "4zCFtkA6Jc1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_pred = mean_pred.flatten()\n",
        "max_pred = max_pred.flatten()\n",
        "min_pred = min_pred.flatten()\n",
        "sum_pred = sum_pred.flatten()\n",
        "\n",
        "combined_pred = combined_pred.flatten()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NihQW6bPJc1u",
        "colab_type": "text"
      },
      "source": [
        "**![Four Models' Predictions](plots/4_predictions.png)\n",
        "![Combined Prediction](plots/combined_prediction.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOv4j1dAJc1u",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FeFG5SDJc1v",
        "colab_type": "text"
      },
      "source": [
        "We can see that the predicted values have high variance and predicted values fluctuate much. However, the models still were able to capture general trend of the prices. As it is impossible to predict something with 100%, models like above are only used as a general guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2xIRu_oJc1v",
        "colab_type": "text"
      },
      "source": [
        "One way to improve a model is to set a threshold which it limits how much the price can change over a day. For example, we can set it to 10,000 that it won't change above the amount. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phafESnMJc1w",
        "colab_type": "text"
      },
      "source": [
        "Also, the news I used may not (or most likely not) be related to DJIA. Using news that are closely related to it can also improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIdtACnuJc1w",
        "colab_type": "text"
      },
      "source": [
        "Thank you for reading the post and if there is any mistake I made, please let me know!"
      ]
    }
  ]
}