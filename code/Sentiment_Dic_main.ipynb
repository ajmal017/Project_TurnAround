{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain YG에서 만든 코드를 전체 파일에 적용 시킨다. 각각의 DataFrame에 저장시킨뒤 비교\n",
    "#### 딕셔너리를 이용하여 가중치를 합치는 작업을 거친다. Score = sum(tf_rof)/sum(tf) / 이기 때문에 Key값을 word로 주고 value 값을 sum(tf_rof) , sum(tf) 값으로 주면 좋겠다.\n",
    "#### 명사만 추출 했으나 추후에 서술어까지 해야함 .\n",
    "#### 매일매일 등락률 계산을 위해서는 등락률 계산 모듈이 필요함 Sentiment_Dic_Domain file 참고 \n",
    "#### 데이터 프레임 열이름 수정 : Name -> name , Frequency -> tf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeekday(start, end):\n",
    "#start = '20200411' , end = '20200922'\n",
    "    # 평일을 구하기 위한 datetime의 기본적인 변수를 생성해준다. \n",
    "    s_year = int(start[0:4]) # 2020\n",
    "    s_month = int(start[5:6]) # 4\n",
    "    s_day = int(start[6:8]) # 11\n",
    "\n",
    "    e_year = int(end[0:4]) # 2020\n",
    "    e_month = int(end[5:6]) # 9\n",
    "    e_day = int(end[6:8]) # 22 \n",
    "\n",
    "    day_delta = datetime.timedelta(days=1) \n",
    "    start_date = datetime.date(s_year, s_month, s_day)\n",
    "    end_date = datetime.date(e_year, e_month, e_day)\n",
    "    length = (end_date - start_date).days # 시작일 - 끝나는일의 일 수 \n",
    " \n",
    "    # 평일 데이터만 가져오기 위해 위에서 짠 것들을 합쳐준다.\n",
    "    date_weekday = [] # 평일날짜를 넣기위한 리스트 \n",
    "    for i in range(length+1) : # 마지막날을 포함하기 위해 length에 + 1 을 해준다. \n",
    "        date = start_date + i*day_delta # date 는 for 문을 돌며 day_delta로 1일 씩 더해지게 된다. \n",
    "        date_w = date.weekday() # date에 저장된 날짜를 weekday로 본다 0~6의 값을 가진다. \n",
    "        if date_w < 5 : # 조건문을 걸어 0~4 월~금 만 date_weekday 리스트에 추가하도록한다. \n",
    "            date_weekday.append(str(date))\n",
    "            \n",
    "    return date_weekday # 평일만 포함된 date weekday 리스트가 나온다. 이를 이용해 평일 엑셀 데이터만 불러 오도록 한다. \n",
    "\n",
    "\n",
    "# 년/월/일 변수가 / 로 구분 되어있다면 data name 과 맞춰주기 위해 /를 -로 바꿔주는 부분. \n",
    "def replaceSlash(df) :\n",
    "    dates = []\n",
    "    df_date = df.iloc[:,:1]\n",
    "    df_date = df_date.sort_values(by='date',ascending=False)\n",
    "    list1 = df_date.values.tolist()\n",
    "    for i in range(len(df_date)):\n",
    "        tokens = list1[i][0]\n",
    "        token = tokens.split('-')\n",
    "        year = token[0]\n",
    "        month = token[1]\n",
    "        #month = month.replace('0','')\n",
    "        day = token[2]\n",
    "        date = year + '-' + month + '-' + day \n",
    "        dates.append(date)\n",
    "    return dates\n",
    "\n",
    "# 문장내 모든 conma를 제거해주는 함수 \n",
    "def replaceConma(df, start, finish) : # start, finish 열의 인덱스를 지정해 주면된다. \n",
    "    for j in range(start, finish+1):\n",
    "        for i in range(len(df.iloc[:,j])):\n",
    "            df.iloc[:,j][i] = df.iloc[:,j][i].replace(\",\", \"\")\n",
    "    return df\n",
    "# df = replaceAllConma(df, 1, 9) 1열 부터 9열 까지의 모든 데이터의 conma를 삭제.\n",
    "\n",
    "\n",
    "# user dic에 한글자가 있는 경우 이를 삭제해주는 함수 . filename 에는 userdic name 이 들어간다. \n",
    "def delDicOneword(filename) :\n",
    "    with open(f'{filename}.txt', 'r' ) as f :\n",
    "        line = f.readlines()\n",
    "        print(line)\n",
    "\n",
    "    # 개행문자를 포함하여 글자수가 2개 이하인 모든 글자를 삭제. \n",
    "    for i in line :\n",
    "        i.strip()\n",
    "        if len(i) < 3 :\n",
    "            line.remove(i)\n",
    "\n",
    "    with open(f'{filename}.txt', 'w') as f:\n",
    "        for i in line:\n",
    "            f.write(i)\n",
    "\n",
    "# tf score 계산\n",
    "\n",
    "def getTfrofScore(date) :\n",
    "# date = '2020-04-13' \n",
    "\n",
    "    df1 = pd.read_excel(f'./data_news/YG_DATAyg {date}.xlsx')\n",
    "    sentences = [] \n",
    "    list1 = list(set(df1['title'].values.tolist())) # 중복을 제거 한다.\n",
    "    pattern = '[^\\w\\s]'\n",
    "    repl = ''\n",
    "    for i in list1 :\n",
    "        sen = i.split(']')\n",
    "        if len(sen) >= 2 : # 스플릿 했을때 ] 가 있으면 len이 2 가 나오고 없으면 1이 나옴. 그중 문장을 선택하게 조건문을 부여\n",
    "            sen = sen[1]\n",
    "        else :\n",
    "            sen = sen[0]\n",
    "        sen = re.sub(pattern,\" \",sen) # pattern에서 정의한 특수문자 정규표현식 을 제거 \n",
    "        sen = sen.strip() # 양옆의 공백을 제거 \n",
    "        sentences.append(sen)\n",
    "    #sentences\n",
    "\n",
    "    komoran_userdic=Komoran(userdic='./userdicCanSur_YG_Sentiment.txt')\n",
    "    corpus = sentences\n",
    "    wordCount = {}\n",
    "    wordlist = []\n",
    "    answer = []\n",
    "    for i in corpus:\n",
    "        wordlist.append(komoran_userdic.nouns(i))\n",
    "    for i in wordlist:\n",
    "        answer += i\n",
    "    #print(answer)\n",
    "\n",
    "    \n",
    "    attention = {}\n",
    "    attention = Counter(answer)\n",
    "    df1 = pd.DataFrame(attention.keys())\n",
    "    df1.columns =['name']\n",
    "    df1['tf'] = pd.DataFrame(attention.values())\n",
    "    df1 = df1[df1['tf'] >= 2] # 빈도수가 2번 이상인 단어만을 대상으로 한다. \n",
    "    df1.reset_index(inplace = True)\n",
    "    df1 = df1.drop('index', axis =1)\n",
    "\n",
    "    # 한글자인 데이터를 삭제 해준다.\n",
    "    for i in range(len(df1)):\n",
    "        word = df1['name'][i] # iloc로 하면 df를 drop 했을때 키를 못찾는 에러가 발생함. \n",
    "        print(i, word)\n",
    "        if len(word) < 2 : # Name 테이블 안에 글자가 한개이면 이를 삭제\n",
    "            print(i, word)\n",
    "            df1.drop(index=i, inplace=True)\n",
    "\n",
    "\n",
    "    df = df[ df['date'] == date ] # 조건문을 걸어 변수 date 와 같은날의 행의 모든 데이터를 반환 \n",
    "    rof = df['rof'].iloc[0] # 그날의 데이터중 rof값을 반환\n",
    "    \n",
    "    df1['tf_rof'] = df1['tf']*rof # 빈도수 dataFrame에 df_rof 값을 저장\n",
    "    return df1\n",
    "\n",
    "def getSumdict(df, key_col, value_col) :\n",
    "    # df = df_name , key_col = 'Name' , value_col = 'tfrof' 이렇게 key가 될 col과 value가 될 col을 정해주면 합을 구할 수 있다. \n",
    "    a_dict = {} \n",
    "    for i in range(len(df)):\n",
    "        key = df[key_col][i]\n",
    "        value = df[value_col][i]\n",
    "        if key in a_dict:\n",
    "          a_dict[key] += int(value)\n",
    "        else:\n",
    "          a_dict[key] = int(value)\n",
    "    return a_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. stock data 를 불러온다 .\n",
    "2. 모든 데이터의 conma를 삭제한다 .\n",
    "3. stock data 의 년월일을 news data 년월일과 맞추기 위해 slash를 삭제하고 hyphen을 넣는다.  \n",
    "##### 미구현 / 등락률을 계산한다. 계산후 na 값은 drop 한다.(가장 첫날에 해당함)\n",
    "4. news data 를 불러온다.\n",
    "##### 서술어 미구현 / 사전 정의된 Komoran 형태소 분석기를 이용해 뉴스 명사(서술어)의 Tf를 계산한다. (이 부분은 비지도 학습의 soynlp를 사용하여도 무방 하다고 생각. ) \n",
    "5. tfrof score를 구하기 위해 stock data에서 new data의 date에 맞는 등락률을 가져와 tf에 곱한다. tfrof값을 산출. \n",
    "6. 이를 데이터 프레임으로 저장. for문을 돌려 위의 과정을 반복하게 해준다. \n",
    "7. 날짜별 데이터 프레임 이름 리스트를 저장하고, 리스트를 이용하여 데이터 프레임을 concat 하여 밑으로 쭉 붙여준다.\n",
    "8. 딕셔너리를 이용해서 위에서 부터 내려오면서 key 값을 체크하여 tf, tfrof 값을 추가 or 더하여 준다. \n",
    "\n",
    "##### 주의 사항. news file에 반드시 date가 들어가 있어야 계산이 가능하다 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # csv 파일로 들어가 등락률을 미리 계산해주어야함.  등락률 = 당일대비 / 전일종가\n",
    "    df = pd.read_csv('./data_stock/YG_1y.csv', encoding = 'ISO-8859-1')\n",
    "    df.columns = ['date', 'closing', 'diff', 'volume', 'transaction', 'market', 'high', 'low', 'capital', 'stocks', 'rof']\n",
    "    df = replaceConma(df, 1, 9)\n",
    "    # df = replaceSlash(df) # stock data date가 2020/1/3 일때 사용\n",
    "    \n",
    "    # df1 = getTfrofScore('2020-04-11') # 하루치 Score를 구할때 \n",
    "    df_list = [] # concat을 하기위해 df들의 이름을 저장할 리스트를 생성\n",
    "    days = getWeekday('20200411', '20200922') # 내가 기준으로 한 범위\n",
    "    for day in days :\n",
    "        df_list.append(f'df_{day}') # 미리 만든 리스트에 df_2020-11-11 의 형태로 리스트 네임을 저장\n",
    "        df_day = dfgetTfrofScore(day) # 이렇게 for문을 돌리면 각 날짜에 대한 TfrofScore df 를 반환함 \n",
    "    df_concat = pd.concat(df_list) # 반환된 df들을 concat해서 하나의 df로 만듦\n",
    "    \n",
    "    tfrof_dic = getSumdict(df_concat, 'name', 'tfrof')\n",
    "    tf_dic = getSumdict(df_concat, 'name', 'tf')\n",
    "    # 이제 여기서 나온 word 값들을 기반으로 tfrof/tf 값을 수행해주면 score 값이 나온다. \n",
    "    # keys, values 를 이용 list에 넣어서 name, tf / name , tfrof 로 변환후 acseding 시켜서 tfrof/tf 를 계산해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
