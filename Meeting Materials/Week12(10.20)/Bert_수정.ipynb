{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Bert_YGSentiment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNndhrOwyBN5"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_d-wOcVy4-E",
        "outputId": "c9f805ce-2192-4ed7-f2df-70f79818ecbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P1RwCOWyBN9"
      },
      "source": [
        "neg = pd.read_csv('/content/drive/My Drive/Data_0.csv', encoding='CP949').dropna()\n",
        "pos = pd.read_csv('/content/drive/My Drive/Data_1.csv', encoding='CP949').dropna()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyNaCWzcyBOE",
        "outputId": "956d288e-28d3-4240-bb29-ff9d208d18f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "neg = neg.loc[:, ['date','sen','라벨']]\n",
        "neg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>sen</th>\n",
              "      <th>라벨</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019.01.01</td>\n",
              "      <td>\"美 상·하원 외교위원장, 하원 군사위원장 교체\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019.01.01</td>\n",
              "      <td>최종구 금융위원장 \"혁신 발목잡는 금융감독 개선할 것\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019.01.01</td>\n",
              "      <td>새해 화두로 소비자 신뢰 회복과 혁신 강조한 생명·손해보험협회장</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019.01.01</td>\n",
              "      <td>美 기업 '어닝 리세션' 경고음 커진다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019.01.01</td>\n",
              "      <td>\"공무원에 정무적 판단 강요…나도 촛불 들었지만 바뀐 것 없어\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2403</th>\n",
              "      <td>2020.09.23</td>\n",
              "      <td>존슨앤드존슨, 코로나 백신 3상 임상 시작…6만명 대상</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>2020.09.23</td>\n",
              "      <td>'위기의 가을 맞나'…선거 패배로 伊 극우지도자 살비니 '흔들'</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2405</th>\n",
              "      <td>2020.09.23</td>\n",
              "      <td>주멕시코대사관, 현지인 직원 코로나19 확진…대면업무 잠정중단</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2406</th>\n",
              "      <td>2020.09.23</td>\n",
              "      <td>주멕시코대사관, 현지인 직원 확진…대면업무 중단</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2407</th>\n",
              "      <td>2020.09.23</td>\n",
              "      <td>'독극물 중독증' 나발니, 푸틴에 \"내가 중독 자작극 벌였다고?\"(종합)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2408 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            date                                       sen  라벨\n",
              "0     2019.01.01               \"美 상·하원 외교위원장, 하원 군사위원장 교체\"   0\n",
              "1     2019.01.01            최종구 금융위원장 \"혁신 발목잡는 금융감독 개선할 것\"   0\n",
              "2     2019.01.01       새해 화두로 소비자 신뢰 회복과 혁신 강조한 생명·손해보험협회장   0\n",
              "3     2019.01.01                     美 기업 '어닝 리세션' 경고음 커진다   0\n",
              "4     2019.01.01       \"공무원에 정무적 판단 강요…나도 촛불 들었지만 바뀐 것 없어\"   0\n",
              "...          ...                                       ...  ..\n",
              "2403  2020.09.23            존슨앤드존슨, 코로나 백신 3상 임상 시작…6만명 대상   0\n",
              "2404  2020.09.23       '위기의 가을 맞나'…선거 패배로 伊 극우지도자 살비니 '흔들'   0\n",
              "2405  2020.09.23        주멕시코대사관, 현지인 직원 코로나19 확진…대면업무 잠정중단   0\n",
              "2406  2020.09.23                주멕시코대사관, 현지인 직원 확진…대면업무 중단   0\n",
              "2407  2020.09.23  '독극물 중독증' 나발니, 푸틴에 \"내가 중독 자작극 벌였다고?\"(종합)   0\n",
              "\n",
              "[2408 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNKNKOxxyBOG",
        "outputId": "0b731239-18bf-49d2-a3bf-7ab8f13c9910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "pos = pos.loc[:, ['date','sen','라벨']]\n",
        "pos"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>sen</th>\n",
              "      <th>라벨</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019.01.03</td>\n",
              "      <td>김동연 \"소신담긴 정책 모두 관철되진 않아…조율은 다른 문제\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019.01.03</td>\n",
              "      <td>\"中 공유자전거 '오포' 보증금 반환, 1000만명 중 24만명 꼴\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019.01.03</td>\n",
              "      <td>[column of the week] '시장주의' 밀어붙인 미국…'관료주의' 유럽에...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019.01.03</td>\n",
              "      <td>트럼프, 매티스 前 국방 맹비난 \"잘한 일이 없어 해고\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019.01.03</td>\n",
              "      <td>중국 달 탐사선, 인류 최초로 달 뒷면 착륙…NASA \"매우 인상적\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2977</th>\n",
              "      <td>2020.09.28</td>\n",
              "      <td>에어부산도 891억원 유상증자한다…아시아나 \"300억 참여\"(종합)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2978</th>\n",
              "      <td>2020.09.28</td>\n",
              "      <td>이탈리아 연구진 \"모유 수유로는 코로나19 전파 안 돼\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2979</th>\n",
              "      <td>2020.09.28</td>\n",
              "      <td>이디야커피, 알바직원에게 장학금</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2980</th>\n",
              "      <td>2020.09.28</td>\n",
              "      <td>인프라코어 누가 가져갈까…현대중공업그룹 등 3~4곳 인수 참여(종합)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2981</th>\n",
              "      <td>2020.09.28</td>\n",
              "      <td>대구·경북 은행 현금자동입출금기 5년간 707개 감소</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2982 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            date                                                sen  라벨\n",
              "0     2019.01.03                 김동연 \"소신담긴 정책 모두 관철되진 않아…조율은 다른 문제\"   1\n",
              "1     2019.01.03             \"中 공유자전거 '오포' 보증금 반환, 1000만명 중 24만명 꼴\"   1\n",
              "2     2019.01.03  [column of the week] '시장주의' 밀어붙인 미국…'관료주의' 유럽에...   1\n",
              "3     2019.01.03                    트럼프, 매티스 前 국방 맹비난 \"잘한 일이 없어 해고\"   1\n",
              "4     2019.01.03             중국 달 탐사선, 인류 최초로 달 뒷면 착륙…NASA \"매우 인상적\"   1\n",
              "...          ...                                                ...  ..\n",
              "2977  2020.09.28              에어부산도 891억원 유상증자한다…아시아나 \"300억 참여\"(종합)   1\n",
              "2978  2020.09.28                    이탈리아 연구진 \"모유 수유로는 코로나19 전파 안 돼\"   1\n",
              "2979  2020.09.28                                  이디야커피, 알바직원에게 장학금   1\n",
              "2980  2020.09.28             인프라코어 누가 가져갈까…현대중공업그룹 등 3~4곳 인수 참여(종합)   1\n",
              "2981  2020.09.28                      대구·경북 은행 현금자동입출금기 5년간 707개 감소   1\n",
              "\n",
              "[2982 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXTbMxbIyBOJ",
        "outputId": "6319bd1e-72c6-46e8-f514-87c5049a3965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(pos))\n",
        "print(len(neg))\n",
        "# 나중에 SMOTE를 이용해서 neg 를 뻥튀기 해서 시험 해야함 . "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2982\n",
            "2408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NafO2NiJyBOQ"
      },
      "source": [
        "pos['Labeling'] = 1\n",
        "neg['Labeling'] = 0\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8HTgFeCyBOT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoJI2LlqyBOW"
      },
      "source": [
        "pos_train, pos_test = train_test_split(pos, test_size=0.3, shuffle=True, random_state=1004)\n",
        "neg_train, neg_test = train_test_split(neg, test_size=0.3, shuffle=True, random_state=1004)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P1hun9gyBOY",
        "outputId": "c1f2b6c6-d8ad-44dd-d645-9a3d9a51603f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(len(pos_train)/len(pos)*100)\n",
        "print(len(pos_test)/len(pos)*100)\n",
        "print(len(neg_train)/len(neg)*100)\n",
        "print(len(neg_test)/len(neg)*100)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69.98658618376929\n",
            "30.013413816230717\n",
            "69.97508305647841\n",
            "30.024916943521596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5-LjS1syBOd"
      },
      "source": [
        "# # Smote를 사용해본다\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# smote = SMOTE(random_state=0)\n",
        "# pos_train_over,neg_train_over = smote.fit_sample(pos_train,neg_train)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDIXk5ZzyBOf",
        "outputId": "56b8f577-edde-4463-9e53-756d444e158e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train = pd.concat([pos_train, neg_train])\n",
        "test = pd.concat([pos_test, neg_test])\n",
        "print(len(train))\n",
        "print(len(test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3772\n",
            "1618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi0T8tzTyBOh"
      },
      "source": [
        "train_df = train.sample(frac=0.4, random_state=999)\n",
        "test_df = test.sample(frac=0.4, random_state=999)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7gzYY5X1v7q"
      },
      "source": [
        "train_df=train_df[['date','sen','Labeling']]\n",
        "test_df=test_df[['date','sen','Labeling']]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyIINsi7yBOl",
        "outputId": "e0dcfebf-0e34-46f2-a4c0-c57c6647b5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "train_df.columns = ['id','document','label']\n",
        "test_df.columns = ['id','document','label']\n",
        "test_df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>2019.10.13</td>\n",
              "      <td>철원 민통선 멧돼지서 ASF…'DMZ 철책' 방역 뚫렸나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469</th>\n",
              "      <td>2020.03.17</td>\n",
              "      <td>브라질서 코로나19 첫 사망자 발생…상파울루 60대 남성</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>2019.04.17</td>\n",
              "      <td>마크롱 \"노트르담 5년 내 복원\"…40년 더 걸릴 수도</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1538</th>\n",
              "      <td>2020.03.23</td>\n",
              "      <td>G20, 코로나19 사태 극복 공동전략 마련키로</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>2019.07.16</td>\n",
              "      <td>군인들이 유격후 가장 먹고 싶은 급식 1위 삼계탕 쏜 김정문알로에</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1288</th>\n",
              "      <td>2020.02.03</td>\n",
              "      <td>캘리포니아서 부부가 신종코로나 감염…미 확진자 11명</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1695</th>\n",
              "      <td>2020.04.06</td>\n",
              "      <td>정수영 매스아시아 대표 \"짧은 거리 이동할 땐 고고씽…전용 앱 통해 편리하게 이용\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2399</th>\n",
              "      <td>2020.07.02</td>\n",
              "      <td>[1사1병영] LG하우시스, 제37보병사단에 위문품</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>2019.09.26</td>\n",
              "      <td>'우크라 의혹' 내부고발장 공개…\"백악관, 통화기록 은폐 시도\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2365</th>\n",
              "      <td>2020.07.01</td>\n",
              "      <td>중국 온라인여행사 '한국 관광상품' 판매에 200만명 지켜봤다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>647 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id                                        document  label\n",
              "814   2019.10.13                 철원 민통선 멧돼지서 ASF…'DMZ 철책' 방역 뚫렸나      1\n",
              "1469  2020.03.17                 브라질서 코로나19 첫 사망자 발생…상파울루 60대 남성      0\n",
              "235   2019.04.17                  마크롱 \"노트르담 5년 내 복원\"…40년 더 걸릴 수도      0\n",
              "1538  2020.03.23                      G20, 코로나19 사태 극복 공동전략 마련키로      1\n",
              "404   2019.07.16            군인들이 유격후 가장 먹고 싶은 급식 1위 삼계탕 쏜 김정문알로에      0\n",
              "...          ...                                             ...    ...\n",
              "1288  2020.02.03                   캘리포니아서 부부가 신종코로나 감염…미 확진자 11명      1\n",
              "1695  2020.04.06  정수영 매스아시아 대표 \"짧은 거리 이동할 땐 고고씽…전용 앱 통해 편리하게 이용\"      1\n",
              "2399  2020.07.02                    [1사1병영] LG하우시스, 제37보병사단에 위문품      1\n",
              "660   2019.09.26             '우크라 의혹' 내부고발장 공개…\"백악관, 통화기록 은폐 시도\"      0\n",
              "2365  2020.07.01              중국 온라인여행사 '한국 관광상품' 판매에 200만명 지켜봤다      1\n",
              "\n",
              "[647 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc9mjTBD2JLv",
        "outputId": "3769fb5e-39ad-4976-da0c-a6ee0f47de22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        }
      },
      "source": [
        "!pip install pytorch_transformers"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/dd/8ed2e36ab9e8eb006e0abd298e0d6c4ff89bf00caafd3cf21a58b700e061/boto3-1.16.0-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.6.0+cu101)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/38/317bec1175e74b817a51ca806f3e5822f340bb3b344e8693425122499f4f/botocore-1.19.0-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 36.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch_transformers) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.0->boto3->pytorch_transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=894815265f5bfbcf9f8b20662bf51cff8869d79e4a8e4ac67d5f23ddee014cec\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.0 has requirement urllib3<1.26,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, sacremoses, sentencepiece, pytorch-transformers\n",
            "Successfully installed boto3-1.16.0 botocore-1.19.0 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oFylOw_VNk-",
        "outputId": "960aa97f-5ed9-43ed-94a4-50985c5ad67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install awscli awsebcli botocore==1.18.18 --upgrade"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting awscli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/bf/3beb2e763d0c5d7db634a76f71faf5a577e315606207b33fd3578fbe5a4b/awscli-1.18.160-py2.py3-none-any.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 4.5MB/s \n",
            "\u001b[?25hCollecting awsebcli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/26/6e51c423dddda05204f36c927f079aa1972617c3cafcd4b434cd034e7cd1/awsebcli-3.19.1.tar.gz (249kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 40.8MB/s \n",
            "\u001b[?25hCollecting botocore==1.18.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/72/984ac8f33b5c8df5ff63f323a8724f65b4d0f8956968b942b77d35d3a1ef/botocore-1.18.18-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 34.0MB/s \n",
            "\u001b[?25hCollecting colorama<0.4.4,>=0.2.5; python_version != \"3.4\"\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: PyYAML<5.4,>=3.10; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from awscli) (3.13)\n",
            "Collecting rsa<=4.5.0,>=3.1.2; python_version != \"3.4\"\n",
            "  Downloading https://files.pythonhosted.org/packages/26/f8/8127fdda0294f044121d20aac7785feb810e159098447967a6103dedfb96/rsa-4.5-py2.py3-none-any.whl\n",
            "Collecting docutils<0.16,>=0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 57.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.3.3)\n",
            "Collecting cement==2.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/60/608f0b8975f4ee7deaaaa7052210d095e0b96e7cd3becdeede9bd13674a1/cement-2.8.2.tar.gz (165kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 57.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future<0.17.0,>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from awsebcli) (0.16.0)\n",
            "Collecting pathspec==0.5.9\n",
            "  Downloading https://files.pythonhosted.org/packages/84/2a/bfee636b1e2f7d6e30dd74f49201ccfa5c3cf322d44929ecc6c137c486c5/pathspec-0.5.9.tar.gz\n",
            "Collecting python-dateutil<2.8.1,>=2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 55.0MB/s \n",
            "\u001b[?25hCollecting requests<2.21,>=2.20.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/17/5cbb026005115301a8fb2f9b0e3e8d32313142fe8b617070e7baad20554f/requests-2.20.1-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools>=20.0 in /usr/local/lib/python3.6/dist-packages (from awsebcli) (50.3.0)\n",
            "Collecting semantic_version==2.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/8d/49a968bafda84c2f1c39a9ed429e37cb75cc03896e8d6b873001e6456fad/semantic_version-2.5.0-py3-none-any.whl\n",
            "Collecting six<1.12.0,>=1.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from awsebcli) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from awsebcli) (1.24.3)\n",
            "Collecting wcwidth<0.2.0,>=0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/d5/1ecdac957e3ea12c1b319fcdee8b6917ffaff8b4644d673c4d72d2f20b49/wcwidth-0.1.9-py2.py3-none-any.whl\n",
            "Collecting docker-compose<1.26.0,>=1.25.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/35/1dfbb8e6b2ce5d290622a49cae0a7f3cf09cdc4341380a600aee00530881/docker_compose-1.25.5-py2.py3-none-any.whl (139kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 54.6MB/s \n",
            "\u001b[?25hCollecting blessed>=1.9.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/37/241fec1c8fa767b445c5afb5cfa7eb78cef07f85489b51c2cf292b530265/blessed-1.17.11-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.18.18) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=4.5.0,>=3.1.2; python_version != \"3.4\"->awscli) (0.4.8)\n",
            "Collecting idna<2.8,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.20.1->awsebcli) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.21,>=2.20.1->awsebcli) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: docopt<1,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from docker-compose<1.26.0,>=1.25.2->awsebcli) (0.6.2)\n",
            "Collecting dockerpty<1,>=0.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/ee/e9ecce4c32204a6738e0a5d5883d3413794d7498fe8b06f44becc028d3ba/dockerpty-0.4.1.tar.gz\n",
            "Collecting texttable<2,>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/06/f5/46201c428aebe0eecfa83df66bf3e6caa29659dbac5a56ddfd83cae0d4a4/texttable-1.6.3-py2.py3-none-any.whl\n",
            "Collecting cached-property<2,>=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Collecting docker[ssh]<5,>=3.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/8c/8d42dbd83679483db207535f4fb02dc84325fa78b290f057694b057fcd21/docker-4.3.1-py2.py3-none-any.whl (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.3MB/s \n",
            "\u001b[?25hCollecting websocket-client<1,>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jsonschema<4,>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from docker-compose<1.26.0,>=1.25.2->awsebcli) (2.6.0)\n",
            "Collecting paramiko>=2.4.2; extra == \"ssh\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 49.7MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/62/30f6936941d87a5ed72efb24249437824f6b2c953901245b58c91fde2f27/cryptography-3.1.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 51.1MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB)\n",
            "\u001b[K     |████████████████████████████████| 962kB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cffi>=1.1 in /usr/local/lib/python3.6/dist-packages (from bcrypt>=3.1.3->paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=3.7.0->docker-compose<1.26.0,>=1.25.2->awsebcli) (1.14.3)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4.2; extra == \"ssh\"->docker[ssh]<5,>=3.7.0->docker-compose<1.26.0,>=1.25.2->awsebcli) (2.20)\n",
            "Building wheels for collected packages: awsebcli, cement, pathspec, dockerpty\n",
            "  Building wheel for awsebcli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for awsebcli: filename=awsebcli-3.19.1-cp36-none-any.whl size=357561 sha256=f19a213bdf6fd6eb5f769d03b044cde9f5b0f940695da56aaee345279a7231c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/8c/0c/0f27126aa6b5cd9d0ca7f8b7afe663b053accabb441ada7d30\n",
            "  Building wheel for cement (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cement: filename=cement-2.8.2-cp36-none-any.whl size=99714 sha256=52e54c041732d9e7ab2bbf46d836207c19e2fbabbff9fc73a3422878dbc20b15\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/9e/02/0af61a0ed625ea3decf29b8602fc0cbecc38943f19e076bb2e\n",
            "  Building wheel for pathspec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathspec: filename=pathspec-0.5.9-cp36-none-any.whl size=26357 sha256=b19b51532b55ae4836ae688aec69130b06c839d79b5ead2a857259aabbda1c8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/cb/7e/ce6e6062c69446e39e328170524ca8213498bc66a74c6a210b\n",
            "  Building wheel for dockerpty (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dockerpty: filename=dockerpty-0.4.1-cp36-none-any.whl size=16606 sha256=6eef88731e97f098490de809256e5b24dacea8456a7d016a58b58bfe8321a495\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/1e/86/bd0a97a0907c6c654af654d5875d1d4383dd1f575f77cee4aa\n",
            "Successfully built awsebcli cement pathspec dockerpty\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-hub 0.9.0 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.3.0 has requirement requests<3,>=2.21.0, but you'll have requests 2.20.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: nbclient 0.5.0 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.20.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: dm-tree 0.1.5 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: boto3 1.16.0 has requirement botocore<1.20.0,>=1.19.0, but you'll have botocore 1.18.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: awscli 1.18.160 has requirement botocore==1.19.0, but you'll have botocore 1.18.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: awsebcli 3.19.1 has requirement botocore<1.18,>=1.17, but you'll have botocore 1.18.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: awsebcli 3.19.1 has requirement PyYAML<5.4,>=5.3.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: colorama, rsa, docutils, six, python-dateutil, botocore, awscli, cement, pathspec, idna, requests, semantic-version, wcwidth, dockerpty, texttable, cached-property, websocket-client, bcrypt, cryptography, pynacl, paramiko, docker, docker-compose, blessed, awsebcli\n",
            "  Found existing installation: rsa 4.6\n",
            "    Uninstalling rsa-4.6:\n",
            "      Successfully uninstalled rsa-4.6\n",
            "  Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: python-dateutil 2.8.1\n",
            "    Uninstalling python-dateutil-2.8.1:\n",
            "      Successfully uninstalled python-dateutil-2.8.1\n",
            "  Found existing installation: botocore 1.19.0\n",
            "    Uninstalling botocore-1.19.0:\n",
            "      Successfully uninstalled botocore-1.19.0\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: wcwidth 0.2.5\n",
            "    Uninstalling wcwidth-0.2.5:\n",
            "      Successfully uninstalled wcwidth-0.2.5\n",
            "Successfully installed awscli-1.18.160 awsebcli-3.19.1 bcrypt-3.2.0 blessed-1.17.11 botocore-1.18.18 cached-property-1.5.2 cement-2.8.2 colorama-0.4.3 cryptography-3.1.1 docker-4.3.1 docker-compose-1.25.5 dockerpty-0.4.1 docutils-0.15.2 idna-2.7 paramiko-2.7.2 pathspec-0.5.9 pynacl-1.4.0 python-dateutil-2.8.0 requests-2.20.1 rsa-4.5 semantic-version-2.5.0 six-1.11.0 texttable-1.6.3 wcwidth-0.1.9 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "botocore",
                  "dateutil",
                  "six",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS6tcGHqyBOn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZwO0aMXyBOq",
        "outputId": "2bb84fc9-3e07-43f8-edc6-ffad1203dbc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nsmc'...\n",
            "remote: Enumerating objects: 14763, done.\u001b[K\n",
            "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
            "Receiving objects: 100% (14763/14763), 56.19 MiB | 20.92 MiB/s, done.\n",
            "Resolving deltas: 100% (1749/1749), done.\n",
            "Checking out files: 100% (14737/14737), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52zTF3WJWKmn"
      },
      "source": [
        "class NsmcDataset(Dataset):\n",
        "    ''' Naver Sentiment Movie Corpus Dataset '''\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.iloc[idx, 1]\n",
        "        label = self.df.iloc[idx, 2]\n",
        "        return text, label"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EammALOEyBOt"
      },
      "source": [
        "nsmc_train_dataset = NsmcDataset(train_df)\n",
        "train_loader = DataLoader(nsmc_train_dataset, batch_size=2, shuffle=True, num_workers=2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4agoxPOXyBOv",
        "outputId": "92357790-284c-405e-9b4b-96907fab039c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "model.to(device)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 995526/995526 [00:00<00:00, 2516069.85B/s]\n",
            "100%|██████████| 625/625 [00:00<00:00, 122772.57B/s]\n",
            "100%|██████████| 714314041/714314041 [00:18<00:00, 38190935.89B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjrwzXPgyBOy"
      },
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "itr = 1\n",
        "p_itr = 500\n",
        "epochs = 1\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for text, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "        sample = torch.tensor(padded_list)\n",
        "        sample, label = sample.to(device), label.to(device)\n",
        "        labels = torch.tensor(label)\n",
        "        outputs = model(sample, labels=labels)\n",
        "        loss, logits = outputs\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "        correct = pred.eq(labels)\n",
        "        total_correct += correct.sum().item()\n",
        "        total_len += len(labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if itr % p_itr == 0:\n",
        "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "            total_loss = 0\n",
        "            total_len = 0\n",
        "            total_correct = 0\n",
        "\n",
        "        itr+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fppvccguyBO0"
      },
      "source": [
        "# evaluation\n",
        "model.eval()\n",
        "\n",
        "nsmc_eval_dataset = NsmcDataset(test_df)\n",
        "eval_loader = DataLoader(nsmc_eval_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
        "\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "for text, label in eval_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "    sample = torch.tensor(padded_list)\n",
        "    sample, label = sample.to(device), label.to(device)\n",
        "    labels = torch.tensor(label)\n",
        "    outputs = model(sample, labels=labels)\n",
        "    _, logits = outputs\n",
        "\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "    correct = pred.eq(labels)\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(labels)\n",
        "\n",
        "print('Test accuracy: ', total_correct / total_len)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Qi47IF4dD8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}