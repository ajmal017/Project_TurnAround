{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "트위터 크롤러\n",
      "입력 형식에 맞게 입력해주세요.\n",
      " 시작하시려면 Enter를 눌러주세요.\n",
      "==================================================\n",
      "검색어 입력(블록체인): 주식\n",
      "검색 시작 날짜 입력(2018-01-01): 2018-01-01\n",
      "검색 끝 날짜 입력(2018-01-02): 2018-01-02\n",
      "[[], []]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-29540130cc4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mcrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstartdate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menddate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-29540130cc4e>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0menddate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"검색 끝 날짜 입력(2018-01-02): \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mcrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstartdate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menddate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-29540130cc4e>\u001b[0m in \u001b[0;36mcrawler\u001b[1;34m(query, startdate, enddate)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotaltweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotaltweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotaldate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotaltweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from selenium import webdriver \n",
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "from selenium.webdriver.common.desired_capabilities import  DesiredCapabilities \n",
    "import time \n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import datetime as dt \n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Created on Thu Feb 14 10:47:07 2019\n",
    "\n",
    "@author: User\n",
    "\"\"\"\n",
    "\n",
    "browser = 'C:/Users/user/Project_cslee/twitter_crawling-master/chromedriver_win32/chromedriver.exe' \n",
    "RESULT_PATH ='C:/Users/user/Project_cslee/twitter_crawling-master/chromedriver_win32/twitter_crawling_ex_result'  #결과 저장할 경로\n",
    "driver = webdriver.Chrome(browser)\n",
    "\n",
    "def crawler(query,startdate,enddate):\n",
    "    startdate_lsit=startdate.split('-')\n",
    "    start_d=startdate_lsit.pop()\n",
    "    start_m=startdate_lsit.pop()\n",
    "    start_y=startdate_lsit.pop()\n",
    "    \n",
    "    enddate_list=enddate.split('-')\n",
    "    end_d=enddate_list.pop()\n",
    "    end_m=enddate_list.pop()\n",
    "    end_y=enddate_list.pop()\n",
    "    \n",
    "    \n",
    "    startdate=dt.date(year=int(start_y),month=int(start_m),day=int(start_d)) #시작날짜 \n",
    "    enddate=dt.date(year=int(end_y),month=int(end_m),day=int(end_d)+1) # 끝날짜\n",
    "    untildate=dt.date(year=int(start_y),month=int(start_m),day=int(start_d)+1) # 시작날짜 +1 \n",
    "    \n",
    "    totaltweets=[] \n",
    "    totaldate=[]\n",
    "    while not enddate==startdate: \n",
    "        \n",
    "        url='https://twitter.com/search?q='+query+'%20since%3A'+str(startdate)+'%20until%3A'+str(untildate)+'&amp;amp;amp;amp;amp;amp;lang=eg' \n",
    "        driver.get(url) \n",
    "        html = driver.page_source \n",
    "        soup=BeautifulSoup(html,'html.parser') \n",
    "        \n",
    "        lastHeight = driver.execute_script(\"return document.body.scrollHeight\") \n",
    "        while True: \n",
    "            \n",
    "            dailyfreq={'Date':startdate} \n",
    "    \n",
    "            wordfreq=0 \n",
    "            tweets=soup.find_all(\"span\", {\"class\": \"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\"}) \n",
    "            date=soup.find_all(\"span\",{\"class\": \"_timestamp js-short-timestamp\"})\n",
    "            #date=soup.find_all(\"time\",{\"datetime\": \"2018-\"})\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "            time.sleep(1) \n",
    "             \n",
    "            newHeight = driver.execute_script(\"return document.body.scrollHeight\") \n",
    "             \n",
    "            if newHeight != lastHeight: \n",
    "                html = driver.page_source \n",
    "                soup=BeautifulSoup(html,'html.parser') \n",
    "                \n",
    "                tweets=soup.find_all(\"span\", {\"class\": \"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\"}) \n",
    "                #tweets=soup.find_all(\"p\", {\"class\": \"TweetTextSize\"}) \n",
    "                date=soup.find_all(\"span\",{\"class\": \"_timestamp js-short-timestamp\"})\n",
    "                wordfreq=len(tweets) \n",
    "            else: \n",
    "                dailyfreq['Frequency']=wordfreq \n",
    "                wordfreq=0 \n",
    "                startdate=untildate \n",
    "                untildate+=dt.timedelta(days=1) \n",
    "                dailyfreq={} \n",
    "                totaltweets.append(tweets) \n",
    "                totaldate.append(date)\n",
    "                break \n",
    "    \n",
    "            lastHeight = newHeight\n",
    "      \n",
    "    print(totaldate)\n",
    "    df = pd.DataFrame(columns=['date','message'])\n",
    "    #print(df)\n",
    "    number=1 \n",
    "    for i in range(len(totaltweets)): \n",
    "        for j in range(len(totaltweets[i])): \n",
    "            df = df.append({'date': (totaldate[i][j]).text,'message':(totaltweets[i][j]).text}, ignore_index=True) \n",
    "            number = number+1\n",
    "    print(number)\n",
    "    print(df)\n",
    "    df.to_excel(RESULT_PATH+\"twitter_result.xlsx\",sheet_name='sheet1')\n",
    "\n",
    "\n",
    "def main():\n",
    "    info_main = input(\"=\"*50+\"\\n\"+\"트위터 크롤러\"+\"\\n\"+\"입력 형식에 맞게 입력해주세요.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
    "    query=input(\"검색어 입력(블록체인): \") \n",
    "    startdate=input(\"검색 시작 날짜 입력(2018-01-01): \") \n",
    "    enddate=input(\"검색 끝 날짜 입력(2018-01-02): \")\n",
    "    \n",
    "    crawler(query,startdate,enddate)\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
