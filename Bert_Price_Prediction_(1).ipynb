{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": true,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Price Prediction (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejihoon6065/Project_TurnAround/blob/Hyundai/Bert_Price_Prediction_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyF9_YmbOFTP",
        "colab_type": "text"
      },
      "source": [
        "# Intro "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji4WyRC0OFTP",
        "colab_type": "text"
      },
      "source": [
        "When we want to predict next day's (week's or month's even) prices of a certain stock, first thing we do is to get as much as information about a company and 'guess' what it will be likely. This was usually done by hands without much help from using computers in the past. Even if one was used, it did not help much because of limits on resources such as computing power. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ3roKTHOFTQ",
        "colab_type": "text"
      },
      "source": [
        "However, as technology is getting better and faster computers are manufactured every second, we began to start utilizing them to help us for predicetion. In this post, I am sharing what I did to predict DJIA's adjusted closing prices with news articles as input features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6GqxT1sOFTQ",
        "colab_type": "text"
      },
      "source": [
        "The data used is from [Kaggle Dataset](https://www.kaggle.com/aaron7sun/stocknews), uploaded by Aaron7sun. It has 25 news articles each day from 2008-06-08 to 2016-07-01, total of 1989 days of samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW6hyqMjOFTR",
        "colab_type": "text"
      },
      "source": [
        "There are three csv files but I only used 'Combined_News_DJIA' because I made a model that predicts only with articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H50fFwrOFTR",
        "colab_type": "text"
      },
      "source": [
        "Common approaches before was just to use RNN, GRU, LSTM or ARIMA models that rely on past values. However, my approach was to use same day's news articles and try to get how much they affect the day's opening value. If it affects positively, the closing will result in higher value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGlsQsiCOFTS",
        "colab_type": "text"
      },
      "source": [
        "Since the data is in string format and not numeric, I used pre-trained BERT to convert them into vectors of floating values, which I got from [Mxnet's Model Zoo](https://gluon-nlp.mxnet.io/model_zoo/bert/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MEgA2IEOFTS",
        "colab_type": "text"
      },
      "source": [
        "### What is BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKcEzGtxOFTT",
        "colab_type": "text"
      },
      "source": [
        "BERT is an encoder that given sets of words (or phrases), converts them into appropriate floating values. Unlike word2vec which has fixed value for each word, it can capture significance of a word in a sentence. So for the same word in two different sentences, it can output different values if it has different meaning or impact on them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6slAIoBOFTT",
        "colab_type": "text"
      },
      "source": [
        "As an example, we can look at two sentences.\n",
        "1. I hate seeing you\n",
        "2. I hate leaving you\n",
        "\n",
        "If we are to predict my feeling about you with word2vec, we are forced to make a model only with 'seeing' and 'leaving' because they both contain 'I', 'hate', and 'you' in the same position that the model will not gain much from them. But if we use BERT, it's possible to capture that 'hate seeing' has negative feeling while 'hate leaving' has positive one because 'hate' will then have differert values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVVDMFfGOFTT",
        "colab_type": "text"
      },
      "source": [
        "Another example of is to predict a rating of a restaurant. With the sentence 'Bob hates this restaurant', word2vec might have following values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVJnBGYHOFTU",
        "colab_type": "text"
      },
      "source": [
        "1. Bob : 3\n",
        "2. hates : -7\n",
        "3. this : 0\n",
        "4. restaurant : 3\n",
        "\n",
        "If we make a (naive) model that just sums up values, with above numbers and predict if Bob's rating will be positive or negative, we will get a negative rating. But what happens if we change 'hates' to 'dislikes' which has the value of -5. Then the output of the model will be positive with the value of 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzMZGvQfOFTU",
        "colab_type": "text"
      },
      "source": [
        "If we define a model with word2vec, we would have to consider all kinds of possibility and many different combinations to correctly output a desired result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOBh_DRYOFTU",
        "colab_type": "text"
      },
      "source": [
        "This is where BERT differs from word2vec as it has the capability of capturing each word's impact. As the purpose of the post is not about BERT, I will skip the rest of the explanation and will have another in later post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHIK_mQGOFTV",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-25T21:58:32.565197Z",
          "start_time": "2019-05-25T21:58:32.556891Z"
        },
        "id": "ZswFcwLEOFTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX1ooUCEOPMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1fd8fda9-f153-41c7-9381-b8c73d897caa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-25T21:58:32.886257Z",
          "start_time": "2019-05-25T21:58:32.709164Z"
        },
        "id": "vH7E9rQ7OFTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "combined_news_path = '/content/drive/My Drive/Combined_News_DJIA.csv'\n",
        "\n",
        "news_djia = pd.read_csv(combined_news_path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-25T21:58:33.822885Z",
          "start_time": "2019-05-25T21:58:33.750127Z"
        },
        "id": "buU4NKnuOFTa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f1eb053-d49a-496b-fd7d-d32952871318"
      },
      "source": [
        "news_djia.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1989, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-25T21:58:45.774321Z",
          "start_time": "2019-05-25T21:58:45.723987Z"
        },
        "id": "-GltsZoPOFTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "06d9f39b-ae14-4b5e-f123-57b90afbc4d6"
      },
      "source": [
        "news_djia.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Label</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-08-08</td>\n",
              "      <td>0</td>\n",
              "      <td>b\"Georgia 'downs two Russian warplanes' as cou...</td>\n",
              "      <td>b'BREAKING: Musharraf to be impeached.'</td>\n",
              "      <td>b'Russia Today: Columns of troops roll into So...</td>\n",
              "      <td>b'Russian tanks are moving towards the capital...</td>\n",
              "      <td>b\"Afghan children raped with 'impunity,' U.N. ...</td>\n",
              "      <td>b'150 Russian tanks have entered South Ossetia...</td>\n",
              "      <td>b\"Breaking: Georgia invades South Ossetia, Rus...</td>\n",
              "      <td>b\"The 'enemy combatent' trials are nothing but...</td>\n",
              "      <td>b'Georgian troops retreat from S. Osettain cap...</td>\n",
              "      <td>b'Did the U.S. Prep Georgia for War with Russia?'</td>\n",
              "      <td>b'Rice Gives Green Light for Israel to Attack ...</td>\n",
              "      <td>b'Announcing:Class Action Lawsuit on Behalf of...</td>\n",
              "      <td>b\"So---Russia and Georgia are at war and the N...</td>\n",
              "      <td>b\"China tells Bush to stay out of other countr...</td>\n",
              "      <td>b'Did World War III start today?'</td>\n",
              "      <td>b'Georgia Invades South Ossetia - if Russia ge...</td>\n",
              "      <td>b'Al-Qaeda Faces Islamist Backlash'</td>\n",
              "      <td>b'Condoleezza Rice: \"The US would not act to p...</td>\n",
              "      <td>b'This is a busy day:  The European Union has ...</td>\n",
              "      <td>b\"Georgia will withdraw 1,000 soldiers from Ir...</td>\n",
              "      <td>b'Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
              "      <td>b'Caucasus in crisis: Georgia invades South Os...</td>\n",
              "      <td>b'Indian shoe manufactory  - And again in a se...</td>\n",
              "      <td>b'Visitors Suffering from Mental Illnesses Ban...</td>\n",
              "      <td>b\"No Help for Mexico's Kidnapping Surge\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-08-11</td>\n",
              "      <td>1</td>\n",
              "      <td>b'Why wont America and Nato help us? If they w...</td>\n",
              "      <td>b'Bush puts foot down on Georgian conflict'</td>\n",
              "      <td>b\"Jewish Georgian minister: Thanks to Israeli ...</td>\n",
              "      <td>b'Georgian army flees in disarray as Russians ...</td>\n",
              "      <td>b\"Olympic opening ceremony fireworks 'faked'\"</td>\n",
              "      <td>b'What were the Mossad with fraudulent New Zea...</td>\n",
              "      <td>b'Russia angered by Israeli military sale to G...</td>\n",
              "      <td>b'An American citizen living in S.Ossetia blam...</td>\n",
              "      <td>b'Welcome To World War IV! Now In High Definit...</td>\n",
              "      <td>b\"Georgia's move, a mistake of monumental prop...</td>\n",
              "      <td>b'Russia presses deeper into Georgia; U.S. say...</td>\n",
              "      <td>b'Abhinav Bindra wins first ever Individual Ol...</td>\n",
              "      <td>b' U.S. ship heads for Arctic to define territ...</td>\n",
              "      <td>b'Drivers in a Jerusalem taxi station threaten...</td>\n",
              "      <td>b'The French Team is Stunned by Phelps and the...</td>\n",
              "      <td>b'Israel and the US behind the Georgian aggres...</td>\n",
              "      <td>b'\"Do not believe TV, neither Russian nor Geor...</td>\n",
              "      <td>b'Riots are still going on in Montreal (Canada...</td>\n",
              "      <td>b'China to overtake US as largest manufacturer'</td>\n",
              "      <td>b'War in South Ossetia [PICS]'</td>\n",
              "      <td>b'Israeli Physicians Group Condemns State Tort...</td>\n",
              "      <td>b' Russia has just beaten the United States ov...</td>\n",
              "      <td>b'Perhaps *the* question about the Georgia - R...</td>\n",
              "      <td>b'Russia is so much better at war'</td>\n",
              "      <td>b\"So this is what it's come to: trading sex fo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  ...                                              Top25\n",
              "0  2008-08-08  ...           b\"No Help for Mexico's Kidnapping Surge\"\n",
              "1  2008-08-11  ...  b\"So this is what it's come to: trading sex fo...\n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-25T21:59:05.747912Z",
          "start_time": "2019-05-25T21:59:03.775101Z"
        },
        "id": "rqlbPP-eOFTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_djia = news_djia.drop(labels='Label', axis=1)\n",
        "\n",
        "# Some column values are not in string so convert them\n",
        "news_djia = news_djia.apply(lambda x: x.map(lambda y: str(y)), axis=1)\n",
        "\n",
        "# Remove starting b' and b\" characters\n",
        "news_djia = news_djia.apply(lambda x: x.map(lambda y: y.replace('b\"', '').replace(\"b'\", '').replace('\"', '')), axis=1)\n",
        "\n",
        "# Set each strings of articles to list of articles for bert_embedding\n",
        "news_djia.iloc[:, 1:] = news_djia.iloc[:, 1:].apply(lambda x: x.map(lambda y: [y]), axis=1)\n",
        "\n",
        "# Move Date to Index\n",
        "news_djia = news_djia.set_index(news_djia.iloc[:, 0]).drop('Date', axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-25T21:59:13.299811Z",
          "start_time": "2019-05-25T21:59:13.245368Z"
        },
        "id": "vGI-OcwNOFTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "8d0e9398-8284-4fb2-e1cb-8f1898c31e80"
      },
      "source": [
        "news_djia.head(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>[Georgia 'downs two Russian warplanes' as coun...</td>\n",
              "      <td>[BREAKING: Musharraf to be impeached.']</td>\n",
              "      <td>[Russia Today: Columns of troops roll into Sou...</td>\n",
              "      <td>[Russian tanks are moving towards the capital ...</td>\n",
              "      <td>[Afghan children raped with 'impunity,' U.N. o...</td>\n",
              "      <td>[150 Russian tanks have entered South Ossetia ...</td>\n",
              "      <td>[Breaking: Georgia invades South Ossetia, Russ...</td>\n",
              "      <td>[The 'enemy combatent' trials are nothing but ...</td>\n",
              "      <td>[Georgian troops retreat from S. Osettain capi...</td>\n",
              "      <td>[Did the U.S. Prep Georgia for War with Russia?']</td>\n",
              "      <td>[Rice Gives Green Light for Israel to Attack I...</td>\n",
              "      <td>[Announcing:Class Action Lawsuit on Behalf of ...</td>\n",
              "      <td>[So---Russia and Georgia are at war and the NY...</td>\n",
              "      <td>[China tells Bush to stay out of other countri...</td>\n",
              "      <td>[Did World War III start today?']</td>\n",
              "      <td>[Georgia Invades South Ossetia - if Russia get...</td>\n",
              "      <td>[Al-Qaeda Faces Islamist Backlash']</td>\n",
              "      <td>[Condoleezza Rice: The US would not act to pre...</td>\n",
              "      <td>[This is a busy day:  The European Union has a...</td>\n",
              "      <td>[Georgia will withdraw 1,000 soldiers from Ira...</td>\n",
              "      <td>[Why the Pentagon Thinks Attacking Iran is a B...</td>\n",
              "      <td>[Caucasus in crisis: Georgia invades South Oss...</td>\n",
              "      <td>[Indian shoe manufactory  - And again in a ser...</td>\n",
              "      <td>[Visitors Suffering from Mental Illnesses Bann...</td>\n",
              "      <td>[No Help for Mexico's Kidnapping Surge]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "      <td>[Why wont America and Nato help us? If they wo...</td>\n",
              "      <td>[Bush puts foot down on Georgian conflict']</td>\n",
              "      <td>[Jewish Georgian minister: Thanks to Israeli t...</td>\n",
              "      <td>[Georgian army flees in disarray as Russians a...</td>\n",
              "      <td>[Olympic opening ceremony fireworks 'faked']</td>\n",
              "      <td>[What were the Mossad with fraudulent New Zeal...</td>\n",
              "      <td>[Russia angered by Israeli military sale to Ge...</td>\n",
              "      <td>[An American citizen living in S.Ossetia blame...</td>\n",
              "      <td>[Welcome To World War IV! Now In High Definiti...</td>\n",
              "      <td>[Georgia's move, a mistake of monumental propo...</td>\n",
              "      <td>[Russia presses deeper into Georgia; U.S. says...</td>\n",
              "      <td>[Abhinav Bindra wins first ever Individual Oly...</td>\n",
              "      <td>[ U.S. ship heads for Arctic to define territo...</td>\n",
              "      <td>[Drivers in a Jerusalem taxi station threaten ...</td>\n",
              "      <td>[The French Team is Stunned by Phelps and the ...</td>\n",
              "      <td>[Israel and the US behind the Georgian aggress...</td>\n",
              "      <td>[Do not believe TV, neither Russian nor Georgi...</td>\n",
              "      <td>[Riots are still going on in Montreal (Canada)...</td>\n",
              "      <td>[China to overtake US as largest manufacturer']</td>\n",
              "      <td>[War in South Ossetia [PICS]']</td>\n",
              "      <td>[Israeli Physicians Group Condemns State Tortu...</td>\n",
              "      <td>[ Russia has just beaten the United States ove...</td>\n",
              "      <td>[Perhaps *the* question about the Georgia - Ru...</td>\n",
              "      <td>[Russia is so much better at war']</td>\n",
              "      <td>[So this is what it's come to: trading sex for...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         Top1  ...                                              Top25\n",
              "Date                                                           ...                                                   \n",
              "2008-08-08  [Georgia 'downs two Russian warplanes' as coun...  ...            [No Help for Mexico's Kidnapping Surge]\n",
              "2008-08-11  [Why wont America and Nato help us? If they wo...  ...  [So this is what it's come to: trading sex for...\n",
              "\n",
              "[2 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16YkEcZSOFTk",
        "colab_type": "text"
      },
      "source": [
        "I removed the label column and moved the date values to index. Then I removed starting b' or b\" since it is not an actual word that I need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dujw91cFOFTk",
        "colab_type": "text"
      },
      "source": [
        "The reason I converted a string to a list of words is so that BERT will output values for each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGSftjtPOFTk",
        "colab_type": "text"
      },
      "source": [
        "It is possible some news articles contain non-alphanumeric but I did not preprocess them but doing so will likely improve a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20RM195AOFTl",
        "colab_type": "text"
      },
      "source": [
        "You can [download files](https://gluon-nlp.mxnet.io/_downloads/sentence_embedding.zip) necessary to run BERT from [Mxnet BERT page](https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html). Also to run it, you have to install mxnet with pip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBFkL2NOOFTl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "0cbd4a00-7864-476f-ae00-84873473dc47"
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 52kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.18.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.7.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to35aaU1PINJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "79f11d25-8b5a-4af8-d830-0cd7fb093a04"
      },
      "source": [
        "!pip install bert"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/e6/55ed98ef52b168a38192da1aff7265c640f214009790220664ee3b4cb52a/bert-2.2.0.tar.gz\n",
            "Collecting erlastic\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/30/f40d99fe35c38c2e0415b1e746c89569f2483e64ef65d054b9f0f382f234/erlastic-2.0.0.tar.gz\n",
            "Building wheels for collected packages: bert, erlastic\n",
            "  Building wheel for bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert: filename=bert-2.2.0-cp36-none-any.whl size=3756 sha256=961fda593a0dc0663ba5e9449d35a2e1c75d664f26d083124fe9f7cd26461582\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/71/b7/941459453bd38e5d97a8c886361dee19325e9933c9cf88ad46\n",
            "  Building wheel for erlastic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for erlastic: filename=erlastic-2.0.0-cp36-none-any.whl size=6789 sha256=3795262de60812756169c5628cfa6bb85640a55ca4342df5f56358fc1d0f5506\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/62/46/93c713a5f061aeeb4f16eb6bf5ee798816e6ddda70faa78e69\n",
            "Successfully built bert erlastic\n",
            "Installing collected packages: erlastic, bert\n",
            "Successfully installed bert-2.2.0 erlastic-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZmNj-dqPTIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "084beef3-3d31-4ab9-ce65-bcf25b2e2911"
      },
      "source": [
        "!pip install bert.embedding"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert.embedding\n",
            "  Downloading https://files.pythonhosted.org/packages/62/85/e0d56e29a055d8b3ba6da6e52afe404f209453057de95b90c01475c3ff75/bert_embedding-1.0.1-py3-none-any.whl\n",
            "Collecting typing==3.6.6\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl\n",
            "Collecting gluonnlp==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/07/037585c23bccec19ce333b402997d98b09e43cc8d2d86dc810d57249c5ff/gluonnlp-0.6.0.tar.gz (209kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 10.7MB/s \n",
            "\u001b[?25hCollecting numpy==1.14.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8MB 240kB/s \n",
            "\u001b[?25hCollecting mxnet==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/e9/241aadccc4522f99adee5b6043f730d58adb7c001e0a68865a3728c3b4ae/mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6MB)\n",
            "\u001b[K     |████████████████████████████████| 29.6MB 105kB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.4.0->bert.embedding) (0.8.4)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.4.0->bert.embedding) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert.embedding) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert.embedding) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert.embedding) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert.embedding) (1.24.3)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-cp36-none-any.whl size=259917 sha256=674d9dfe2f1a8991bcf0e3d724b22fa88ba9530a53f4119194c81ce779e0dfae\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/48/ac/a77c79aa416ba6dd7bf487f2280b0471034f66141617965914\n",
            "Successfully built gluonnlp\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.8.25 has requirement numpy>=1.15.1, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.0.1.post1 has requirement numpy>=1.16, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: typing, numpy, gluonnlp, mxnet, bert.embedding\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: gluonnlp 1.0.0.dev20200914\n",
            "    Uninstalling gluonnlp-1.0.0.dev20200914:\n",
            "      Successfully uninstalled gluonnlp-1.0.0.dev20200914\n",
            "  Found existing installation: mxnet 1.7.0.post1\n",
            "    Uninstalling mxnet-1.7.0.post1:\n",
            "      Successfully uninstalled mxnet-1.7.0.post1\n",
            "Successfully installed bert.embedding gluonnlp-0.6.0 mxnet-1.4.0 numpy-1.14.6 typing-3.6.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "mxnet",
                  "numpy",
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFjJMuBtPood",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c5e5697-05a0-477b-9800-3e20a3af8107"
      },
      "source": [
        "!pip install https://github.com/dmlc/gluon-nlp/tarball/master"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/dmlc/gluon-nlp/tarball/master\n",
            "\u001b[?25l  Downloading https://github.com/dmlc/gluon-nlp/tarball/master (892kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gluonnlp==1.0.0.dev20200914) (1.18.5)\n",
            "Collecting sacremoses>=0.0.38\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 5.2MB/s \n",
            "\u001b[?25hCollecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/20/6326a9a0c6f0527612bae748c4c03df5cd69cf06dfb2cf59d85c6e165a6a/flake8-3.8.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp==1.0.0.dev20200914) (20.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gluonnlp==1.0.0.dev20200914) (2019.12.20)\n",
            "Collecting contextvars\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from gluonnlp==1.0.0.dev20200914) (0.14.1)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from gluonnlp==1.0.0.dev20200914) (3.12.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from gluonnlp==1.0.0.dev20200914) (1.0.5)\n",
            "Collecting tokenizers==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp==1.0.0.dev20200914) (7.1.2)\n",
            "Collecting youtokentome>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 56.0MB/s \n",
            "\u001b[?25hCollecting fasttext!=0.9.2,>=0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from gluonnlp==1.0.0.dev20200914) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses>=0.0.38->gluonnlp==1.0.0.dev20200914) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses>=0.0.38->gluonnlp==1.0.0.dev20200914) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses>=0.0.38->gluonnlp==1.0.0.dev20200914) (4.41.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs>=0.1.6->gluonnlp==1.0.0.dev20200914) (3.13)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting pyflakes<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/5b/fd01b0c696f2f9a6d2c839883b642493b431f28fa32b29abc465ef675473/pyflakes-2.2.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from flake8->gluonnlp==1.0.0.dev20200914) (1.7.0)\n",
            "Collecting pycodestyle<2.7.0,>=2.6.0a1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==1.0.0.dev20200914) (2.4.7)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->gluonnlp==1.0.0.dev20200914) (49.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->gluonnlp==1.0.0.dev20200914) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->gluonnlp==1.0.0.dev20200914) (2018.9)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext!=0.9.2,>=0.9.1->gluonnlp==1.0.0.dev20200914) (2.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8->gluonnlp==1.0.0.dev20200914) (3.1.0)\n",
            "Building wheels for collected packages: gluonnlp, sacremoses, contextvars, fasttext\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-1.0.0.dev20200914-cp36-none-any.whl size=282405 sha256=5c45a01306af80e9c2a0847f2e3be8b500b148f4ed3ec0d915e76028f01aa4be\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jxp5a8qr/wheels/6b/b9/9f/7231495bd26c1c760f56d4f0dae5b358624d94d822d9149fac\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=c3f8b822f167cfd3b34e3d3514e7f753bd1ffd7e26ea0a5721a4a5768a68fb30\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=c87765df8ca45d768c8cf6dd790a65ed271cc02af1555331297369b926ba2b63\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2386015 sha256=cbb3bfd20e2f077151040e026a1b3eb9e2a89a4709f9fab5b769329edab14757\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built gluonnlp sacremoses contextvars fasttext\n",
            "Installing collected packages: sacremoses, yacs, portalocker, sacrebleu, pyflakes, mccabe, pycodestyle, flake8, immutables, contextvars, sentencepiece, tokenizers, youtokentome, fasttext, gluonnlp\n",
            "Successfully installed contextvars-2.4 fasttext-0.9.1 flake8-3.8.3 gluonnlp-1.0.0.dev20200914 immutables-0.14 mccabe-0.6.1 portalocker-2.0.0 pycodestyle-2.6.0 pyflakes-2.2.0 sacrebleu-1.4.14 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1 yacs-0.1.8 youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCQQdR4iOFTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mxnet as mx\n",
        "\n",
        "from mxnet import gluon\n",
        "from bert_embedding import BertEmbedding\n",
        "\n",
        "# Get GPU\n",
        "ctx = mx.gpu(0)\n",
        "\n",
        "# Define a model in GPU for faster training\n",
        "bert_embedding = BertEmbedding(model='bert_12_768_12', dataset_name='book_corpus_wiki_en_cased')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34VMQrWqOFTq",
        "colab_type": "text"
      },
      "source": [
        "You can change the model to another and [this page](https://gluon-nlp.mxnet.io/model_zoo/bert/index.html) has parameters for that. Additionally you can change the dataset to a different one. The model I loaded outputs an embedding in the shape of 768, as it can be seen in the name of model. Bigger number will generate bigger features which might boost accuracy of the model so feel free to try different models as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiZkzt9MOFTq",
        "colab_type": "text"
      },
      "source": [
        "Next is the result of passing first two samples into BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90P1Fn8IOFTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "a7f9f87d-f71d-4916-9923-2b00ee4ba064"
      },
      "source": [
        "example_embedding = news_djia.iloc[:2, :].apply(lambda x: x.map(lambda y: bert_embedding(y)))\n",
        "example_embedding"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>[([Georgia, ', downs, two, Russian, warplanes,...</td>\n",
              "      <td>[([BREAKING, :, Musharraf, to, be, impeached, ...</td>\n",
              "      <td>[([Russia, Today, :, Columns, of, troops, roll...</td>\n",
              "      <td>[([Russian, tanks, are, moving, towards, the, ...</td>\n",
              "      <td>[([Afghan, children, raped, with, ', impunity,...</td>\n",
              "      <td>[([150, Russian, tanks, have, entered, South, ...</td>\n",
              "      <td>[([Breaking, :, Georgia, invades, South, Osset...</td>\n",
              "      <td>[([The, ', enemy, combatent, ', trials, are, n...</td>\n",
              "      <td>[([Georgian, troops, retreat, from, S, ., Oset...</td>\n",
              "      <td>[([Did, the, U, ., S, ., Prep, Georgia, for, W...</td>\n",
              "      <td>[([Rice, Gives, Green, Light, for, Israel, to,...</td>\n",
              "      <td>[([Announcing, :, Class, Action, Lawsuit, on, ...</td>\n",
              "      <td>[([So, -, -, -, Russia, and, Georgia, are, at,...</td>\n",
              "      <td>[([China, tells, Bush, to, stay, out, of, othe...</td>\n",
              "      <td>[([Did, World, War, III, start, today, ?, '], ...</td>\n",
              "      <td>[([Georgia, Invades, South, Ossetia, -, if, Ru...</td>\n",
              "      <td>[([Al, -, Qaeda, Faces, Islamist, Backlash, ']...</td>\n",
              "      <td>[([Condoleezza, Rice, :, The, US, would, not, ...</td>\n",
              "      <td>[([This, is, a, busy, day, :, The, European, U...</td>\n",
              "      <td>[([Georgia, will, withdraw, 1, ,, 000, soldier...</td>\n",
              "      <td>[([Why, the, Pentagon, Thinks, Attacking, Iran...</td>\n",
              "      <td>[([Caucasus, in, crisis, :, Georgia, invades, ...</td>\n",
              "      <td>[([Indian, shoe, manufactory, -, And, again, i...</td>\n",
              "      <td>[([Visitors, Suffering, from, Mental, Illnesse...</td>\n",
              "      <td>[([No, Help, for, Mexico, ', s, Kidnapping, Su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "      <td>[([Why, wont, America, and, Nato, help, us, ?,...</td>\n",
              "      <td>[([Bush, puts, foot, down, on, Georgian, confl...</td>\n",
              "      <td>[([Jewish, Georgian, minister, :, Thanks, to, ...</td>\n",
              "      <td>[([Georgian, army, flees, in, disarray, as, Ru...</td>\n",
              "      <td>[([Olympic, opening, ceremony, fireworks, ', f...</td>\n",
              "      <td>[([What, were, the, Mossad, with, fraudulent, ...</td>\n",
              "      <td>[([Russia, angered, by, Israeli, military, sal...</td>\n",
              "      <td>[([An, American, citizen, living, in, S, ., Os...</td>\n",
              "      <td>[([Welcome, To, World, War, IV, !, Now, In, Hi...</td>\n",
              "      <td>[([Georgia, ', s, move, ,, a, mistake, of, mon...</td>\n",
              "      <td>[([Russia, presses, deeper, into, Georgia, ;, ...</td>\n",
              "      <td>[([Abhinav, Bindra, wins, first, ever, Individ...</td>\n",
              "      <td>[([U, ., S, ., ship, heads, for, Arctic, to, d...</td>\n",
              "      <td>[([Drivers, in, a, Jerusalem, taxi, station, t...</td>\n",
              "      <td>[([The, French, Team, is, Stunned, by, Phelps,...</td>\n",
              "      <td>[([Israel, and, the, US, behind, the, Georgian...</td>\n",
              "      <td>[([Do, not, believe, TV, ,, neither, Russian, ...</td>\n",
              "      <td>[([Riots, are, still, going, on, in, Montreal,...</td>\n",
              "      <td>[([China, to, overtake, US, as, largest, manuf...</td>\n",
              "      <td>[([War, in, South, Ossetia, [, PICS, ], '], [[...</td>\n",
              "      <td>[([Israeli, Physicians, Group, Condemns, State...</td>\n",
              "      <td>[([Russia, has, just, beaten, the, United, Sta...</td>\n",
              "      <td>[([Perhaps, *, the, *, question, about, the, G...</td>\n",
              "      <td>[([Russia, is, so, much, better, at, war, '], ...</td>\n",
              "      <td>[([So, this, is, what, it, ', s, come, to, :, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         Top1  ...                                              Top25\n",
              "Date                                                           ...                                                   \n",
              "2008-08-08  [([Georgia, ', downs, two, Russian, warplanes,...  ...  [([No, Help, for, Mexico, ', s, Kidnapping, Su...\n",
              "2008-08-11  [([Why, wont, America, and, Nato, help, us, ?,...  ...  [([So, this, is, what, it, ', s, come, to, :, ...\n",
              "\n",
              "[2 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4SjN8_KOFTt",
        "colab_type": "text"
      },
      "source": [
        "The output of bert_embedding is a tuple whose first entry is words and second is the floating values corresponding to each of them. Since I did not need any string values, I extracted numeric values by doing next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWofWJgKTuas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "nNJ-48_wOFTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(x):\n",
        "    \n",
        "    # Compact code\n",
        "    # return np.array(x[0][1]).sum(axis=0)\n",
        "    \n",
        "    features = np.array(x[0][1])\n",
        "    features = features.sum(axis=0)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KET9vgIpOFTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "c41655c7-4505-4e54-cb22-16aa49953a64"
      },
      "source": [
        "example_embedding = example_embedding.apply(lambda x: x.map(extract_features))\n",
        "example_embedding"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>[3.4343593, 0.89766926, -2.2731159, 2.6207273,...</td>\n",
              "      <td>[1.5841044, 0.3651707, -2.7459157, -0.7592665,...</td>\n",
              "      <td>[1.6664425, 0.78698075, -1.8210568, 0.19420537...</td>\n",
              "      <td>[1.2475847, -1.6762671, -4.9052677, -1.2343652...</td>\n",
              "      <td>[3.7802272, -0.58494264, -4.4265456, -2.084756...</td>\n",
              "      <td>[6.2831354, -2.1698499, -4.3652363, -0.2826851...</td>\n",
              "      <td>[2.5234256, -4.296561, -1.4806514, -0.82675654...</td>\n",
              "      <td>[4.6181684, 1.6290157, -2.4697719, 1.2765439, ...</td>\n",
              "      <td>[3.6747978, -4.586723, -3.1463275, -2.662077, ...</td>\n",
              "      <td>[2.273276, -6.1183367, -3.834559, -3.4150221, ...</td>\n",
              "      <td>[2.498811, -4.485126, -4.4073887, -2.6550674, ...</td>\n",
              "      <td>[-1.7069225, -1.0635066, -3.5754585, 2.8024619...</td>\n",
              "      <td>[-1.2736461, -2.8189285, -4.58098, 4.570773, 7...</td>\n",
              "      <td>[-0.5036616, -0.7411705, -1.5258787, -1.235189...</td>\n",
              "      <td>[0.7533729, -0.8578297, 0.083378404, 0.7738523...</td>\n",
              "      <td>[4.8692017, -3.098872, -3.3744936, 1.1512774, ...</td>\n",
              "      <td>[0.67757106, 0.40796664, -1.2753848, -0.430121...</td>\n",
              "      <td>[3.9853826, -1.947959, -5.3694806, -2.9952905,...</td>\n",
              "      <td>[3.5076091, 2.4318266, -0.8598722, 1.729223, 2...</td>\n",
              "      <td>[3.2756999, -2.0283217, -3.9403336, -5.527244,...</td>\n",
              "      <td>[2.0910888, 1.1763185, -6.940914, -1.0599109, ...</td>\n",
              "      <td>[1.0531394, -1.4839482, -0.2102995, -0.0948405...</td>\n",
              "      <td>[5.1034346, -1.7355354, -7.810959, 1.496744, 6...</td>\n",
              "      <td>[0.9602088, 3.2546334, -2.083721, -0.19298653,...</td>\n",
              "      <td>[0.9675671, -0.0868776, -1.7552404, 1.6409774,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "      <td>[4.3726134, -2.7928586, -5.8791585, 6.665765, ...</td>\n",
              "      <td>[1.5314145, 0.33340392, -0.26014385, 0.1285088...</td>\n",
              "      <td>[1.4045095, -3.7549505, -3.26506, -0.55188835,...</td>\n",
              "      <td>[1.3028452, -7.201991, -1.9817094, -1.7650374,...</td>\n",
              "      <td>[0.38248762, 1.9304385, -0.9879122, -0.0849634...</td>\n",
              "      <td>[2.7923086, -1.367171, -0.4131552, -2.2261844,...</td>\n",
              "      <td>[2.8151383, -2.6929421, -3.1940496, -0.9433340...</td>\n",
              "      <td>[4.82897, -7.0465093, 0.57416075, -2.9042096, ...</td>\n",
              "      <td>[-0.70016146, 4.2033854, 0.78425217, 3.064478,...</td>\n",
              "      <td>[1.7868799, 0.4861898, -0.59401107, 1.1086818,...</td>\n",
              "      <td>[3.6118486, -0.64954025, 1.7564191, -1.8382015...</td>\n",
              "      <td>[-1.2037139, -2.2116253, -5.0030403, 4.700337,...</td>\n",
              "      <td>[2.426506, -3.475122, -0.7171306, -2.72811, 1....</td>\n",
              "      <td>[1.2772789, -1.779817, 1.5488563, 0.042267412,...</td>\n",
              "      <td>[1.131243, -0.8605994, -1.6347471, -1.7167449,...</td>\n",
              "      <td>[2.896379, -1.5717525, -2.8346763, -1.608649, ...</td>\n",
              "      <td>[7.5717945, -2.2538946, 0.51045245, 0.16108486...</td>\n",
              "      <td>[3.0750847, 1.9874792, -1.6867796, 4.9326515, ...</td>\n",
              "      <td>[1.0883623, 1.6837845, -1.1653569, -0.7572401,...</td>\n",
              "      <td>[1.1617029, -0.7679175, -1.4622602, -1.3730854...</td>\n",
              "      <td>[0.30732304, -0.46207255, -1.5561316, -0.21302...</td>\n",
              "      <td>[-1.5455484, -0.837101, -1.9839878, 0.3174597,...</td>\n",
              "      <td>[1.9832282, -0.8678921, -6.023197, 1.49993, -1...</td>\n",
              "      <td>[0.65908015, -1.0287197, -2.9924257, 0.0656182...</td>\n",
              "      <td>[1.1136361, 3.6621566, -0.060658894, 4.421232,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         Top1  ...                                              Top25\n",
              "Date                                                           ...                                                   \n",
              "2008-08-08  [3.4343593, 0.89766926, -2.2731159, 2.6207273,...  ...  [0.9675671, -0.0868776, -1.7552404, 1.6409774,...\n",
              "2008-08-11  [4.3726134, -2.7928586, -5.8791585, 6.665765, ...  ...  [1.1136361, 3.6621566, -0.060658894, 4.421232,...\n",
              "\n",
              "[2 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpuWpWlyOFTx",
        "colab_type": "text"
      },
      "source": [
        "With the function above, now I have a dataframe with 25 columns of numeric values. Same thing was applied to the whole dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fj6Y5dlOFTx",
        "colab_type": "text"
      },
      "source": [
        "Using BERT model on CPU took more than an hour so I had to use on Google Clout Platform with one Tesla v4 which still took about 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9jSu4A7OFTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_embedding = news_djia.apply(lambda x: x.map(lambda y: bert_embedding(y)))\n",
        "\n",
        "# Remove word and only keep numeric vectors\n",
        "news_embedding = news_embedding.apply(lambda x: x.map(extract_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eRsWdYjOFT1",
        "colab_type": "text"
      },
      "source": [
        "After that, I aggregated all columns into one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jw9Ggf8OFT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_embedding['combined'] = news_embedding.values.tolist()\n",
        "\n",
        "news_embedding = news_embedding[['combined']]\n",
        "\n",
        "news_embedding.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOBSmjJaOFT4",
        "colab_type": "text"
      },
      "source": [
        "Each article differs in the number of words that the shape of each embedding is also different. So I cannot just put them into a model because then it will have to have flexible input size. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biGhTL93OFT4",
        "colab_type": "text"
      },
      "source": [
        "Instead, by using min, max, sum and mean over each data sample's embedding element-wise, I extracted extreme values. For example by using max, it will take the strongest features among others. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVWlMVM0OFT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_embedding = news_embedding['combined'].map(lambda x: np.min(x, axis=0)).to_frame()\n",
        "max_embedding = news_embedding['combined'].map(lambda x: np.max(x, axis=0)).to_frame()\n",
        "sum_embedding = news_embedding['combined'].map(lambda x: np.sum(x, axis=0)).to_frame()\n",
        "mean_embedding = news_embedding['combined'].map(lambda x: np.mean(x, axis=0)).to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3k5Gzh2lOFT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_embedding.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCImiZ8VOFT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save them for easier access later\n",
        "path = 'embedding_files/'\n",
        "\n",
        "min_embedding.to_json(path+'min_embedding.json')\n",
        "max_embedding.to_json(path+'max_embedding.json')\n",
        "sum_embedding.to_json(path+'sum_embedding.json')\n",
        "mean_embedding.to_json(path+'mean_embedding.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJHHKoM7OFT-",
        "colab_type": "text"
      },
      "source": [
        "I had to make a different post for actual model implementation because putting all together was too long for one. You can find it [here]()"
      ]
    }
  ]
}