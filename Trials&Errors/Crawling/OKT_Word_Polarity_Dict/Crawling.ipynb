{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import requests \n",
    "from urllib.request import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./data/kospi.xls', sheet_name = 'Sheet4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2003-04-28\n",
       "1     2001-09-12\n",
       "2     2008-12-14\n",
       "3     2003-03-19\n",
       "4     1998-08-19\n",
       "5     1998-10-30\n",
       "6     2000-10-15\n",
       "7     1998-12-06\n",
       "8     2004-05-18\n",
       "9     2011-08-15\n",
       "10    2000-04-27\n",
       "11    1997-12-15\n",
       "12    1995-05-28\n",
       "13    1998-04-10\n",
       "14    2008-11-25\n",
       "15    1998-01-23\n",
       "16    2007-11-25\n",
       "17    2001-01-11\n",
       "18    2003-02-16\n",
       "19    2001-11-25\n",
       "20    2002-06-27\n",
       "21    2002-10-13\n",
       "22    2008-09-18\n",
       "23    2000-06-08\n",
       "24    2002-01-01\n",
       "25    2001-01-30\n",
       "26    1996-12-18\n",
       "27    1998-02-26\n",
       "28    1999-01-03\n",
       "29    1999-08-22\n",
       "30    2000-06-04\n",
       "31    2000-11-26\n",
       "32    2001-10-03\n",
       "33    1999-03-18\n",
       "34    2001-04-18\n",
       "35    2009-04-08\n",
       "36    2003-03-17\n",
       "37    1998-05-28\n",
       "38    1998-07-17\n",
       "39    1999-02-25\n",
       "40    1999-06-30\n",
       "41    2001-05-16\n",
       "42    1999-01-06\n",
       "43    2004-10-03\n",
       "44    2000-04-06\n",
       "45    1997-01-13\n",
       "46    1998-11-18\n",
       "47    2001-01-04\n",
       "48    2003-01-02\n",
       "49    1998-12-03\n",
       "50    2000-01-09\n",
       "51    1998-10-11\n",
       "52    2002-10-17\n",
       "53    2008-02-13\n",
       "54    1899-12-30\n",
       "55    1899-12-30\n",
       "56    1899-12-30\n",
       "Name: minus2, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['minus2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_date = df['minus2'].dropna().astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003 04 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\user\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 09 12\n",
      "2008 12 14\n",
      "2003 03 19\n",
      "1998 08 19\n",
      "1998 10 30\n",
      "2000 10 15\n",
      "1998 12 06\n",
      "2004 05 18\n",
      "2011 08 15\n",
      "2000 04 27\n",
      "1997 12 15\n",
      "1995 05 28\n",
      "1998 04 10\n",
      "2008 11 25\n",
      "1998 01 23\n",
      "2007 11 25\n",
      "2001 01 11\n",
      "2003 02 16\n",
      "2001 11 25\n",
      "2002 06 27\n",
      "2002 10 13\n",
      "2008 09 18\n",
      "2000 06 08\n",
      "2002 01 01\n",
      "2001 01 30\n",
      "1996 12 18\n",
      "1998 02 26\n",
      "1999 01 03\n",
      "1999 08 22\n",
      "2000 06 04\n",
      "2000 11 26\n",
      "2001 10 03\n",
      "1999 03 18\n",
      "2001 04 18\n",
      "2009 04 08\n",
      "2003 03 17\n",
      "1998 05 28\n",
      "1998 07 17\n",
      "1999 02 25\n",
      "1999 06 30\n",
      "2001 05 16\n",
      "1999 01 06\n",
      "2004 10 03\n",
      "2000 04 06\n",
      "1997 01 13\n",
      "1998 11 18\n",
      "2001 01 04\n",
      "2003 01 02\n",
      "1998 12 03\n",
      "2000 01 09\n",
      "1998 10 11\n",
      "2002 10 17\n",
      "2008 02 13\n",
      "1899 12 30\n",
      "1899 12 30\n",
      "1899 12 30\n"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "\n",
    "\n",
    "for i in range(len(list_date)):\n",
    "    year = str(list_date[i])[0:4]\n",
    "    month = str(list_date[i])[5:7]\n",
    "    day = str(list_date[i])[8:10]\n",
    "    print(year,month,day)\n",
    "\n",
    "    list2 = []\n",
    "    list2.append(str(year)+'.'+month+'.'+day)\n",
    "    for j in range(15) : # range : 페이지 수 입력\n",
    "        page = j+1\n",
    "        url = f'https://hankyung.com/economy?date={year}.{month}.{day}&page={page}'\n",
    "        #url = 'https://www.hankyung.com/economy?date=2020.04.01'\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "        #print(html)\n",
    "        soup = BeautifulSoup(response.content.decode('UTF-8','replace'))\n",
    "        #print(soup)\n",
    "        #tit1 = soup.find_all('div', 'article') # 1~ 10\n",
    "        tit2 = soup.find_all('h3', 'tit') # 나머지 \n",
    "        #print(tit1)\n",
    "        #print(tit2)\n",
    "\n",
    "        for i in range(len(tit2)):\n",
    "            a=str(tit2[i])\n",
    "            a2 = a.split('>')[2]\n",
    "            a3 = a2.split('<')[0]\n",
    "            list2.append(a3)\n",
    "    list1.append(list2)\n",
    "\n",
    "pd_list = pd.DataFrame(list1)\n",
    "pd_list.to_csv(\"result.csv\", mode='w', encoding='utf-8-sig', index=False)\n",
    "    # error뜰 때 인코딩을 utf-8-sig로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
