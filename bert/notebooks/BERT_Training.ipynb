{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End BERT (Training) \n",
    "이 챕터는 TensorFlow BERT Training 과정을 안내하기 위해 작성되었습니다. 이 문서의 목차는 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. 모델 아키텍처](#model)\n",
    "* [2. 데이터셋](#dataset)\n",
    "* [3. NVIDIA가 제공하는 Training  기법 특징](#features)\n",
    "* [4. BERT Training 시작하기](#training)\n",
    "* [5. Training 성능](#performance)\n",
    "* [6. 결론](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "## 1. 모델 아키텍처\n",
    "BERT 의 Training 과정은 크게  Pre-training 단계와 Fine-Tuning 단계로 구분됩니다. Pre-Training단계에서는 방대한 데이터셋으로 부터 언어와 문장 자체에 대한 이해도를 높이고, Fine-Tuning 단계에서는 pre-trained된 파라미터들을 downstream 작업을 통해 특화된 작업을 이해할수 있게 해줍니다.\n",
    "<img src=\"../data/images/bert_pipeline.png?raw=true\" width=\"700\">\n",
    "그림 1. BERT Training pipeline\n",
    "\n",
    "그림 1은 BERT Training pipeline으로 Pre-Training 단계와 Fine-Tuning 단계를 나타냅니다.  Pre-Training 단계에서는 large scale의 unannotated 데이터셋을 이용해 unsupervised 방법으로 Training 을 진행합니다. Fine-Tuning 단계에서는  일반적으로 하나 또는 두개 의 레이어를 추가한 후 특정한 작업을 위해 Training 을 하게 되는데, 특정한 작업을 위한 annotated 데이터를 사용하고 Pre-Trained 모델의 weight를 기반으로 Training 을 진행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "## 2. 데이터셋\n",
    "\n",
    "전체 데이터셋 용량은 170GB 이상이고, 다운로드 하는데 15시간 이상 소요됩니다. BookCorpus 서버가 일시적으로 과부화되어 HTTP 403 이나 503 오류를 발생시킬수 있습니다. 따라서 누락된 파일은 나중에 다시 다운 받으시거나 무시하시길 바랍니다. 데이터셋은 크게 Pre-training을 위한 데이터셋과 Fine-tuning을 위한 데이터셋으로 나뉠 수가 있습니다. \n",
    "* Pre-training을 위한 데이터셋: [Wikipedia](https://dumps.wikimedia.org/) (2.5B words) , [BookCorpus](http://yknzhu.wixsite.com/mbweb) (800M words)\n",
    "* Fine-Tuing을 위한 데이터셋:  [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) (Stanford Question Answering Dataset), [GLUE](https://gluebenchmark.com/) (The General Language Understanding Evaluation benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features'></a>\n",
    "## 3. NVIDIA가 제공하는 BERT Training 기법 특징\n",
    "다음에 소개할 기술들은 추가적인 노력없이 BERT Training 시간을 단축할 수 있습니다. \n",
    "\n",
    "### a. Multi-GPU Training\n",
    "Multi-GPU Training을 위해 horovod를 사용하였습니다. Horovod는 NCCL을 사용하여 효율적인 Multi-GPU training 환경을 제공합니다. 자세한 내용은 [TensorFlow 튜토리얼](https://github.com/horovod/horovod/#usage)을 참고해 주시길 바랍니다.\n",
    "\n",
    "\n",
    "### b. Automatic Mixed Precision (AMP) Training\n",
    "\n",
    "TensorFlow Automatic Mixed Precision (TF-AMP)를 사용하여 Mixed-precision  Training 을 수행합니다. 환경 변수에 의해 제어되며 추가적인 소스 코드 변경 없이 자동적으로 그래프를 재 작성하고 Loss scaling을 수행하여 Training  시간을 단축시켜줍니다.\n",
    "\n",
    "![](https://developer.nvidia.com/sites/default/files/dev-ai-tech-amp-workflow-graphic-950353-r7-web.png)\n",
    "\n",
    "그림 2. Automatic Mixed Precision Training\n",
    "\n",
    "그림 2는 Automatic Mixed Precision Training 단계를 설명한 것으로, Mixed-precision Training 을 사용하려면 두 단계가 필요합니다. FP32의 데이터 타입을 FP16의 데이터타입으로 변경하고, FP16으로 변경함에 따라 gradient 값 손실이 발생하는데 손실되는 gradient 값을 보존하기 위해 loss scaling을 사용합니다.\n",
    "\n",
    "Mixed-Precision Training의 장점은 다음과 같습니다. \n",
    "* 텐서코어를 사용하여 Linear 또는 convolutional 레이어 연산과 같은 math-intensive 연산의 속도를 향상\n",
    "* FP16 precision 은 FP32 precision 보다 절반의 메모리를 사용하여 메모리 절약 효과\n",
    "* Training 모델의 메모리 요구량을 줄여줌으로써 더 큰 모델이나 큰 mini-batch 사용 가능\n",
    "\n",
    "Mixed-precision Training 에 대한 자세한 내용은 다음 링크를 참고 하시길 바랍니다.\n",
    "* [Mixed-precision Training 논문](https://docs.nvidia.com/deeplearning/dgx/tensorflow-user-guide/index.html#tfamp)과 [Mixed-precsision Training 문서](https://docs.nvidia.com/deeplearning/dgx/tensorflow-user-guide/index.html#tfamp)\n",
    "* [Mixed-precision Training 기법 블로그](https://docs.nvidia.com/deeplearning/dgx/tensorflow-user-guide/index.html#tfamp)\n",
    "* [TensorFlow Automatic Mixed-precision User guide 문서](https://docs.nvidia.com/deeplearning/dgx/tensorflow-user-guide/index.html#tfamp)\n",
    "\n",
    "### c. LAMB (Layerwise Adaptive Moments based optimizer)\n",
    "[LAMB](https://arxiv.org/abs/1904.00962)는 Large batch 최적화 기법으로, large mini-batch를 사용하여 딥러닝 모델을 학습 시 가속화하는데 도움을 줍니다. 저희는 gradient pre-normalization과 bias correction 추가하여 LAMB (NVLAMB)를 구현했습니다. 보다 자세한 내용은 [여기](https://medium.com/nvidia-ai/a-guide-to-optimizer-implementation-for-bert-at-scale-8338cc7f45fd)를 참조해주세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "## 4. BERT Training 시작하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Docker build 하기\n",
    "BERT Training 환경을 위해 docker image를 생성하고 빌드합니다. CUDA 10.1과 TensorFlow 1.14.0 기반의 nvcr.io/nvidia/tensorflow:19.08-py3 이미지를 사용해서 빌드했으며 자세한 환경은 [여기](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_19.08.html#rel_19.08)를 참고하시길 바랍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build . --rm -t bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Dataset 다운로드 및 TFRecord 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f bert_training\n",
    "docker run -d --runtime=nvidia -v $PWD:/workspace/bert \\\n",
    "    --rm --shm-size=1g --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 --ipc=host -t -i \\\n",
    "    --name bert_training \\\n",
    "    bert \n",
    "docker exec -t bert bash data/create_datasets_from_start.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-training을 위한 bokscorpus와 wikicorpus_en 데이터셋을 합치고 TFRecord 셋으로 만드는 과정과 Fine-tuning을 위한 SQuAD, GRUE 데이터셋을  다운로드 하는 과정을 포함합니다. 많은 시간이 소요되므로 반드시 여유시간을 가지고 실행시켜주세요. 데이터셋 다운로드 Link가 깨질수 있으니 다시 시도하거나, 깨진 링크는 무시하셔도 됩니다.\n",
    "\n",
    "`data/create_datasets_from_start.sh` 스크립트가 포함하는 코드 구성은 다음과 같습니다. \n",
    "TFRecord 셋으로 만드는 과정에서는 두 파라미터 max_seq_length 와 max_predictions_per_seq를 설정해주시면 됩니다. \n",
    "\n",
    "데이터 생성 스크립트에서는 다음과 같은 파라미터 값을 사용하여 TFRecord 셋을 생성합니다.\n",
    " \n",
    "| max_seq_length | max_predictions_per_seq |\n",
    "|----------------|-------------------------|\n",
    "|   128          |           20            |\n",
    "|   512          |           80            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data/create_datasets_from_start.sh 구성\n",
    "# export BERT_PREP_WORKING_DIR=\"${BERT_PREP_WORKING_DIR}\"\n",
    "\n",
    "# # Download\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action download --dataset bookscorpus\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action download --dataset wikicorpus_en\n",
    "\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action download --dataset google_pretrained_weights  # Includes vocab\n",
    "\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action download --dataset squad\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action download --dataset \"CoLA\"\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action download --dataset \"MRPC\"\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action download --dataset \"MNLI\"\n",
    "\n",
    "\n",
    "# # Properly format the text files\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action text_formatting --dataset bookscorpus\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action text_formatting --dataset wikicorpus_en\n",
    "\n",
    "\n",
    "# # Shard the text files (group wiki+books then shard)\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action sharding --dataset books_wiki_en_corpus\n",
    "\n",
    "\n",
    "# # Create TFRecord files Phase 1\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action create_tfrecord_files --dataset books_wiki_en_corpus --max_seq_length 128 \\\n",
    "#  --max_predictions_per_seq 20 --vocab_file ${BERT_PREP_WORKING_DIR}/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/vocab.txt\n",
    "\n",
    "\n",
    "# # Create TFRecord files Phase 2\n",
    "# python3 ${BERT_PREP_WORKING_DIR}/bertPrep.py --action create_tfrecord_files --dataset books_wiki_en_corpus --max_seq_length 512 \\\n",
    "#  --max_predictions_per_seq 80 --vocab_file ${BERT_PREP_WORKING_DIR}/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개별 실행 코드는 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# docker exec -t bert python3 data/bertPrep.py --action download --dataset bookscorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋과 TFRecord 셋은 기본적으로 다음과 같은 위치에 다운로드 되고 생성됩니다.\n",
    "* SQuAD v1.1 - `data/download/squad/v1.1`\n",
    "* SQuAD v2.0 - `data/download/squad/v2.0`\n",
    "* GLUE The Corpus of Linguistic Acceptability (CoLA) - `data/download/CoLA`\n",
    "* GLUE Microsoft Research Paraphrase Corpus (MRPC) - `data/download/MRPC`\n",
    "* GLUE The Multi-Genre NLI Corpus (MNLI) - `data/download/MNLI`\n",
    "* BERT Large - `data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16`\n",
    "* BERT Base - `data/download/google_pretrained_weights/uncased_L-12_H-768_A-12`\n",
    "* BERT - `data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16`\n",
    "* Wikipedia + BookCorpus TFRecords - `data/tfrecords<config>/books_wiki_en_corpus`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. BERT Training 하기\n",
    "\n",
    "BERT Training은 Pre-training과 Find-Tuinng 단계로 나뉘어 지게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Training 시작하기 (스크립트)\n",
    "\n",
    "BERT는 언어 표현을 위해 bidirectional representations을 pre-train하게 디자인되었습니다. 다음 스크립트는 Wikipedia와 BookCorpus 데이터셋으로 Pre-training을 진행합니다. \n",
    "두 개의 optimizer (LAMB / Adam)를 이용한 pre-training 스크립트를 제공하고 파라미터는 다음과 같습니다. \n",
    "* Pre-Training with LAMB paper \n",
    "\n",
    "`\n",
    "scripts/run_pretraining_lamb.sh <train_batch_size_phase1> <train_batch_size_phase2> <eval_batch_size> <learning_rate_phase1> <learning_rate_phase2> <precision> <use_xla> <num_gpus> <warmup_steps_phase1> <warmup_steps_phase2> <train_steps> <save_checkpoint_steps> <num_accumulation_phase1> <num_accumulation_steps_phase2> <bert_model>\n",
    "`\n",
    "\n",
    "`run_pretraining_lamb.sh` 스크립트는 LAMB optimizer를 사용하고 두 개의 phase로 나눠서 pre-training을 진행합니다. 첫 phase는 sequence length를 128로 설정 한 뒤 전체 training step의 90%를 수행합니다. 두 번째 phase는 sequence length를 512로 설정 한 뒤 나머지 10%를 수행합니다. \n",
    "\n",
    "* Pre-Training with original BERT paper\n",
    "\n",
    "`\n",
    "scripts/run_pretraining_adam.sh <train_batch_size_per_gpu> <eval_batch_size> <learning_rate_per_gpu> <precision> <use_xla> <num_gpus> <warmup_steps> <train_steps> <save_checkpoint_steps> <num_accumulation_steps> <seq_len> <max_pred_per_seq>\n",
    "`\n",
    "\n",
    "`run_pretraining_adam.sh` 스크립트는 Adam optimizaer를 사용합니다. \n",
    "<a id='pre_train_param'></a>\n",
    "파라미터 설명은 다음과 같습니다. \n",
    "\n",
    "* `<training_batch_size_phase*>` 각 respective phase에서의 GPU당 batch size, 큰 batch size는 Training의 효율성을 주지만, 더 많은 메모리를 요구함. \n",
    "\n",
    "* `<eval_batch_size>` Training 후 evaluation에서의 GPU 당 batch size \n",
    "\n",
    "* `<learning_rate_phase1>` batch size가 256일 때 Learning rate는 기본 1e-4 \n",
    "\n",
    "* `<learning_rate_phase2>` batch size가 256일 때 Learning rate는 기본 1e-4 \n",
    "\n",
    "* `<precision>` 모델의 precision(정밀도) fp32 또는 fp16을 선택\n",
    "\n",
    "    * `fp32`: 32-bit IEEE single precision floats 사용\n",
    "    * `fp16`: Automatic Mixed Precision 사용 \n",
    "    \n",
    "* `<use_xla>` XLA JIT 사용 여부\n",
    "* `<num_gpus>` Traning 시 사용하는 GPU 개수. 노드의 GPU 개수보다 같거나 작아야 함.\n",
    "\n",
    "* `<warmup_steps_phase*>` 처음 training을 시작하고 나서 warm-up step 개수\n",
    "\n",
    "* `<training_steps>` 전체 Training step \n",
    "\n",
    "* `<save_checkpoint_steps>` checkpoints 저장 빈도( 기본 100, 100 step마다 checkpoint 저장)\n",
    "\n",
    "* `<num_accumulation_phase*>` weight 업데이트 전에 gradient를 N번 누적하여 respective phase에서 더 높은 batch size를 가지는데 사용\n",
    "\n",
    "* `<bert_model>` Pre-Training시 사용되는 BERT 모델 (large: BERT Large model, base: BERT base model) \n",
    "\n",
    "    * `large` : BERT Large model\n",
    "    * `base` : BERT Base model\n",
    "    \n",
    "    \n",
    "주의: TFRecord 셋으로 구성한 seq_length와 max_prediction_per_seq를 사용하여 모델을 Training하거나, 사용할 seq_length 와 max_predictions_per_seq에 맞는 TFRecord 셋을 구성해줘야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLA과 LAMB optimizer를 사용한 V100 32G 4GPU 에서의 BERT Base FP16 Pre-training 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "docker rm -f bert_training\n",
    "\n",
    "docker run --rm -t \\\n",
    "    --runtime=nvidia \\\n",
    "    --name=bert_training \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -v $PWD:/workspace/bert \\\n",
    "    -v $PWD/results:/results \\\n",
    "    bert bash scripts/run_pretraining_lamb.sh 16 4 8 7.5e-4 5e-4 fp16 true 4 2000 200 7820 100 128 512 base\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Training 시작하기 (커스텀)\n",
    "\n",
    "BERT_CONFIG의 json 파일을 수정해서 BERT모델을 수정하실 수 있습니다. \n",
    "파라미터 설명은 [여기](#pre_train_param)을 참조 하시길 바랍니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "train_batch_size=${1:-14}\n",
    "eval_batch_size=${2:-8}\n",
    "learning_rate=${3:-\"1e-4\"}\n",
    "precision=${4:-\"manual_fp16\"}\n",
    "use_xla=${5:-\"true\"}\n",
    "num_gpus=${6:-8}\n",
    "warmup_steps=${7:-\"10000\"}\n",
    "train_steps=${8:-1144000}\n",
    "save_checkpoints_steps=${9:-5000}\n",
    "bert_model=${10:-\"base\"}\n",
    "num_accumulation_steps=${11:-1}\n",
    "seq_len=${12:-128}\n",
    "max_pred_per_seq=${13:-20}\n",
    "\n",
    "DATA_DIR=data/tfrecord/lower_case_1_seq_len_${seq_len}_max_pred_${max_pred_per_seq}_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus\n",
    "\n",
    "if [ \"$bert_model\" = \"large\" ] ; then\n",
    "    export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_config.json\n",
    "else\n",
    "    export BERT_CONFIG=data/download/google_pretrained_weights/uncased_L-12_H-768_A-12/bert_config.json\n",
    "fi\n",
    "\n",
    "PREC=\"\"\n",
    "if [ \"$precision\" = \"fp16\" ] ; then\n",
    "   PREC=\"--use_fp16\"\n",
    "elif [ \"$precision\" = \"fp32\" ] ; then\n",
    "   PREC=\"\"\n",
    "elif [ \"$precision\" = \"manual_fp16\" ] ; then\n",
    "   PREC=\"--manual_fp16\"\n",
    "else\n",
    "   echo \"Unknown <precision> argument\"\n",
    "   exit -2\n",
    "fi\n",
    "\n",
    "if [ \"$use_xla\" = \"true\" ] ; then\n",
    "    PREC=\"$PREC --use_xla\"\n",
    "    echo \"XLA activated\"\n",
    "fi\n",
    "\n",
    "export GBS=$(expr $train_batch_size \\* $num_gpus \\* $num_accumulation_steps)\n",
    "printf -v TAG \"tf_bert_pretraining_adam_%s_%s_gbs%d\" \"$bert_model\" \"$precision\" $GBS\n",
    "DATESTAMP=`date +'%y%m%d%H%M%S'`\n",
    "\n",
    "#Edit to save logs & checkpoints in a different directory\n",
    "RESULTS_DIR=${RESULTS_DIR:-/results/${TAG}_${DATESTAMP}}\n",
    "LOGFILE=$RESULTS_DIR/$TAG.$DATESTAMP.log\n",
    "mkdir -m 777 -p $RESULTS_DIR\n",
    "printf \"Saving checkpoints to %s\\n\" \"$RESULTS_DIR\"\n",
    "printf \"Logs written to %s\\n\" \"$LOGFILE\"\n",
    "\n",
    "INPUT_FILES=\"$DATA_DIR/training\"\n",
    "EVAL_FILES=\"$DATA_DIR/test\"\n",
    "\n",
    "mpi=\"\"\n",
    "if [ $num_gpus -gt 1 ] ; then\n",
    "   mpi=\"mpiexec --allow-run-as-root -np $num_gpus --bind-to socket\"\n",
    "fi\n",
    "\n",
    "docker rm -f bert_training\n",
    "\n",
    "docker run --rm -t \\\n",
    "    --runtime=nvidia \\\n",
    "    --name=bert_training \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -v $PWD:/workspace/bert \\\n",
    "    -v $PWD/results:/results \\\n",
    "    bert $mpi python3 /workspace/bert/run_pretraining.py \\\n",
    "     --input_files_dir=$INPUT_FILES \\\n",
    "     --eval_files_dir=$EVAL_FILES \\\n",
    "     --output_dir=$RESULTS_DIR \\\n",
    "     --bert_config_file=$BERT_CONFIG \\\n",
    "     --do_train=True \\\n",
    "     --do_eval=True \\\n",
    "     --train_batch_size=$train_batch_size \\\n",
    "     --eval_batch_size=$eval_batch_size \\\n",
    "     --max_seq_length=$seq_len \\\n",
    "     --max_predictions_per_seq=$max_pred_per_seq \\\n",
    "     --num_train_steps=$train_steps \\\n",
    "     --num_accumulation_steps=$num_accumulation_steps \\\n",
    "     --num_warmup_steps=$warmup_steps \\\n",
    "     --save_checkpoints_steps=$save_checkpoints_steps \\\n",
    "     --learning_rate=$learning_rate \\\n",
    "     --horovod $PREC \\\n",
    "     --allreduce_post_accumulation=True\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning 시작하기 (스크립트)\n",
    "\n",
    "FIne-Tuning 단계에서는 기존의 pre-trained BERT 모델에 하나의 output layer를 추가함으로써 Q&A 시스템 문제 및 GRUE 벤치마크를 수행할 수 있습니다. GLUE benchmakr와 Q&A 문제를 Fine-tuning 하기 위한 스크립트를 제공합니다. Pre-Training 단계에서 생성된 checkpoint를 파라미터로 넣어 학습을 진행해야 합니다. \n",
    "\n",
    "* `scripts/run_squad.sh`\n",
    "\n",
    "Q&A 문제에 대한 fine-tuning 스트립트로써, SQuAD 데이터 셋을 사용하여 학습을 진행합니다. 사용가능한 SQuAD 데이터셋 버전은 1.1과 2.0입니다. \n",
    "\n",
    "`bash scripts/run_squad.sh <batch_size_per_gpu> <learning_rate_per_gpu> <precision> <use_xla> <num_gpus> <seq_length> <doc_stride> <bert_model> <squad_version> <checkpoint> <epochs>`\n",
    "\n",
    "* \n",
    "`bash scripts/run_glue.sh`\n",
    "\n",
    "GRUE 벤치마크 중 CoLA, MRPC,  MNLI를 지원합니다. \n",
    "\n",
    "`bash scripts/run_glue.sh <task_name> <batch_size_per_gpu> <learning_rate_per_gpu> <precision> <use_xla> <num_gpus> <seq_length> <doc_stride> <bert_model> <epochs> <warmup_proportion> <checkpoint>`\n",
    " \n",
    " <a id='fine_tuning_param'></a>\n",
    " Fine-tuning을 위한 파라미터는 다음과 같습니다.\n",
    " * `<batch_size_per_gpu>` GPU당 batch size\n",
    " * `<learning_rate_per_gpu>` Learning rate\n",
    " * `<precision>` 모델의 precision(정밀도) fp32 또는 fp16을 선택\n",
    " * `<use_xla>` XLA JIT 사용 여부\n",
    " * `<num_gpus> ` Traning 시 사용하는 GPU 개수\n",
    " * `<seq_length>` 최대 input sequence 길이 \n",
    " * `<doc_stride>` 긴 문서를 chunk로 나눌 때의 stride\n",
    " * `<bert_model>`   Pre-Training시 사용되는 BERT 모델 (large: BERT Large model, base: BERT base model) \n",
    " * `<squad_version>`  SQuAD 데이터셋 버전 \n",
    "     * 1.1: SQuAD v1.1\n",
    "     * 2.0: SQuAD v2.0\n",
    " * `<checkpoint>`  pre-trained checkpoint\n",
    " * `<warmup_proportion>` linear learning rate warpup 실행을 위한 Training 비율 0.1 일 경우 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※__앞서 `<checkpoint>`에 Pre-Training으로 생성된 checkpoint를 사용하시길 바랍니다. 다음 실행 코드에서는 다운받은 pre-trained 모델 chekcpoint를 사용하였습니다. (pre-training 으로 생성된 checkpoint는 BERT 기본 폴더 내 results 폴더나 docker container 내에 /results 폴더에 위치하게 됩니다.)__\n",
    "\n",
    "SQuAD v1.1 데이터셋을 이용한 V100 32G 4GPU 에서의 BERT FP16 Fine-tuning 예제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "docker rm -f bert_training\n",
    "\n",
    "docker run --rm -t \\\n",
    "    --runtime=nvidia \\\n",
    "    --name=bert_training \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -v $PWD:/workspace/bert \\\n",
    "    -v $PWD/results:/results \\\n",
    "    bert bash scripts/run_squad.sh 10 5e-6 fp16 true 4 384 128 base 1.1 data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16/bert_model.ckpt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning 시작하기 (커스텀)\n",
    "\n",
    "\n",
    "SQuAD 데이터셋을 위한 코드로써 `run_squad.py` 파일을 실행해서 학습 할 수 있습니다. pre-trainig 단계에서 training된 checkpoint를 `init_checkpoint`에 설정해 주시길 바랍니다. 파라미터 설명은 [여기](#fine_tuning_param)을 참조 하시길 바랍니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "batch_size=${1:-\"8\"}\n",
    "learning_rate=${2:-\"5e-6\"}\n",
    "precision=${3:-\"fp16\"}\n",
    "use_xla=${4:-\"true\"}\n",
    "num_gpu=${5:-\"4\"}\n",
    "seq_length=${6:-\"384\"}\n",
    "doc_stride=${7:-\"128\"}\n",
    "bert_model=${8:-\"large\"}\n",
    "\n",
    "if [ \"$bert_model\" = \"large\" ] ; then\n",
    "    export BERT_DIR=data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16\n",
    "else\n",
    "    export BERT_DIR=data/download/google_pretrained_weights/uncased_L-12_H-768_A-12\n",
    "fi\n",
    "\n",
    "squad_version=${9:-\"1.1\"}\n",
    "\n",
    "export SQUAD_DIR=data/download/squad/v${squad_version}\n",
    "if [ \"$squad_version\" = \"1.1\" ] ; then\n",
    "    version_2_with_negative=\"False\"\n",
    "else\n",
    "    version_2_with_negative=\"True\"\n",
    "fi\n",
    "\n",
    "init_checkpoint=${10:-\"$BERT_DIR/bert_model.ckpt\"}\n",
    "epochs=${11:-\"2.0\"}\n",
    "\n",
    "echo \"Squad directory set as \" $SQUAD_DIR \" BERT directory set as \" $BERT_DIR\n",
    "\n",
    "use_fp16=\"\"\n",
    "if [ \"$precision\" = \"fp16\" ] ; then\n",
    "        echo \"fp16 activated!\"\n",
    "        use_fp16=\"--use_fp16\"\n",
    "fi\n",
    "\n",
    "if [ \"$use_xla\" = \"true\" ] ; then\n",
    "    use_xla_tag=\"--use_xla\"\n",
    "    echo \"XLA activated\"\n",
    "else\n",
    "    use_xla_tag=\"\"\n",
    "fi\n",
    "\n",
    "if [ $num_gpu -gt 1 ] ; then\n",
    "    mpi_command=\"mpirun -np $num_gpu -H localhost:$num_gpu \\\n",
    "    --allow-run-as-root -bind-to none -map-by slot \\\n",
    "    -x NCCL_DEBUG=INFO \\\n",
    "    -x LD_LIBRARY_PATH \\\n",
    "    -x PATH -mca pml ob1 -mca btl ^openib\"\n",
    "else\n",
    "    mpi_command=\"\"\n",
    "fi\n",
    "\n",
    "export GBS=$(expr $batch_size \\* $num_gpu)\n",
    "printf -v TAG \"tf_bert_finetuning_squad_%s_%s_gbs%d\" \"$bert_model\" \"$precision\" $GBS\n",
    "DATESTAMP=`date +'%y%m%d%H%M%S'`\n",
    "\n",
    "#Edit to save logs & checkpoints in a different directory\n",
    "RESULTS_DIR=results/${TAG}_${DATESTAMP}\n",
    "LOGFILE=$RESULTS_DIR/$TAG.$DATESTAMP.log\n",
    "mkdir -m 777 -p $RESULTS_DIR\n",
    "printf \"Saving checkpoints to %s\\n\" \"$RESULTS_DIR\"\n",
    "printf \"Logs written to %s\\n\" \"$LOGFILE\"\n",
    "\n",
    "#Check if all necessary files are available before training\n",
    "for DIR_or_file in $SQUAD_DIR $RESULTS_DIR $BERT_DIR/bert_config.json $BERT_DIR/vocab.txt; do\n",
    "  if [ ! -d \"$DIR_or_file\" ] && [ ! -f \"$DIR_or_file\" ]; then\n",
    "     echo \"Error! $DIR_or_file directory missing. Please mount correctly\"\n",
    "     exit -1\n",
    "  fi\n",
    "done\n",
    "\n",
    "docker rm -f bert_training\n",
    "\n",
    "docker run --rm -t \\\n",
    "    --runtime=nvidia \\\n",
    "    --name=bert_training \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -v $PWD:/workspace/bert \\\n",
    "    -v $PWD/results:/results \\\n",
    "    bert $mpi_command python run_squad.py \\\n",
    "    --vocab_file=$BERT_DIR/vocab.txt \\\n",
    "    --bert_config_file=$BERT_DIR/bert_config.json \\\n",
    "    --init_checkpoint=$init_checkpoint \\\n",
    "    --do_train=True \\\n",
    "    --train_file=$SQUAD_DIR/train-v${squad_version}.json \\\n",
    "    --do_predict=True \\\n",
    "    --predict_file=$SQUAD_DIR/dev-v${squad_version}.json \\\n",
    "    --train_batch_size=$batch_size \\\n",
    "    --learning_rate=$learning_rate \\\n",
    "    --num_train_epochs=$epochs \\\n",
    "    --max_seq_length=$seq_length \\\n",
    "    --doc_stride=$doc_stride \\\n",
    "    --save_checkpoints_steps 1000 \\\n",
    "    --output_dir=$RESULTS_DIR \\\n",
    "    --horovod \"$use_fp16\" \\\n",
    "    $use_xla_tag --version_2_with_negative=${version_2_with_negative} |& tee $LOGFILE; \n",
    "    python $SQUAD_DIR/evaluate-v${squad_version}.py $SQUAD_DIR/dev-v${squad_version}.json ${RESULTS_DIR}/predictions.json |& tee -a $LOGFILE\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 GLUe benchmark를 위한 코드로써 `run_classifier.py` 파일을 실행해서 학습 할 수 있습니다. `task_name`에 GLUE benchmark의 task (CoLA, MRPC,  MNLI)를 입력해주시면 됩니다. 또한 pre-trainig 단계에서 training된 checkpoint를 `init_checkpoint`에 설정해 주시길 바랍니다. \n",
    "\n",
    "파라미터 설명은 [여기](#fine_tuning_param)을 참조 하시길 바랍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "task_name=${1:-\"MRPC\"}\n",
    "batch_size=${2:-\"32\"}\n",
    "learning_rate=${3:-\"2e-5\"}\n",
    "precision=${4:-\"fp16\"}\n",
    "use_xla=${5:-\"true\"}\n",
    "num_gpu=${6:-\"8\"}\n",
    "seq_length=${7:-\"128\"}\n",
    "doc_stride=${8:-\"64\"}\n",
    "bert_model=${9:-\"large\"}\n",
    "\n",
    "if [ \"$bert_model\" = \"large\" ] ; then\n",
    "    export BERT_DIR=data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16\n",
    "else\n",
    "    export BERT_DIR=data/download/google_pretrained_weights/uncased_L-12_H-768_A-12\n",
    "fi\n",
    "export GLUE_DIR=data/download\n",
    "\n",
    "\n",
    "epochs=${10:-\"3.0\"}\n",
    "ws=${11:-\"0.1\"}\n",
    "init_checkpoint=${12:-\"$BERT_DIR/bert_model.ckpt\"}\n",
    "\n",
    "echo \"GLUE directory set as \" $GLUE_DIR \" BERT directory set as \" $BERT_DIR\n",
    "\n",
    "use_fp16=\"\"\n",
    "if [ \"$precision\" = \"fp16\" ] ; then\n",
    "        echo \"fp16 activated!\"\n",
    "        use_fp16=\"--use_fp16\"\n",
    "fi\n",
    "\n",
    "if [ \"$use_xla\" = \"true\" ] ; then\n",
    "    use_xla_tag=\"--use_xla\"\n",
    "    echo \"XLA activated\"\n",
    "else\n",
    "    use_xla_tag=\"\"\n",
    "fi\n",
    "\n",
    "if [ $num_gpu -gt 1 ] ; then\n",
    "    mpi_command=\"mpirun -np $num_gpu -H localhost:$num_gpu \\\n",
    "    --allow-run-as-root -bind-to none -map-by slot \\\n",
    "    -x NCCL_DEBUG=INFO \\\n",
    "    -x LD_LIBRARY_PATH \\\n",
    "    -x PATH -mca pml ob1 -mca btl ^openib\"\n",
    "else\n",
    "    mpi_command=\"\"\n",
    "fi\n",
    "\n",
    "export GBS=$(expr $batch_size \\* $num_gpu)\n",
    "printf -v TAG \"tf_bert_finetuning_glue_%s_%s_%s_gbs%d\" \"$task_name\" \"$bert_model\" \"$precision\" $GBS\n",
    "DATESTAMP=`date +'%y%m%d%H%M%S'`\n",
    "#Edit to save logs & checkpoints in a different directory\n",
    "RESULTS_DIR=results/${TAG}_${DATESTAMP}\n",
    "LOGFILE=$RESULTS_DIR/$TAG.$DATESTAMP.log\n",
    "mkdir -m 777 -p $RESULTS_DIR\n",
    "printf \"Saving checkpoints to %s\\n\" \"$RESULTS_DIR\"\n",
    "printf \"Logs written to %s\\n\" \"$LOGFILE\"\n",
    "\n",
    "#Check if all necessary files are available before training\n",
    "for DIR_or_file in $GLUE_DIR/${task_name} $RESULTS_DIR $BERT_DIR/vocab.txt $BERT_DIR/bert_config.json; do\n",
    "  echo $DIR_or_file\n",
    "  if [ ! -d \"$DIR_or_file\" ] && [ ! -f \"$DIR_or_file\" ]; then\n",
    "     echo \"Error! $DIR_or_file directory missing. Please mount correctly\"\n",
    "     exit -1\n",
    "  fi\n",
    "done\n",
    "\n",
    "docker rm -f bert_training\n",
    "\n",
    "docker run --rm -t \\\n",
    "    --runtime=nvidia \\\n",
    "    --name=bert_training \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -v $PWD:/workspace/bert \\\n",
    "    -v $PWD/results:/results \\\n",
    "    bert $mpi_command python3 run_classifier.py \\\n",
    "    --task_name=$task_name \\\n",
    "    --do_train=true \\\n",
    "    --do_eval=true \\\n",
    "    --data_dir=$GLUE_DIR/$task_name \\\n",
    "    --vocab_file=$BERT_DIR/vocab.txt \\\n",
    "    --bert_config_file=$BERT_DIR/bert_config.json \\\n",
    "    --init_checkpoint=$init_checkpoint \\\n",
    "    --max_seq_length=$seq_length \\\n",
    "    --doc_stride=$doc_stride \\\n",
    "    --train_batch_size=$batch_size \\\n",
    "    --learning_rate=$learning_rate \\\n",
    "    --num_train_epochs=$epochs \\\n",
    "    --output_dir=$RESULTS_DIR \\\n",
    "    --horovod \"$use_fp16\" \\\n",
    "    $use_xla_tag --warmup_proportion=$ws |& tee $LOGFILE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorRT로 변환하기 위한 파일 목록입니다. 꼭 TensorRT 적용하기 전에 확인해주세요**\n",
    "* `checkpoint`: 모든 Pre-training 과 fune-tuning 의 checkpoint는 `./results` 폴더에 저장됩니다. \n",
    "* `BERT_config.json`: BERT 모델의 layer정보로써 이 예제에서는 BERT base 인 경우 `./data/download/google_pretrained_weights/uncased_L-12_H-768_A-12`, BERT large 인 경우 `./data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16` 를 사용했습니다. \n",
    "* `vocab.txt` : vocabulary 정보보써 이 예제에서는 BERT base 인 경우 `data/download/google_pretrained_weights/uncased_L-12_H-768_A-12`, BERT large 인 경우 `./data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16` 를 참고해주세요 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='performance'></a>\n",
    "## 5. Training 성능\n",
    "\n",
    "<img src=\"../data/images/time_single_node.PNG\" width=\"500\">\n",
    "그림 3. single-node에서 DGX-1와 DGX-2의 training 시간 비교\n",
    "\n",
    "그림 3은 DGX-1과 DGX-2에서의 Pre-training 단계에서 Mixed precision training 시간을 비교한 것으로, DGX-2에서 약 2배의 training 시간 향상을 가져옵니다.\n",
    "<img src=\"../data/images/time_multi_node.PNG\" width=\"500\">\n",
    "그림 4. Multi-node에서 DGX-1와 DGX-2H의 training 시간 비교\n",
    "\n",
    "그림 4는 Multi-node에서 DGX-1과 DGX-2H의 Mixed Precision Trainig 시간을 비교한 것으로 4, 16, 32개의 node에서 DGX-2H가 DGX-1보다 각각   2.8, 2.6, 2.9배의 trianing 시간이 향상되었습니다.\n",
    "\n",
    "<img src=\"../data/images/time_multi_node_precision.PNG\" width=\"500\">\n",
    "그림 5. 64개 node에서 Mixed Precision과 FP32 간의 training 시간 비교 \n",
    "\n",
    "그림 5는 DGX-2H 64개의 노드에서 Single Precision과 Mixed Precision Trainig 시간을 비교한 Mixed Precision Trianig 시간이 Single Precisin Trainig 시간에 비해 약 2.3배 향상되었습니다. BERT의 Pre-training 단계의 trainig 시간을 3시간 이내에 마칠수 있음을 의미합니다. \n",
    "\n",
    "자세한 성능과 성능 측정 파라미터 값은 [여기](https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT#performance)를 참고 하시길 바랍니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## 6. 결론\n",
    "\n",
    "지금까지  BERT 모델 아키텍처에 설명, BERT Training에 필요한 단계인 Pre-training과 Fine-tuning 각 단계에서의 Training 과정에 대해 설명했습니다. 이제 여러분께서는 BERT Triaing 과정을 모두 마치셨습니다. BERT End-to-End의 다음 과정은 Low latency & High throughput을 위한 Infernece와 Inference service를 위한 TensorRT 와 TensorRT inference server 적용 과정을 설명 드리겠습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
